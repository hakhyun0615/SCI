{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from Dataset.WindowDataset import WindowDataset\n",
    "\n",
    "from Model.AE import Auto\n",
    "from Model.LSTM import LSTM\n",
    "from Model.LODE2 import ODEVAE\n",
    "\n",
    "from utils import combine_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco = '콜금리'\n",
    "batch_size = 4\n",
    "max_apart_size = 70  # 0~60평 아파트만 돌림\n",
    "real_estate_weighted_average = 0.5\n",
    "input_size = 5 # LSTM input_size\n",
    "hidden_size = 16 # LSTM hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_estimate(x):\n",
    "    vec = np.zeros((max_apart_size))\n",
    "    vec[int(x[0])] = x[1]\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(x):\n",
    "    date = str(x[1])\n",
    "    if len(date)==1:\n",
    "        date = '0'+date\n",
    "    return str(x[0])+date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../데이터/merge.csv')\n",
    "df['시점'] = df[['계약년월','계약일']].apply(get_date, axis=1)\n",
    "df = df[:100] # /////////////////////////////////\n",
    "df['시점'] = df['시점'].apply(lambda x : int(x))\n",
    "df['평수'] = df['전용면적(㎡)'].apply(lambda x : x // 3.3058)\n",
    "df['가격'] = df['거래금액(만원)'].apply(lambda x : int(x.replace(',','')))\n",
    "df = df.sort_values(by=['시점'])\n",
    "df['부동산'] = df[['평수','가격']].apply(make_estimate,axis=1)\n",
    "df.reset_index(drop=True)\n",
    "df['경제'] = df[eco]\n",
    "df = df[['시점','부동산','경제']]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df = len(df)\n",
    "train_len = int(len_df *0.6)\n",
    "val_len = int(len_df * 0.3)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_dataset = WindowDataset(df[:train_len])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "val_dataset = WindowDataset(df[train_len:train_len+val_len])\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "test_dataset = WindowDataset(df[train_len+val_len:])\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 2])\n",
      "torch.Size([4, 5, 70])\n",
      "torch.Size([4, 2])\n",
      "torch.Size([4, 70])\n"
     ]
    }
   ],
   "source": [
    "# x1 : 시점/경제 데이터, x2 : 부동산 데이터\n",
    "# y1 : 시점/경제 라벨링, y2 : 부동산 라벨링\n",
    "\n",
    "for x1,x2,y1,y2 in train_loader:\n",
    "    print(x1.shape)\n",
    "    print(x2.shape)\n",
    "    print(y1.shape)\n",
    "    print(y2.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 입출력 크기\n",
    "\n",
    "input_size = 256\n",
    "window_size = 5\n",
    "hidden_size = 512\n",
    "latent_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작동하는지 실험\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1136, -0.3061,  0.4087,  ...,  0.4813,  0.2837, -0.4833]],\n",
       " \n",
       "         [[-0.1697, -0.2385,  0.3640,  ...,  0.5128,  0.2821, -0.4445]],\n",
       " \n",
       "         [[-0.1436, -0.2685,  0.3844,  ...,  0.4979,  0.2829, -0.4623]],\n",
       " \n",
       "         [[-0.1119, -0.3078,  0.4099,  ...,  0.4802,  0.2837, -0.4844]],\n",
       " \n",
       "         [[-0.1570, -0.2523,  0.3737,  ...,  0.5054,  0.2825, -0.4530]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([[-4.8150e-01,  1.1618e+00,  3.8211e-01, -1.8602e+00,  2.0590e+00,\n",
       "          -9.4056e-01,  2.6450e-01, -1.0925e+00,  4.9431e-01, -1.7121e+00,\n",
       "          -1.6731e-01,  1.7961e-01, -1.1171e+00,  2.3820e-01,  1.0659e+00,\n",
       "          -1.8009e+00,  8.3070e-01,  1.0767e+00,  1.1459e+00,  1.1616e+00,\n",
       "          -6.9344e-01, -3.9435e-01,  2.1104e+00, -2.0717e-01, -5.6266e-01,\n",
       "           7.2708e-01,  6.5230e-01, -1.0362e+00, -4.3800e-01,  1.5745e+00,\n",
       "          -1.1096e+00,  1.3129e+00, -7.2656e-01,  1.7568e+00,  2.1071e-02,\n",
       "           1.2842e+00,  4.9609e-01,  1.0825e+00, -2.9317e-02,  1.3731e+00,\n",
       "           5.1156e-01,  1.4700e-01, -5.3849e-02, -1.0301e+00, -1.7880e+00,\n",
       "           1.2337e+00, -1.3710e+00, -5.4685e-02, -1.4843e+00,  6.3641e-01,\n",
       "           4.0991e-01, -1.0726e+00, -4.0259e-01, -6.4523e-03, -2.5687e-01,\n",
       "          -8.9030e-01, -8.0984e-01,  1.4381e-02, -8.2523e-01,  3.4210e-01,\n",
       "          -1.8074e+00,  2.4939e-01,  6.5217e-01,  1.2367e-01, -4.5583e-01,\n",
       "          -1.0999e+00, -1.5305e+00,  4.6743e-01, -8.9700e-01, -6.2252e-02,\n",
       "           7.4511e-01,  8.0286e-01, -8.6686e-01, -3.9151e-01, -1.7102e+00,\n",
       "          -7.5552e-01, -1.8606e-02,  2.0173e+00, -1.4074e+00,  2.1523e+00,\n",
       "           4.1983e-01, -1.8331e+00,  2.2406e+00,  4.5409e-01,  1.3481e+00,\n",
       "          -1.1478e+00, -6.2911e-03, -2.9273e-01, -6.0388e-01, -5.7160e-01,\n",
       "           1.2966e+00,  1.8190e+00,  3.9604e-01,  1.5803e-01, -2.1926e-01,\n",
       "          -5.5450e-01,  1.4265e+00,  9.7250e-02,  9.2047e-01, -6.2274e-01,\n",
       "           1.1089e+00, -2.7788e-01,  5.6445e-01,  1.3969e+00, -6.0032e-01,\n",
       "          -6.3050e-01,  8.7505e-02,  3.0078e-01, -1.2733e+00, -1.9183e+00,\n",
       "           7.0256e-01, -3.2741e-01, -9.8175e-01, -2.4039e+00,  1.5469e-02,\n",
       "           4.8115e-01,  1.0410e+00,  1.0759e+00, -6.2258e-01,  1.6626e+00,\n",
       "           1.4080e+00,  5.8890e-01,  7.4267e-01,  6.0404e-02,  3.4591e-01,\n",
       "          -6.4967e-01,  3.6580e-01,  2.1816e-01,  1.2026e+00, -1.7345e+00,\n",
       "          -3.5824e-01, -1.2836e+00, -1.6834e+00,  3.1227e-01, -4.2016e-01,\n",
       "           9.7026e-01, -5.0225e-01, -4.1888e-01, -1.3676e-01, -4.1474e-01,\n",
       "          -4.7675e-02, -1.8512e-03, -7.2892e-01, -6.0837e-01, -1.8591e+00,\n",
       "           3.6210e-01,  1.8781e+00,  4.4777e-01, -2.0494e-01,  1.5168e+00,\n",
       "           1.1095e+00, -5.8849e-01,  7.7208e-01, -7.0917e-01,  4.0962e-01,\n",
       "           7.0294e-01,  6.1034e-01,  4.0085e-01,  8.5431e-01,  9.8725e-01,\n",
       "           2.5387e-01, -4.9756e-01,  4.2603e-01, -3.1258e-01, -7.5041e-02,\n",
       "           1.3932e-01,  2.9846e-02, -6.0454e-01,  1.9129e+00, -4.9076e-01,\n",
       "          -3.9792e-01, -1.4704e+00, -1.1251e+00,  1.1615e+00, -8.1898e-01,\n",
       "           6.2537e-01, -2.0458e-01, -9.1658e-02,  1.8115e+00, -4.7253e-01,\n",
       "          -2.2878e-02,  1.2645e-01, -7.0166e-01, -2.0920e-01, -1.0499e+00,\n",
       "           6.5715e-01, -1.5208e+00,  1.0542e+00, -2.8125e-01,  1.1345e+00,\n",
       "          -2.6382e-01, -1.2737e+00, -2.0826e+00, -4.1208e-01, -5.2523e-01,\n",
       "          -1.0751e+00,  7.8964e-01, -3.6543e-01,  1.5507e+00,  1.2225e+00,\n",
       "           1.2123e+00,  3.5902e-01, -1.3762e+00, -9.3501e-01, -5.3915e-01,\n",
       "           3.9634e-01,  1.5482e+00, -6.0407e-01, -2.1204e+00,  6.4598e-01,\n",
       "          -1.7532e+00, -1.1914e+00,  6.0662e-01,  4.6131e-01,  1.0486e+00,\n",
       "          -1.9040e+00,  3.8941e-01, -1.2228e-01,  4.9729e-01, -4.4086e-02,\n",
       "           2.2920e-01, -2.0165e-03, -2.8056e-01,  1.5221e+00, -1.0657e-01,\n",
       "          -2.3129e-01,  5.4005e-01,  3.3978e-01, -1.4556e+00, -1.0459e+00,\n",
       "          -8.1381e-01,  4.4325e-01,  9.5983e-01, -3.0374e-01,  7.0374e-01,\n",
       "          -8.4335e-02, -1.7891e-01,  1.0546e+00, -1.4097e-01, -4.6442e-01,\n",
       "           2.7601e-01, -2.0964e-01,  3.6152e-01, -3.3370e-01, -2.9064e-01,\n",
       "           4.7524e-01,  3.4608e-01,  9.9628e-01, -5.4986e-01, -1.5728e+00,\n",
       "          -7.4403e-01,  1.2105e-01,  8.3288e-01, -1.1656e+00, -1.1670e+00,\n",
       "          -1.6471e+00,  4.6585e-01,  4.4373e-01, -1.2840e+00, -9.5177e-01,\n",
       "          -1.4613e+00, -9.3488e-01, -1.4248e+00,  8.5862e-01, -1.4637e+00,\n",
       "          -1.7805e-01, -2.5829e-02, -5.1843e-01,  1.2807e+00,  1.5116e-01,\n",
       "           1.4762e+00,  2.1989e-01, -9.6581e-01,  2.8858e+00,  1.4823e+00,\n",
       "          -9.1687e-01,  1.5484e+00, -6.8685e-01, -3.3194e-01,  1.5075e-01,\n",
       "           2.6960e-01,  1.0237e+00, -2.6850e-01, -3.0088e-01, -1.6653e-02,\n",
       "          -2.3304e-01,  1.2158e+00,  3.8622e-01, -4.6501e-01,  4.6069e-01,\n",
       "          -1.6998e-01, -1.4273e+00, -3.7873e-01, -3.8113e-01, -6.4769e-01,\n",
       "           1.0523e+00,  1.1021e+00,  1.4728e+00,  1.0771e+00, -1.4097e+00,\n",
       "           3.4624e-01,  1.0095e+00, -2.7545e-01, -8.8903e-01, -1.6939e+00,\n",
       "           9.6343e-02, -2.4859e+00, -4.8210e-01,  1.6159e+00,  2.5955e+00,\n",
       "           8.2033e-03, -1.0678e+00, -1.7283e+00,  2.5941e-01, -1.0140e-01,\n",
       "           3.3746e-01, -4.5185e-01,  1.6889e+00, -1.0594e+00,  4.1436e-01,\n",
       "          -4.4535e-01,  3.6868e-01, -5.8860e-01, -3.3854e-02, -2.5301e-01,\n",
       "           1.3578e+00,  6.3340e-01, -1.4302e+00,  2.3653e-01,  2.0471e+00,\n",
       "          -2.8927e-01, -9.3430e-01,  1.4017e+00,  3.5889e-02, -1.0121e+00,\n",
       "           2.4495e+00,  4.7086e-01,  3.0957e+00, -1.0765e+00,  1.2713e+00,\n",
       "           2.0677e-01,  6.4860e-01, -2.4480e-01,  6.8062e-01, -1.4595e+00,\n",
       "           1.3571e+00, -2.1492e-01, -4.2258e-01,  6.8146e-01,  5.9334e-01,\n",
       "           4.7016e-02,  3.1473e+00, -1.0518e+00, -9.3990e-01, -1.0193e+00,\n",
       "          -5.7364e-01,  7.7769e-01,  4.3388e-01, -1.4863e+00,  9.3511e-01,\n",
       "           2.8305e-01,  2.1205e+00,  2.3433e-01, -1.0262e+00, -4.8196e-01,\n",
       "          -3.4324e-01,  1.0048e+00,  3.0535e-01, -9.4547e-01,  1.6238e+00,\n",
       "          -5.6231e-02, -3.2923e-01, -6.6642e-01,  1.1800e+00, -1.1548e+00,\n",
       "          -3.5633e-01, -8.1978e-02, -5.0194e-01,  6.8543e-01, -4.0216e-01,\n",
       "           1.4473e+00,  9.5663e-01, -8.1572e-01,  4.2133e-01, -1.2014e+00,\n",
       "           2.7166e-02, -7.5309e-01,  4.8237e-01, -1.3227e-01, -8.1264e-01,\n",
       "           1.3988e+00, -1.2324e+00, -1.7878e+00,  1.3158e-01,  3.5237e-02,\n",
       "          -5.0456e-01,  1.0897e+00,  1.3120e+00, -1.6320e-01, -2.7103e-01,\n",
       "           1.5200e+00, -6.5541e-01,  1.7059e-01, -6.8753e-01,  1.5312e+00,\n",
       "          -8.7352e-01, -2.5455e-01, -1.5147e+00, -5.9529e-01, -2.0681e+00,\n",
       "           5.1301e-01,  2.0390e+00,  2.8010e-01, -1.5178e+00,  4.2521e-01,\n",
       "          -1.4727e+00,  4.7140e-01,  1.5300e+00,  5.7709e-01,  8.7573e-01,\n",
       "           3.7674e-01, -2.7449e-01, -8.6294e-01,  4.5900e-01, -6.6256e-01,\n",
       "           4.3705e-01,  1.7672e+00,  6.8504e-01,  6.9384e-02, -1.6881e-01,\n",
       "           8.8837e-01,  2.2959e+00,  9.2210e-01, -5.0930e-01, -5.2962e-01,\n",
       "           5.0294e-01, -5.3722e-01, -6.1748e-01, -2.4947e+00,  3.2798e-01,\n",
       "          -1.9188e+00, -1.7785e-01, -5.8098e-01, -1.6492e+00, -5.1388e-01,\n",
       "           2.0084e+00,  5.6416e-01, -1.5199e+00, -4.5961e-01, -3.1358e-01,\n",
       "           8.9346e-02, -1.4025e+00,  3.8214e-01,  2.9865e-01,  1.5078e+00,\n",
       "          -1.3977e-02, -1.4549e+00, -1.2422e+00, -2.0947e+00,  3.4540e-01,\n",
       "          -1.5442e+00, -2.6645e-02, -9.8617e-01, -1.1797e+00, -4.1452e-03,\n",
       "           7.9870e-01,  3.1233e-01,  1.1556e+00, -5.2848e-01, -2.9748e-01,\n",
       "           1.6609e-01,  2.2570e-01,  1.0209e+00,  3.1377e-01, -1.6467e+00,\n",
       "           6.1813e-01,  1.5919e-01, -5.6423e-01, -1.3441e+00, -1.5896e+00,\n",
       "          -4.9314e-01, -6.7030e-02, -9.5857e-01, -7.0871e-01, -1.2103e+00,\n",
       "           1.7317e+00, -5.1112e-01,  9.3795e-01, -1.4302e+00, -7.4484e-01,\n",
       "           1.4679e-02, -4.4594e-01,  9.4876e-01, -9.8830e-01,  2.1034e+00,\n",
       "          -2.4920e-01, -1.1532e+00,  1.5072e+00, -1.5501e+00, -1.0410e+00,\n",
       "          -6.6930e-01,  7.6230e-01,  1.3691e+00, -7.9296e-01, -1.2695e+00,\n",
       "           1.0958e+00, -1.5363e+00,  5.4759e-01,  1.1096e-01,  1.2289e+00,\n",
       "           2.4070e-01,  5.5775e-02]], grad_fn=<AddBackward0>),\n",
       " tensor([[-0.1683,  0.2541,  0.0181, -0.0181,  0.0670,  0.1439, -0.2252, -0.0420,\n",
       "          -0.0071, -0.1331, -0.1561, -0.2470, -0.1073, -0.1538, -0.0383,  0.0249,\n",
       "          -0.1722,  0.0247,  0.0714, -0.0824, -0.2069,  0.0032,  0.0207,  0.0463,\n",
       "          -0.1170, -0.0910,  0.0182,  0.0603,  0.0041,  0.2342,  0.0164,  0.0201,\n",
       "          -0.0095, -0.0343,  0.0094,  0.0274,  0.0260, -0.1605,  0.0645, -0.0547,\n",
       "           0.0260, -0.1000,  0.0717, -0.0022,  0.1711, -0.0172,  0.0469, -0.0648,\n",
       "           0.0366, -0.1019, -0.0354,  0.1746,  0.1006,  0.0084,  0.0914,  0.0721,\n",
       "          -0.1524,  0.0181, -0.0728,  0.1631,  0.0075,  0.0560,  0.0971,  0.0496,\n",
       "           0.0593,  0.0440,  0.0414,  0.1010, -0.0291, -0.0027,  0.0945, -0.0030,\n",
       "          -0.0875,  0.0923,  0.0437, -0.0074, -0.0395,  0.0370,  0.0691,  0.0663,\n",
       "           0.1031, -0.0907,  0.0529, -0.0236, -0.1800, -0.1232, -0.0104,  0.1569,\n",
       "          -0.0925,  0.0357,  0.1882, -0.0054,  0.1016,  0.0493, -0.1784,  0.1723,\n",
       "           0.1578,  0.0545,  0.0044, -0.1357,  0.1140,  0.1422,  0.0537,  0.0205,\n",
       "          -0.1784,  0.0399,  0.1256, -0.0572, -0.1352,  0.1780,  0.2923,  0.0812,\n",
       "           0.0878, -0.1084,  0.0362,  0.1633, -0.0987,  0.1642,  0.1709, -0.2053,\n",
       "           0.1277, -0.0781, -0.0025,  0.0984,  0.0145,  0.0793, -0.2528, -0.0142,\n",
       "           0.2447,  0.0116, -0.1414, -0.0990, -0.0820, -0.1392, -0.0083, -0.2026,\n",
       "          -0.0135,  0.1371, -0.0345,  0.0461,  0.0282,  0.1491, -0.0229, -0.1276,\n",
       "          -0.0269, -0.0340, -0.0618, -0.0189,  0.1223,  0.0434, -0.0026,  0.0163,\n",
       "          -0.0132, -0.0633,  0.2332, -0.1034,  0.0100,  0.0825,  0.1288, -0.0458,\n",
       "          -0.1020, -0.1988, -0.0043,  0.2404,  0.0877, -0.1871, -0.0614,  0.0021,\n",
       "          -0.1737, -0.1319, -0.0084, -0.1767,  0.0952, -0.0767,  0.1418,  0.1135,\n",
       "           0.0149,  0.1142, -0.1391, -0.3250, -0.1003, -0.0059,  0.2515, -0.0584,\n",
       "           0.1609, -0.3137, -0.0191,  0.0131, -0.2192,  0.1974, -0.0389,  0.3521,\n",
       "           0.0142, -0.1637, -0.0383, -0.0206, -0.1722,  0.1661, -0.0339,  0.0219,\n",
       "          -0.0685,  0.0516, -0.1794,  0.0004, -0.1575,  0.0207,  0.1172, -0.2249,\n",
       "           0.0892, -0.0330, -0.0050,  0.1040,  0.0136, -0.0228,  0.0916, -0.2470,\n",
       "          -0.0622,  0.0542, -0.0804, -0.0139,  0.2308,  0.0024, -0.3496, -0.0039,\n",
       "          -0.0561,  0.2041,  0.1649,  0.0380, -0.0420,  0.0308,  0.1472, -0.2252,\n",
       "           0.2023, -0.1135, -0.0648,  0.1436, -0.1075, -0.0009,  0.1358,  0.1201,\n",
       "           0.0427,  0.0301, -0.0591, -0.1193, -0.0324,  0.1008, -0.0521, -0.0945,\n",
       "          -0.1398, -0.0988, -0.0805,  0.1192, -0.0680,  0.0229, -0.0481, -0.1535,\n",
       "          -0.1595,  0.1667,  0.0174, -0.1548,  0.0908, -0.0353, -0.0425, -0.1635,\n",
       "           0.0557, -0.1245, -0.0966, -0.4013, -0.0014,  0.1812,  0.0293,  0.0311,\n",
       "           0.0033, -0.0629,  0.0136,  0.0960,  0.1235, -0.0490,  0.2707,  0.0986,\n",
       "           0.0331, -0.0901,  0.2195, -0.0625,  0.0712, -0.1954, -0.0969,  0.0394,\n",
       "          -0.1230,  0.0252, -0.0355, -0.1199, -0.0557,  0.1083, -0.0254, -0.2180,\n",
       "           0.1982,  0.2325,  0.0239, -0.1574,  0.0331,  0.0273,  0.0337,  0.0741,\n",
       "          -0.0667, -0.2660,  0.0281, -0.1810,  0.1091,  0.2536, -0.1975, -0.0505,\n",
       "          -0.2621,  0.0939,  0.0997, -0.0462,  0.0804, -0.1152,  0.0123,  0.1866,\n",
       "          -0.0709, -0.0137,  0.1665,  0.0023, -0.2334,  0.1782, -0.0106, -0.0352,\n",
       "          -0.0113, -0.1307, -0.1929, -0.0426,  0.1569, -0.0176, -0.0806,  0.1543,\n",
       "          -0.0064,  0.1204,  0.0308,  0.1616,  0.2166, -0.0526,  0.0372,  0.0832,\n",
       "           0.0149,  0.0429,  0.0665, -0.0720,  0.0363, -0.0524, -0.2320,  0.0731,\n",
       "           0.0592,  0.1076, -0.3344,  0.1275, -0.1068,  0.0505,  0.0948, -0.0252,\n",
       "           0.0360, -0.1387,  0.0309, -0.1118,  0.1342,  0.0046, -0.2771, -0.1233,\n",
       "          -0.0234, -0.0273,  0.0916,  0.1048, -0.1954,  0.1889, -0.1503, -0.1592,\n",
       "           0.1080, -0.0967,  0.0569,  0.0756,  0.1057, -0.1274,  0.0093, -0.1570,\n",
       "          -0.0408,  0.0634,  0.0467, -0.0174, -0.1266, -0.1163, -0.2532, -0.2002,\n",
       "          -0.1416,  0.2466, -0.2777, -0.0685,  0.0299,  0.0112,  0.0794,  0.0122,\n",
       "           0.2543,  0.0377,  0.2278,  0.2951,  0.1288, -0.0935, -0.0832,  0.0057,\n",
       "          -0.0706, -0.1577, -0.0331, -0.1260, -0.0538, -0.0437,  0.0931, -0.0368,\n",
       "           0.0416, -0.0168, -0.0114, -0.1385, -0.1079, -0.2665,  0.1587, -0.0228,\n",
       "           0.0780, -0.0994, -0.0853,  0.0050,  0.0103, -0.1144, -0.0482, -0.0999,\n",
       "          -0.0563,  0.1285,  0.1824, -0.0319,  0.0282,  0.2984,  0.0585, -0.1214,\n",
       "          -0.0079,  0.0655, -0.0159, -0.1199, -0.1903,  0.1648,  0.1354,  0.2106,\n",
       "           0.0020,  0.0603,  0.0389, -0.0355,  0.0280, -0.1230, -0.1018,  0.1917,\n",
       "           0.2387, -0.1201, -0.1491, -0.0367, -0.1802,  0.0473, -0.0037,  0.2379,\n",
       "          -0.1387,  0.1864,  0.0740,  0.2678, -0.3325, -0.1482, -0.1505,  0.0106,\n",
       "          -0.0661, -0.0594, -0.0470, -0.0208,  0.1192,  0.0160, -0.0612,  0.0816,\n",
       "           0.1360, -0.0661, -0.2095, -0.0792,  0.1605,  0.0222,  0.1552, -0.1410,\n",
       "           0.0334,  0.0921,  0.0284, -0.1915,  0.0037,  0.0679, -0.0980, -0.0167,\n",
       "           0.2205,  0.1742,  0.0667, -0.1168, -0.2235,  0.0312, -0.1719,  0.0864,\n",
       "          -0.0508,  0.0167,  0.1602, -0.0472, -0.0055,  0.0505,  0.0770,  0.0493]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " tensor([[ 0.0815, -0.0756,  0.0980,  0.1091, -0.0815, -0.1124, -0.2675, -0.0948,\n",
       "          -0.0047,  0.1609,  0.1721, -0.1493,  0.0613, -0.1231, -0.0526,  0.1108,\n",
       "           0.1984,  0.1985,  0.0374, -0.1533,  0.0279, -0.2154,  0.0507, -0.1158,\n",
       "           0.0939, -0.0059, -0.0791, -0.1072, -0.0223,  0.0482,  0.2490, -0.1573,\n",
       "          -0.0694,  0.1353, -0.0451, -0.1086, -0.0439, -0.0885,  0.1597, -0.0542,\n",
       "          -0.1506, -0.1144, -0.1533, -0.0156,  0.1639, -0.2086, -0.1259,  0.2396,\n",
       "          -0.0895, -0.0808, -0.2105, -0.0937, -0.0427, -0.0708,  0.0737,  0.0833,\n",
       "           0.1490, -0.1134, -0.0790, -0.2067,  0.0881, -0.0719,  0.0463, -0.2522,\n",
       "          -0.1421, -0.1110, -0.0693,  0.0364,  0.2462, -0.1131,  0.2059,  0.0360,\n",
       "          -0.0630, -0.0086, -0.0115,  0.0804,  0.0768, -0.0762,  0.0682,  0.1260,\n",
       "          -0.0371, -0.1037,  0.0806, -0.1457, -0.0442,  0.1420,  0.0927,  0.1364,\n",
       "           0.0262, -0.0830, -0.1614, -0.0939, -0.0566,  0.0621,  0.1141, -0.1108,\n",
       "           0.0469,  0.0973,  0.1089, -0.0355,  0.2405,  0.0714, -0.0179,  0.0015,\n",
       "           0.1368, -0.0861, -0.0608,  0.0165,  0.1789, -0.0704, -0.0201, -0.0040,\n",
       "          -0.0513, -0.1315,  0.2729, -0.0624, -0.0194, -0.0370,  0.0092, -0.0524,\n",
       "          -0.0515,  0.0572,  0.0260, -0.2063,  0.0365, -0.1166, -0.0548,  0.0488,\n",
       "           0.1692, -0.1579, -0.2242, -0.0997,  0.2405,  0.0700,  0.0564,  0.1155,\n",
       "           0.0243,  0.0890, -0.0107, -0.0158,  0.0518,  0.1600,  0.0623,  0.0462,\n",
       "          -0.0628,  0.0590,  0.0807, -0.0934, -0.0039,  0.0523, -0.1791, -0.0784,\n",
       "           0.0036, -0.1153, -0.1491, -0.0913, -0.0917,  0.0361,  0.0324, -0.0131,\n",
       "           0.0200, -0.0571, -0.0042,  0.0206, -0.0250,  0.1233, -0.0797,  0.1336,\n",
       "           0.1644,  0.0360, -0.0608, -0.2881, -0.0539,  0.0147,  0.1519,  0.0090,\n",
       "           0.0462,  0.0155, -0.1421, -0.1020, -0.0534,  0.0186,  0.1375,  0.0571,\n",
       "           0.0397,  0.0663, -0.1130, -0.0247, -0.1881, -0.1531,  0.2506,  0.0927,\n",
       "          -0.0687,  0.0918,  0.0133,  0.0429,  0.2063,  0.0134,  0.0905,  0.0428,\n",
       "          -0.0877, -0.0092,  0.0312, -0.0348, -0.0551, -0.0940,  0.0759,  0.0163,\n",
       "          -0.0373, -0.0415, -0.1831,  0.0484, -0.0019, -0.0273, -0.0918, -0.0060,\n",
       "          -0.1063, -0.0416, -0.0258,  0.1736,  0.0452, -0.0159, -0.0521, -0.0158,\n",
       "           0.0470,  0.1562,  0.0947, -0.0561,  0.1850, -0.0189,  0.1570, -0.1596,\n",
       "          -0.0199,  0.0586,  0.1057,  0.1182, -0.1960,  0.2979, -0.1613, -0.2639,\n",
       "          -0.1168, -0.1573,  0.1117, -0.1145,  0.0706, -0.2020, -0.0913, -0.1086,\n",
       "          -0.0057,  0.1091, -0.1271, -0.2251,  0.0113, -0.0475,  0.0667,  0.0950,\n",
       "           0.1497, -0.1443,  0.0071,  0.0248,  0.0543,  0.1051,  0.0611,  0.0224,\n",
       "          -0.2162,  0.0145, -0.1098, -0.2063,  0.2546, -0.1732,  0.0905, -0.2011,\n",
       "           0.0124,  0.1087,  0.2358,  0.0860, -0.0244,  0.0896,  0.1302, -0.1397,\n",
       "           0.0863,  0.0788,  0.1526,  0.0646, -0.1244,  0.0033, -0.1554, -0.0202,\n",
       "          -0.0231, -0.1367,  0.0221,  0.0266, -0.1026,  0.0244, -0.2135,  0.0119,\n",
       "          -0.0420,  0.0890, -0.0493,  0.1111, -0.1116, -0.0526,  0.0441, -0.1739,\n",
       "           0.1427, -0.0885,  0.0414, -0.0127, -0.0210,  0.0249,  0.0329,  0.0440,\n",
       "          -0.3243, -0.0942, -0.1362,  0.0116, -0.1700, -0.1718,  0.0602,  0.0444,\n",
       "          -0.0517,  0.1510, -0.1729, -0.1008, -0.0506, -0.0040, -0.0372, -0.2017,\n",
       "           0.0703, -0.2805, -0.0311, -0.2770,  0.0501,  0.1559,  0.0993,  0.1966,\n",
       "          -0.0347, -0.0175, -0.1727, -0.1023, -0.0109,  0.0469, -0.0705,  0.2045,\n",
       "          -0.0581,  0.1762,  0.0648,  0.0466,  0.0245,  0.0458, -0.1311, -0.0580,\n",
       "           0.0714, -0.0934,  0.0153,  0.2235, -0.0107,  0.0726, -0.1265, -0.1292,\n",
       "           0.0669, -0.0339, -0.0314, -0.1174,  0.0309,  0.0402, -0.1005, -0.0331,\n",
       "           0.0198,  0.0598,  0.0362,  0.0313, -0.0989,  0.1009, -0.1905,  0.1982,\n",
       "          -0.0216,  0.0694, -0.0827, -0.0328, -0.1030,  0.0191,  0.0527, -0.0912,\n",
       "           0.1026,  0.0012, -0.1614, -0.0372,  0.0102,  0.0424,  0.1862, -0.0184,\n",
       "           0.0349, -0.0668,  0.0028, -0.1405,  0.1297,  0.0345,  0.1181,  0.0646,\n",
       "          -0.2134,  0.0014,  0.2181, -0.0763,  0.2831, -0.1186,  0.1256,  0.2818,\n",
       "          -0.1213,  0.0992, -0.0206, -0.0480,  0.0975,  0.1564, -0.1254, -0.0236,\n",
       "          -0.2271, -0.1981, -0.0007, -0.1060, -0.1496,  0.1759,  0.1594,  0.0798,\n",
       "          -0.0122,  0.1274,  0.1223, -0.2052,  0.0398,  0.1919,  0.1018,  0.0562,\n",
       "          -0.0175,  0.0782,  0.0476,  0.1333,  0.0732, -0.0075, -0.0435,  0.1655,\n",
       "           0.1488,  0.0215,  0.1438, -0.1421,  0.0558, -0.1473, -0.1605, -0.0967,\n",
       "           0.1389, -0.2114, -0.1794,  0.0910, -0.0275,  0.1408, -0.0221, -0.0435,\n",
       "           0.0831, -0.0706,  0.0042,  0.0561,  0.0264,  0.0280,  0.0801, -0.0536,\n",
       "          -0.1108, -0.0561,  0.2039, -0.0833, -0.1954,  0.0219,  0.0813,  0.0027,\n",
       "          -0.1397,  0.1915,  0.0005,  0.2443,  0.0480,  0.1399, -0.0973, -0.0800,\n",
       "           0.1753, -0.1623, -0.0439, -0.1215, -0.2392, -0.2992,  0.0571,  0.0601,\n",
       "          -0.0644, -0.0540,  0.0100,  0.0098,  0.0524, -0.1237,  0.1721, -0.0916,\n",
       "          -0.0790, -0.0471,  0.1275,  0.0960, -0.0986, -0.1019,  0.0599,  0.1713,\n",
       "          -0.0651,  0.1340,  0.0339,  0.0205,  0.0922, -0.0332, -0.0619,  0.0085]],\n",
       "        grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### 모델\n",
    "Model = ODEVAE(input_size,hidden_size,latent_size)\n",
    "\n",
    "model = Model.to(device)\n",
    "\n",
    "print('작동하는지 실험')\n",
    "basic_data = torch.rand((window_size,1,input_size))\n",
    "t = torch.rand((window_size,1,1))\n",
    "model(basic_data,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu is available\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# 데이터 & 모델에 device 붙임!!!\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device} is available')\n",
    "\n",
    "# model = Model.to(device)\n",
    "\n",
    "# print('작동하는지 실험')\n",
    "# basic_data = torch.rand((1,input_size))\n",
    "# model(basic_data)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "lr = 1e-3\n",
    "num_epochs = 10\n",
    "noise_std = 0.02\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Auto:\n\tUnexpected key(s) in state_dict: \"encoder.4.weight\", \"encoder.4.bias\", \"decoder.4.weight\", \"decoder.4.bias\". \n\tsize mismatch for encoder.0.weight: copying a param with shape torch.Size([64, 7]) from checkpoint, the shape in current model is torch.Size([32, 70]).\n\tsize mismatch for encoder.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for encoder.2.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([16, 32]).\n\tsize mismatch for encoder.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for decoder.0.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([32, 16]).\n\tsize mismatch for decoder.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for decoder.2.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([70, 32]).\n\tsize mismatch for decoder.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([70]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m economy_embedding_model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mCheckpoint/economic_best_model.pth\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      4\u001b[0m real_estate_embedding_model \u001b[39m=\u001b[39m Auto(max_apart_size)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 5\u001b[0m real_estate_embedding_model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mCheckpoint/embedding_best_model.pth\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\hkyoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Auto:\n\tUnexpected key(s) in state_dict: \"encoder.4.weight\", \"encoder.4.bias\", \"decoder.4.weight\", \"decoder.4.bias\". \n\tsize mismatch for encoder.0.weight: copying a param with shape torch.Size([64, 7]) from checkpoint, the shape in current model is torch.Size([32, 70]).\n\tsize mismatch for encoder.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for encoder.2.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([16, 32]).\n\tsize mismatch for encoder.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for decoder.0.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([32, 16]).\n\tsize mismatch for decoder.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for decoder.2.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([70, 32]).\n\tsize mismatch for decoder.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([70])."
     ]
    }
   ],
   "source": [
    "economy_embedding_model = LSTM(input_size,hidden_size).to(device)\n",
    "economy_embedding_model.load_state_dict(torch.load('Checkpoint/economic_best_model.pth'))\n",
    "\n",
    "real_estate_embedding_model = Auto(max_apart_size).to(device)\n",
    "real_estate_embedding_model.load_state_dict(torch.load('Checkpoint/embedding_best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print('epoch : {}'.format(epoch))\n",
    "    running_loss = 0.0\n",
    "    num = 0\n",
    "    print(\"train\")\n",
    "    model.train()\n",
    "    for x1, x2, y1, y2 in train_loader:\n",
    "        x1, x2 = x1.to(device), x2.to(device) # x1: [시점, 경제], x2: 부동산\n",
    "        y1, y2 = y1.to(device), y2.to(device)\n",
    "        \n",
    "        time_data = x1[:,:,0]\n",
    "        eco_data = x1[:,:,1]\n",
    "        \n",
    "        # 경제 데이터\n",
    "        eco_emb = economy_embedding_model(eco_data)\n",
    "        # 부동산 데이터\n",
    "        real_est_emb = real_estate_embedding_model(x2)\n",
    "        \n",
    "        final_emb = combine_tensors(real_est_emb, eco_emb, real_estate_weighted_average, 'sum')\n",
    "        ##########################\n",
    "        # 모델에 x[0], final_emb 가지고 ODE 학습하기!!!!\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_p, z, z_mean, z_log_var = model(final_emb, time_data)\n",
    "        kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean**2 - torch.exp(z_log_var), -1)\n",
    "        loss = 0.5 * ((final_emb-x_p)**2).sum(-1).sum(0) / noise_std**2 + kl_loss\n",
    "        loss = torch.mean(loss)\n",
    "       \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
