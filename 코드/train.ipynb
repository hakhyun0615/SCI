{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "from Dataset.Embedding_Dataset import Embedding_Dataset\n",
    "from Model.Embedding import Embedding\n",
    "\n",
    "from Dataset.LSTM_Dataset import LSTM_Dataset\n",
    "from Model.LSTM import LSTM\n",
    "from Model.NLinear import NLinear\n",
    "from Model.Transformer import Transformer\n",
    "\n",
    "from Dataset.Attention_Dataset import Attention_Dataset\n",
    "from Model.Attention import LSTMSeq2Seq\n",
    "\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = torch.device('cpu') # CPU\n",
    "# DEVICE = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu') # MAC\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # WINDOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "connection_info = \"host=localhost dbname=postgres user=postgres password=hd219833 port=5432\"\n",
    "conn = psycopg2.connect(connection_info)\n",
    "table_1_query = '''\n",
    "    SELECT * FROM building\n",
    "    '''\n",
    "table_2_query = '''\n",
    "    SELECT * FROM economy\n",
    "    '''\n",
    "table_3_query = '''\n",
    "    SELECT * FROM building_price\n",
    "    '''\n",
    "table_1 = pd.read_sql(table_1_query,conn) \n",
    "table_2 = pd.read_sql(table_2_query,conn)\n",
    "table_3 = pd.read_sql(table_3_query,conn) \n",
    "\n",
    "# table_1 = pd.read_csv('../데이터/Table/table_1.csv') \n",
    "# table_2 = pd.read_csv('../데이터/Table/table_2.csv') \n",
    "# table_3 = pd.read_csv('../데이터/Table/table_3.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSE,self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = 1e-7\n",
    "\n",
    "    def forward(self, y, y_hat):\n",
    "        return torch.sqrt(self.mse(y, y_hat) + self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val_losses(train_losses, val_losses):\n",
    "    print(f'Min Validation Loss: {min(val_losses)}')\n",
    "    plt.plot(train_losses[1:], label='Training Loss')\n",
    "    plt.plot(val_losses[1:], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val loss가 연속적으로 오를 때\n",
    "def early_stop_1(val_losses, consecutive_val_loss_increases, max_consecutive_val_loss_increases):\n",
    "    if len(val_losses) > 1 and val_losses[-1] > val_losses[-2]:\n",
    "        consecutive_val_loss_increases += 1\n",
    "        if consecutive_val_loss_increases >= max_consecutive_val_loss_increases:\n",
    "            return True, consecutive_val_loss_increases\n",
    "        else:\n",
    "            return False, consecutive_val_loss_increases\n",
    "    else:\n",
    "        consecutive_val_loss_increases = 0\n",
    "        return False, consecutive_val_loss_increases\n",
    "\n",
    "# val loss가 최저 loss보다 연속적으로 클 때\n",
    "def early_stop_2(avg_val_loss, best_val_loss, consecutive_val_loss_increases, max_consecutive_val_loss_increases):\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        consecutive_val_loss_increases = 0\n",
    "    else:\n",
    "        consecutive_val_loss_increases += 1\n",
    "    if consecutive_val_loss_increases >= max_consecutive_val_loss_increases:\n",
    "        return True, best_val_loss, consecutive_val_loss_increases\n",
    "    else:\n",
    "        return False, best_val_loss, consecutive_val_loss_increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "\n",
    "embedding_lr = 0.00001\n",
    "embedding_weight_decay = 0\n",
    "embedding_batch = 128\n",
    "embedding_epochs = 150\n",
    "encoder_dim_1 = 128\n",
    "encoder_dim_2 = 256\n",
    "encoder_dim_3 = 512\n",
    "embedding_dim = 1024\n",
    "decoder_dim_1 = 512\n",
    "decoder_dim_2 = 256\n",
    "decoder_dim_3 = 128\n",
    "\n",
    "lstm_lr = 0.0001\n",
    "lstm_weight_decay = 0\n",
    "lstm_batch = 128\n",
    "lstm_epochs = 150\n",
    "lstm_hidden_dim = 256\n",
    "lstm_window_size = 10\n",
    "\n",
    "nlinear_lr = 0.0001\n",
    "nlinear_weight_decay = 0\n",
    "nlinear_batch = 128\n",
    "nlinear_epochs = 150\n",
    "nlinear_window_size = 10\n",
    "\n",
    "attention_lr = 0.0001\n",
    "attention_weight_decay = 0\n",
    "attention_batch = 1\n",
    "attention_epochs = 150\n",
    "attention_hidden_dim = 256\n",
    "attention_window_size = 10\n",
    "\n",
    "transformer_lr = 0.0001\n",
    "transformer_weight_decay = 0\n",
    "transformer_batch = 1\n",
    "transformer_epochs = 150\n",
    "transformer_window_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Embedding_Dataset(table_1, table_2, table_3)\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Train Loss: 6.5733, Val Loss: 6.5547\n",
      "Epoch [2/150], Train Loss: 6.1573, Val Loss: 6.1832\n",
      "Epoch [3/150], Train Loss: 5.8074, Val Loss: 5.8475\n",
      "Epoch [4/150], Train Loss: 5.5186, Val Loss: 5.6872\n",
      "Epoch [5/150], Train Loss: 5.2767, Val Loss: 5.6049\n",
      "Epoch [6/150], Train Loss: 5.0581, Val Loss: 5.5336\n",
      "Epoch [7/150], Train Loss: 4.8496, Val Loss: 5.4536\n",
      "Epoch [8/150], Train Loss: 4.6445, Val Loss: 5.3707\n",
      "Epoch [9/150], Train Loss: 4.4425, Val Loss: 5.2748\n",
      "Epoch [10/150], Train Loss: 4.2416, Val Loss: 5.1627\n",
      "Epoch [11/150], Train Loss: 4.0387, Val Loss: 5.0454\n",
      "Epoch [12/150], Train Loss: 3.8370, Val Loss: 4.9116\n",
      "Epoch [13/150], Train Loss: 3.6349, Val Loss: 4.7740\n",
      "Epoch [14/150], Train Loss: 3.4363, Val Loss: 4.6406\n",
      "Epoch [15/150], Train Loss: 3.2418, Val Loss: 4.5186\n",
      "Epoch [16/150], Train Loss: 3.0546, Val Loss: 4.4094\n",
      "Epoch [17/150], Train Loss: 2.8873, Val Loss: 4.3045\n",
      "Epoch [18/150], Train Loss: 2.7439, Val Loss: 4.2002\n",
      "Epoch [19/150], Train Loss: 2.6217, Val Loss: 4.1211\n",
      "Epoch [20/150], Train Loss: 2.5157, Val Loss: 4.0248\n",
      "Epoch [21/150], Train Loss: 2.4257, Val Loss: 3.9718\n",
      "Epoch [22/150], Train Loss: 2.3513, Val Loss: 3.9114\n",
      "Epoch [23/150], Train Loss: 2.2938, Val Loss: 3.8864\n",
      "Epoch [24/150], Train Loss: 2.2485, Val Loss: 3.7833\n",
      "Epoch [25/150], Train Loss: 2.2109, Val Loss: 3.8395\n",
      "Epoch [26/150], Train Loss: 2.1828, Val Loss: 3.6997\n",
      "Epoch [27/150], Train Loss: 2.1580, Val Loss: 3.7280\n",
      "Epoch [28/150], Train Loss: 2.1346, Val Loss: 3.6867\n",
      "Epoch [29/150], Train Loss: 2.1136, Val Loss: 3.6561\n",
      "Epoch [30/150], Train Loss: 2.0936, Val Loss: 3.6863\n",
      "Epoch [31/150], Train Loss: 2.0739, Val Loss: 3.5877\n",
      "Epoch [32/150], Train Loss: 2.0562, Val Loss: 3.6789\n",
      "Epoch [33/150], Train Loss: 2.0401, Val Loss: 3.5694\n",
      "Epoch [34/150], Train Loss: 2.0246, Val Loss: 3.6942\n",
      "Epoch [35/150], Train Loss: 2.0097, Val Loss: 3.5431\n",
      "Epoch [36/150], Train Loss: 1.9965, Val Loss: 3.6627\n",
      "Epoch [37/150], Train Loss: 1.9827, Val Loss: 3.5629\n",
      "Epoch [38/150], Train Loss: 1.9717, Val Loss: 3.5894\n",
      "Epoch [39/150], Train Loss: 1.9579, Val Loss: 3.6078\n",
      "Epoch [40/150], Train Loss: 1.9445, Val Loss: 3.5714\n",
      "Epoch [41/150], Train Loss: 1.9322, Val Loss: 3.5980\n",
      "Epoch [42/150], Train Loss: 1.9186, Val Loss: 3.5624\n",
      "Epoch [43/150], Train Loss: 1.9063, Val Loss: 3.6057\n",
      "Epoch [44/150], Train Loss: 1.8932, Val Loss: 3.5478\n",
      "Epoch [45/150], Train Loss: 1.8813, Val Loss: 3.5810\n",
      "Epoch [46/150], Train Loss: 1.8675, Val Loss: 3.5805\n",
      "Epoch [47/150], Train Loss: 1.8561, Val Loss: 3.5435\n",
      "Epoch [48/150], Train Loss: 1.8414, Val Loss: 3.6040\n",
      "Epoch [49/150], Train Loss: 1.8300, Val Loss: 3.5908\n",
      "Epoch [50/150], Train Loss: 1.8161, Val Loss: 3.5871\n",
      "Epoch [51/150], Train Loss: 1.8040, Val Loss: 3.5720\n",
      "Epoch [52/150], Train Loss: 1.7895, Val Loss: 3.5916\n",
      "Epoch [53/150], Train Loss: 1.7769, Val Loss: 3.5593\n",
      "Epoch [54/150], Train Loss: 1.7643, Val Loss: 3.5777\n",
      "Epoch [55/150], Train Loss: 1.7511, Val Loss: 3.5269\n",
      "Epoch [56/150], Train Loss: 1.7369, Val Loss: 3.5526\n",
      "Epoch [57/150], Train Loss: 1.7229, Val Loss: 3.5425\n",
      "Epoch [58/150], Train Loss: 1.7102, Val Loss: 3.4873\n",
      "Epoch [59/150], Train Loss: 1.6982, Val Loss: 3.5583\n",
      "Epoch [60/150], Train Loss: 1.6881, Val Loss: 3.4128\n",
      "Epoch [61/150], Train Loss: 1.6749, Val Loss: 3.5258\n",
      "Epoch [62/150], Train Loss: 1.6633, Val Loss: 3.4029\n",
      "Epoch [63/150], Train Loss: 1.6476, Val Loss: 3.4853\n",
      "Epoch [64/150], Train Loss: 1.6353, Val Loss: 3.4031\n",
      "Epoch [65/150], Train Loss: 1.6210, Val Loss: 3.4350\n",
      "Epoch [66/150], Train Loss: 1.6094, Val Loss: 3.4376\n",
      "Epoch [67/150], Train Loss: 1.5964, Val Loss: 3.3519\n",
      "Epoch [68/150], Train Loss: 1.5837, Val Loss: 3.4265\n",
      "Epoch [69/150], Train Loss: 1.5701, Val Loss: 3.3259\n",
      "Epoch [70/150], Train Loss: 1.5575, Val Loss: 3.3832\n",
      "Epoch [71/150], Train Loss: 1.5437, Val Loss: 3.3012\n",
      "Epoch [72/150], Train Loss: 1.5305, Val Loss: 3.3614\n",
      "Epoch [73/150], Train Loss: 1.5176, Val Loss: 3.2931\n",
      "Epoch [74/150], Train Loss: 1.5049, Val Loss: 3.3191\n",
      "Epoch [75/150], Train Loss: 1.4913, Val Loss: 3.2880\n",
      "Epoch [76/150], Train Loss: 1.4807, Val Loss: 3.3012\n",
      "Epoch [77/150], Train Loss: 1.4677, Val Loss: 3.2498\n",
      "Epoch [78/150], Train Loss: 1.4577, Val Loss: 3.3068\n",
      "Epoch [79/150], Train Loss: 1.4405, Val Loss: 3.2328\n",
      "Epoch [80/150], Train Loss: 1.4300, Val Loss: 3.2560\n",
      "Epoch [81/150], Train Loss: 1.4181, Val Loss: 3.2795\n",
      "Epoch [82/150], Train Loss: 1.4065, Val Loss: 3.2061\n",
      "Epoch [83/150], Train Loss: 1.3941, Val Loss: 3.2405\n",
      "Epoch [84/150], Train Loss: 1.3819, Val Loss: 3.1835\n",
      "Epoch [85/150], Train Loss: 1.3715, Val Loss: 3.2602\n",
      "Epoch [86/150], Train Loss: 1.3617, Val Loss: 3.2389\n",
      "Epoch [87/150], Train Loss: 1.3544, Val Loss: 3.2199\n",
      "Epoch [88/150], Train Loss: 1.3380, Val Loss: 3.2545\n",
      "Epoch [89/150], Train Loss: 1.3296, Val Loss: 3.2561\n",
      "Epoch [90/150], Train Loss: 1.3130, Val Loss: 3.2447\n",
      "Epoch [91/150], Train Loss: 1.3094, Val Loss: 3.2503\n",
      "Epoch [92/150], Train Loss: 1.2953, Val Loss: 3.2624\n",
      "Epoch [93/150], Train Loss: 1.2867, Val Loss: 3.2316\n",
      "Epoch [94/150], Train Loss: 1.2756, Val Loss: 3.1460\n",
      "Epoch [95/150], Train Loss: 1.2672, Val Loss: 3.1541\n",
      "Epoch [96/150], Train Loss: 1.2618, Val Loss: 3.2368\n",
      "Epoch [97/150], Train Loss: 1.2555, Val Loss: 3.3074 \n",
      "Early Stop Triggered!\n"
     ]
    }
   ],
   "source": [
    "model = Embedding(encoder_dim_1, encoder_dim_2, encoder_dim_3, embedding_dim, decoder_dim_1, decoder_dim_2, decoder_dim_3).to(DEVICE)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=embedding_lr, weight_decay=embedding_weight_decay)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "consecutive_val_loss_increases = 0\n",
    "max_consecutive_val_loss_increases = 3\n",
    "\n",
    "for epoch in range(embedding_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        input = data[0].to(DEVICE)\n",
    "        target = data[1].to(DEVICE)\n",
    "        output = model(input).to(DEVICE)\n",
    "\n",
    "        train_loss = criterion(output, target)\n",
    "        total_train_loss += train_loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            input = data[0].to(DEVICE)\n",
    "            target = data[1].to(DEVICE)\n",
    "            output = model(input).to(DEVICE)\n",
    "\n",
    "            val_loss = criterion(output, target)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    early_stop, consecutive_val_loss_increases = early_stop_1(val_losses, consecutive_val_loss_increases, max_consecutive_val_loss_increases)\n",
    "    # early_stop, best_val_loss, consecutive_val_loss_increases = early_stop_2(avg_val_loss, best_val_loss, consecutive_val_loss_increases, max_consecutive_val_loss_increases)\n",
    "    if early_stop:\n",
    "        print(f'Epoch [{epoch+1}/{embedding_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f} \\nEarly Stop Triggered!')\n",
    "        torch.save(model, f'../데이터/Checkpoint/embedding_tr_{train_ratio}_lr_{embedding_lr}_wd_{embedding_weight_decay}_batch_{embedding_batch}_epochs_{epoch+1}_e1_{encoder_dim_1}_e2_{encoder_dim_1}_e3_{encoder_dim_3}_emb_{embedding_dim}_d1{decoder_dim_1}_d2_{decoder_dim_2}_d3_{decoder_dim_3}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{embedding_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Validation Loss: 3.14601547901447\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_train_val_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../데이터/Checkpoint/embedding_tr_0.8_lr_1e-05_wd_0_batch_128_epochs_97_e1_128_e2_128_e3_512_emb_1024_d1512_d2_256_d3_128.pth')\n",
    "dataset = LSTM_Dataset(model, table_1, table_2, table_3, embedding_dim, lstm_window_size)\n",
    "\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Train Loss: 0.0579, Val Loss: 0.0221\n",
      "Epoch [2/150], Train Loss: 0.0162, Val Loss: 0.0095\n",
      "Epoch [3/150], Train Loss: 0.0064, Val Loss: 0.0048\n",
      "Epoch [4/150], Train Loss: 0.0037, Val Loss: 0.0074\n",
      "Epoch [5/150], Train Loss: 0.0027, Val Loss: 0.0030\n",
      "Epoch [6/150], Train Loss: 0.0019, Val Loss: 0.0020\n",
      "Epoch [7/150], Train Loss: 0.0015, Val Loss: 0.0021\n",
      "Epoch [8/150], Train Loss: 0.0011, Val Loss: 0.0015\n",
      "Epoch [9/150], Train Loss: 0.0009, Val Loss: 0.0011\n",
      "Epoch [10/150], Train Loss: 0.0008, Val Loss: 0.0025\n",
      "Epoch [11/150], Train Loss: 0.0007, Val Loss: 0.0023\n",
      "Epoch [12/150], Train Loss: 0.0007, Val Loss: 0.0016 \n",
      "Early Stop Triggered!\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(embedding_dim, lstm_hidden_dim)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lstm_lr, weight_decay=lstm_weight_decay)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "consecutive_val_loss_increases = 0\n",
    "max_consecutive_val_loss_increases = 3\n",
    "\n",
    "for epoch in range(lstm_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        src = data[0]\n",
    "        trg = data[1]\n",
    "\n",
    "        if trg.sum() != 0: \n",
    "            output, _, _ = model(src)\n",
    "            \n",
    "            train_loss = criterion(output, trg)\n",
    "            total_train_loss += train_loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0]\n",
    "            trg = data[1]\n",
    "\n",
    "            if trg.sum() != 0:\n",
    "                output, _, _ = model(src)\n",
    "\n",
    "                val_loss = criterion(output, trg)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # early_stop, consecutive_val_loss_increases = early_stop_1(val_losses, consecutive_val_loss_increases, max_consecutive_val_loss_increases)\n",
    "    early_stop, best_val_loss, consecutive_val_loss_increases = early_stop_2(avg_val_loss, best_val_loss, consecutive_val_loss_increases, max_consecutive_val_loss_increases)\n",
    "    if early_stop:\n",
    "        print(f'Epoch [{epoch+1}/{embedding_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f} \\nEarly Stop Triggered!')\n",
    "        torch.save(model, f'../데이터/Checkpoint/lstm_tr_{train_ratio}_lr_{lstm_lr}_wd_{lstm_weight_decay}_batch_{lstm_batch}_epochs_{epoch+1}_hdim_{lstm_hidden_dim}_ws_{lstm_window_size}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{lstm_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Validation Loss: 0.0010975338108145701\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_train_val_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../데이터/Checkpoint/embedding_tr_0.8_lr_1e-05_wd_0_batch_128_epochs_97_e1_128_e2_128_e3_512_emb_1024_d1512_d2_256_d3_128.pth')\n",
    "dataset = LSTM_Dataset(model, table_1, table_2, table_3, embedding_dim, nlinear_window_size)\n",
    "\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=nlinear_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=nlinear_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Train Loss: 0.3216, Val Loss: 0.1944\n",
      "Epoch [2/150], Train Loss: 0.1736, Val Loss: 0.1169\n",
      "Epoch [3/150], Train Loss: 0.1104, Val Loss: 0.0789\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m trg \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trg\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 19\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(src)\n\u001b[1;32m     21\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m criterion(output, trg)\n\u001b[1;32m     22\u001b[0m     total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Git/sci/SCI/코드/Model/NLinear.py:24\u001b[0m, in \u001b[0;36mNLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb_dim):\n\u001b[1;32m     23\u001b[0m     linear_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLinear[i](x[:,:,i])\n\u001b[0;32m---> 24\u001b[0m     bn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBatchNorm[i](linear_output)\n\u001b[1;32m     25\u001b[0m     output[:,i] \u001b[38;5;241m=\u001b[39m bn_output\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     27\u001b[0m output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m seq_last\u001b[38;5;241m.\u001b[39msqueeze()  \u001b[38;5;66;03m# num * emb_dim  \u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m   2479\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[1;32m   2480\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = NLinear(embedding_dim, nlinear_window_size).to(DEVICE)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=nlinear_lr, weight_decay=nlinear_weight_decay)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "consecutive_val_loss_increases = 0\n",
    "max_consecutive_val_loss_increases = 3\n",
    "\n",
    "for epoch in range(nlinear_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        src = data[0].to(DEVICE)\n",
    "        trg = data[1].to(DEVICE)\n",
    "\n",
    "        if trg.sum() != 0:\n",
    "            output = model(src)\n",
    "\n",
    "            train_loss = criterion(output, trg)\n",
    "            total_train_loss += train_loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0].to(DEVICE)\n",
    "            trg = data[1].to(DEVICE)\n",
    "\n",
    "            if trg.sum() != 0:\n",
    "                output = model(src)\n",
    "\n",
    "                val_loss = criterion(output, trg)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # early_stop, consecutive_val_loss_increases = early_stop_1(val_losses, consecutive_val_loss_increases, max_consecutive_val_loss_increases)\n",
    "    early_stop, best_val_loss, consecutive_val_loss_increases = early_stop_2(avg_val_loss, best_val_loss, consecutive_val_loss_increases, max_consecutive_val_loss_increases)\n",
    "    if early_stop:\n",
    "        print(f'Epoch [{epoch+1}/{embedding_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f} \\nEarly Stop Triggered!')\n",
    "        torch.save(model, f'../데이터/Checkpoint/nlinear_tr_{train_ratio}_lr_{nlinear_lr}_wd_{nlinear_weight_decay}_batch_{nlinear_batch}_epochs_{epoch+1}_emb_{embedding_dim}_ws_{nlinear_window_size}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{lstm_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../데이터/Checkpoint/embedding_tr_0.8_lr_1e-05_wd_0_batch_128_epochs_97_e1_128_e2_128_e3_512_emb_1024_d1512_d2_256_d3_128.pth')\n",
    "dataset = Attention_Dataset(model, table_1, table_2, table_3, embedding_dim, attention_window_size)\n",
    "\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=attention_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=attention_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMSeq2Seq(embedding_dim, attention_hidden_dim, 1).to(DEVICE)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=attention_lr, weight_decay=attention_weight_decay)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "consecutive_val_loss_increases = 0\n",
    "max_consecutive_val_loss_increases = 3\n",
    "\n",
    "for epoch in range(attention_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        src = data[0][0].to(DEVICE)\n",
    "        max_len = data[1][0].to(DEVICE)\n",
    "        anw = data[2][0].to(DEVICE)\n",
    "        trg = data[3][0].to(DEVICE)\n",
    "        \n",
    "        if len(anw)==0:\n",
    "            continue\n",
    "    \n",
    "        # dong_loss = 0\n",
    "\n",
    "        for index in anw:\n",
    "            output = model(src, index, max_len)\n",
    "\n",
    "            train_loss = criterion(output, trg[index])\n",
    "            total_train_loss += train_loss.item()\n",
    "            # dong_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "        # # dong_loss /= len(anw)\n",
    "        # dong_loss = torch.tensor(dong_loss, requires_grad=True).to(DEVICE)\n",
    "        # dong_loss.backward()\n",
    "        # optimizer.step()\n",
    "            \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0][0].to(DEVICE)\n",
    "            max_len = data[1][0].to(DEVICE)\n",
    "            anw = data[2][0].to(DEVICE)\n",
    "            trg = data[3][0].to(DEVICE)\n",
    "\n",
    "            if len(anw)==0:\n",
    "                continue\n",
    "\n",
    "            for index in anw:\n",
    "                output = model(src, index, max_len)\n",
    "\n",
    "                val_loss = criterion(output, trg[index])\n",
    "                total_val_loss += val_loss.item()\n",
    "                \n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "            \n",
    "    # early_stop, consecutive_val_loss_increases = early_stop_1(val_losses, consecutive_val_loss_increases, max_consecutive_val_loss_increases)\n",
    "    early_stop, best_val_loss, consecutive_val_loss_increases = early_stop_2(avg_val_loss, best_val_loss, consecutive_val_loss_increases, max_consecutive_val_loss_increases)\n",
    "    if early_stop:\n",
    "        print(f'Epoch [{epoch+1}/{embedding_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f} \\nEarly Stop Triggered!')\n",
    "        torch.save(model, f'../데이터/Checkpoint/attention_tr_{train_ratio}_lr_{attention_lr}_wd_{attention_weight_decay}_batch_{attention_batch}_epochs_{epoch+1}_hdim_{attention_hidden_dim}_ws_{attention_window_size}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{lstm_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../데이터/Checkpoint/embedding_tr_0.8_lr_1e-05_wd_0_batch_128_epochs_97_e1_128_e2_128_e3_512_emb_1024_d1512_d2_256_d3_128.pth')\n",
    "dataset = LSTM_Dataset(model, table_1, table_2, table_3, embedding_dim, transformer_window_size)\n",
    "\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=transformer_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=transformer_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     26\u001b[0m         train_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 27\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[1;32m     30\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     adam(\n\u001b[1;32m    164\u001b[0m         params_with_grad,\n\u001b[1;32m    165\u001b[0m         grads,\n\u001b[1;32m    166\u001b[0m         exp_avgs,\n\u001b[1;32m    167\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    168\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    169\u001b[0m         state_steps,\n\u001b[1;32m    170\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    171\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    172\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    173\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    174\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    175\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    176\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    177\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    178\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    179\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    180\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    181\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    182\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m func(params,\n\u001b[1;32m    312\u001b[0m      grads,\n\u001b[1;32m    313\u001b[0m      exp_avgs,\n\u001b[1;32m    314\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    315\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    316\u001b[0m      state_steps,\n\u001b[1;32m    317\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    318\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    319\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    320\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    321\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    322\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    323\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    324\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    325\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    326\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    327\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Transformer(embedding_dim, transformer_window_size, 1, 2, 2).to(DEVICE)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=transformer_lr, weight_decay=transformer_weight_decay)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "consecutive_val_loss_increases = 0\n",
    "max_consecutive_val_loss_increases = 3\n",
    "\n",
    "for epoch in range(transformer_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        src = data[0].to(DEVICE)\n",
    "        trg = data[1].to(DEVICE)\n",
    "\n",
    "        if (trg[0] != 0):\n",
    "            src_mask = model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "            output = model(src, src_mask)\n",
    "\n",
    "            train_loss = criterion(output[0], trg[0])\n",
    "            total_train_loss += train_loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0].to(DEVICE)\n",
    "            trg = data[1].to(DEVICE)\n",
    "\n",
    "            if (trg[0] != 0):\n",
    "                src_mask = model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "                output = model(src, src_mask)\n",
    "\n",
    "                val_loss = criterion(output[0], trg[0])\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # early_stop, consecutive_val_loss_increases = early_stop_1(val_losses, consecutive_val_loss_increases, max_consecutive_val_loss_increases)\n",
    "    early_stop, best_val_loss, consecutive_val_loss_increases = early_stop_2(avg_val_loss, best_val_loss, consecutive_val_loss_increases, max_consecutive_val_loss_increases)\n",
    "    if early_stop:\n",
    "        print(f'Epoch [{epoch+1}/{embedding_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f} \\nEarly Stop Triggered!')\n",
    "        torch.save(model, f'../데이터/Checkpoint/transformer_tr_{train_ratio}_lr_{transformer_lr}_wd_{transformer_weight_decay}_batch_{transformer_batch}_epochs_{epoch+1}_ws_{transformer_window_size}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{lstm_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
