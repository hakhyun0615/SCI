{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Model\\ODEF.py:145: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xe (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  def forward(self, z0, t=Tensor([0., 1.]), return_whole_sequence=False):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from Dataset.Economy_Dataset import Economy_Dataset\n",
    "from Dataset.LSTM_Transaction_Dataset import LSTM_Transaction_Dataset\n",
    "from Dataset.ODE_Transaction_Dataset import ODE_Transaction_Dataset\n",
    "\n",
    "from Model.LSTM import LSTM\n",
    "from Model.NODE import NODE\n",
    "from Model.ODERNN import *\n",
    "from Model.ODEF import *\n",
    "\n",
    "from utils import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 is available\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device} is available')\n",
    "\n",
    "transaction_df = pd.read_csv('../데이터/Transaction/transaction_all.csv')\n",
    "economy_df = pd.read_excel('../데이터/Economy/economy_all.xlsx')\n",
    "\n",
    "transaction_df, economy_df = preprocess(transaction_df, economy_df, window_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경제 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1, Training Loss: 0.002730900188907981, Validation Loss: 0.3143093644015106\n",
      "Epoch 1/1, Training Loss: 0.0027428490575402975, Validation Loss: 0.31638226120186774\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf') \n",
    "for epoch in range(num_epochs + 1):\n",
    "    model.train()\n",
    "    for batch_idx, samples in enumerate(economy_train_loader):\n",
    "        economy_x_train, economy_y_train = samples\n",
    "\n",
    "        prediction, hidden = model(economy_x_train)\n",
    "        cost = criterion(prediction, economy_y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, samples in enumerate(economy_val_loader):\n",
    "            economy_x_val, economy_y_val = samples\n",
    "\n",
    "            prediction, hidden = model(economy_x_val)\n",
    "            loss = criterion(prediction, economy_y_val)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(economy_val_loader)\n",
    "    print(f'Epoch {epoch}/{num_epochs}, Training Loss: {cost.item()}, Validation Loss: {val_loss}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), '../데이터/Checkpoint/best_rnn_economy_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 부동산 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5\n",
    "hidden_size = 16\n",
    "output_size = 1\n",
    "\n",
    "lr = 1e-4\n",
    "num_epochs = 300\n",
    "\n",
    "model = LSTM(input_size=input_size, hidden_size=hidden_size, output_size=output_size, device=device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\LSTM_Transaction_Dataset.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'].astype(str), format='%Y%m')\n",
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\LSTM_Transaction_Dataset.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'].astype(str), format='%Y%m')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 크기 : torch.Size([1, 5])\n",
      "Y 크기 : torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "train_dataset = LSTM_Transaction_Dataset(transaction_df[transaction_df['계약년월']//100 != 2022])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "val_dataset = LSTM_Transaction_Dataset(transaction_df[transaction_df['계약년월']//100 == 2022])\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "for x,y in train_loader:\n",
    "      print(\"X 크기 : {}\".format(x.shape))\n",
    "      print(\"Y 크기 : {}\".format(y.shape))\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.6117407218530259, Validation Loss: 14.002079031714377\n",
      "Epoch 1/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5923233684806786, Validation Loss: 13.707394255354197\n",
      "Epoch 2/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5834284294861106, Validation Loss: 13.429922815436168\n",
      "Epoch 3/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5787264649865445, Validation Loss: 13.457700884191597\n",
      "Epoch 4/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5758154007587396, Validation Loss: 12.538686750355415\n",
      "Epoch 5/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5744807219217634, Validation Loss: 12.282869978535704\n",
      "Epoch 6/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5722420975659814, Validation Loss: 12.30764426834156\n",
      "Epoch 7/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5697369977755824, Validation Loss: 12.228170939297572\n",
      "Epoch 8/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5692159291585782, Validation Loss: 11.970380046447199\n",
      "Epoch 9/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5697781475308579, Validation Loss: 11.377418701379005\n",
      "Epoch 10/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5682551893608588, Validation Loss: 10.950317474479087\n",
      "Epoch 11/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5707848679418622, Validation Loss: 9.893340522070309\n",
      "Epoch 12/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5671033405958515, Validation Loss: 9.299545680840193\n",
      "Epoch 13/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5668721741157793, Validation Loss: 8.60439497930829\n",
      "Epoch 14/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5644995108022146, Validation Loss: 8.065173618442838\n",
      "Epoch 15/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5629150157043782, Validation Loss: 7.680051389303875\n",
      "Epoch 16/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5619491079894939, Validation Loss: 7.5359957479418185\n",
      "Epoch 17/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.561499788077338, Validation Loss: 7.039461940149\n",
      "Epoch 18/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5613354716570544, Validation Loss: 6.531791573038989\n",
      "Epoch 19/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5610697109994786, Validation Loss: 6.280335029410089\n",
      "Epoch 20/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5594366628437094, Validation Loss: 6.218631503779647\n",
      "Epoch 21/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5584855721877305, Validation Loss: 6.334976783233368\n",
      "Epoch 22/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5577206828477839, Validation Loss: 6.681633438028452\n",
      "Epoch 23/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.557577998618524, Validation Loss: 7.240775331295372\n",
      "Epoch 24/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5585642288480014, Validation Loss: 7.686693115321432\n",
      "Epoch 25/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5639658289661272, Validation Loss: 6.728268343905234\n",
      "Epoch 26/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5685774245908916, Validation Loss: 6.761223976283961\n",
      "Epoch 27/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5657068081889208, Validation Loss: 6.576452981692055\n",
      "Epoch 28/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5629102060110567, Validation Loss: 6.648571825168925\n",
      "Epoch 29/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5615354433708445, Validation Loss: 6.850125936178406\n",
      "Epoch 30/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5600959411788823, Validation Loss: 6.963705472065972\n",
      "Epoch 31/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5588543030921809, Validation Loss: 6.917132187864014\n",
      "Epoch 32/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5579714522213681, Validation Loss: 6.716990414130006\n",
      "Epoch 33/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5568408398077264, Validation Loss: 6.1283846329951865\n",
      "Epoch 34/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5569682678594008, Validation Loss: 6.780330227128319\n",
      "Epoch 35/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5581829605042643, Validation Loss: 6.6084818405946795\n",
      "Epoch 36/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.558634902421481, Validation Loss: 6.509974830395624\n",
      "Epoch 37/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5578862526567215, Validation Loss: 5.850356464247691\n",
      "Epoch 38/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.558502855972203, Validation Loss: 5.844025174629035\n",
      "Epoch 39/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5579080978380017, Validation Loss: 5.699019742648335\n",
      "Epoch 40/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5568455864991284, Validation Loss: 5.733735313629359\n",
      "Epoch 41/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5561259612227188, Validation Loss: 5.4342909198279825\n",
      "Epoch 42/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5550681610862924, Validation Loss: 5.197356510637832\n",
      "Epoch 43/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5554582324210398, Validation Loss: 5.612435646350885\n",
      "Epoch 44/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5561882461961528, Validation Loss: 5.424942292331633\n",
      "Epoch 45/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5557151459728892, Validation Loss: 5.157875416757208\n",
      "Epoch 46/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5553512265199396, Validation Loss: 4.920678012695433\n",
      "Epoch 47/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5548880400797565, Validation Loss: 4.653627770004271\n",
      "Epoch 48/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5541702373756509, Validation Loss: 4.215984574622228\n",
      "Epoch 49/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5536077560290208, Validation Loss: 4.0908276983585194\n",
      "Epoch 50/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5531564871725385, Validation Loss: 4.071921332868164\n",
      "Epoch 51/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.553399733409609, Validation Loss: 4.065832877653355\n",
      "Epoch 52/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5534937509116822, Validation Loss: 4.0640912335656045\n",
      "Epoch 53/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5533752483440211, Validation Loss: 4.075426327592878\n",
      "Epoch 54/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5532883575000673, Validation Loss: 4.088393974424203\n",
      "Epoch 55/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5532205851222606, Validation Loss: 4.103422380943013\n",
      "Epoch 56/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5531406650955496, Validation Loss: 4.12657845692467\n",
      "Epoch 57/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5530109999255285, Validation Loss: 4.156389493052361\n",
      "Epoch 58/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.552848889084839, Validation Loss: 4.194852583030805\n",
      "Epoch 59/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5527068954403709, Validation Loss: 4.2536080398791745\n",
      "Epoch 60/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.552575073432764, Validation Loss: 4.326306917643264\n",
      "Epoch 61/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5524077513483884, Validation Loss: 4.388333240916674\n",
      "Epoch 62/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5522061546399885, Validation Loss: 4.860322938516092\n",
      "Epoch 63/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5518314553518434, Validation Loss: 4.95138360703484\n",
      "Epoch 64/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5517105876026339, Validation Loss: 5.007866541733693\n",
      "Epoch 65/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5515914753269249, Validation Loss: 5.0510073511674545\n",
      "Epoch 66/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5514713886513617, Validation Loss: 5.07637948739456\n",
      "Epoch 67/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5513526638101703, Validation Loss: 5.09612499689397\n",
      "Epoch 68/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5512583015572594, Validation Loss: 5.109473159058799\n",
      "Epoch 69/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5511749340919747, Validation Loss: 5.117971704538309\n",
      "Epoch 70/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5511039039126175, Validation Loss: 5.117584251006483\n",
      "Epoch 71/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5510309603563077, Validation Loss: 5.111731446651416\n",
      "Epoch 72/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5509667486997211, Validation Loss: 5.0975710374103524\n",
      "Epoch 73/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5509134155880521, Validation Loss: 5.0776337977432595\n",
      "Epoch 74/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5508782898967748, Validation Loss: 5.051162038881251\n",
      "Epoch 75/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5509012984709433, Validation Loss: 5.015469110546044\n",
      "Epoch 76/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5510039605352656, Validation Loss: 5.085250961461504\n",
      "Epoch 77/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5515549015701732, Validation Loss: 5.429634505901573\n",
      "Epoch 78/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5528733649984537, Validation Loss: 5.216352005160786\n",
      "Epoch 79/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5543481909693331, Validation Loss: 5.101215980473903\n",
      "Epoch 80/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5536368067335296, Validation Loss: 5.025674665786607\n",
      "Epoch 81/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5531754418479965, Validation Loss: 4.955694157303908\n",
      "Epoch 82/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5528353464265154, Validation Loss: 4.903709139512639\n",
      "Epoch 83/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5525458681836848, Validation Loss: 4.862891433035044\n",
      "Epoch 84/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5522865564941936, Validation Loss: 4.825722494635704\n",
      "Epoch 85/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5520471452239564, Validation Loss: 4.7880017248250715\n",
      "Epoch 86/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5518244527883973, Validation Loss: 4.747957973013047\n",
      "Epoch 87/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.551615860307135, Validation Loss: 4.705744596194331\n",
      "Epoch 88/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5514195358622475, Validation Loss: 4.661989041644186\n",
      "Epoch 89/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5512342115677965, Validation Loss: 4.617708778795108\n",
      "Epoch 90/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5510582839833025, Validation Loss: 4.5740907119529455\n",
      "Epoch 91/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5508923167622788, Validation Loss: 4.533508691903917\n",
      "Epoch 92/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5507407548374041, Validation Loss: 4.495047149654183\n",
      "Epoch 93/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.55059750811775, Validation Loss: 4.465332393175606\n",
      "Epoch 94/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5504697038901035, Validation Loss: 4.437270858445805\n",
      "Epoch 95/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5503512857180508, Validation Loss: 4.414182603211259\n",
      "Epoch 96/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5502403012064028, Validation Loss: 4.401858561045782\n",
      "Epoch 97/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5501460604161198, Validation Loss: 4.394549999380153\n",
      "Epoch 98/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.550062673732458, Validation Loss: 4.390929576922685\n",
      "Epoch 99/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5499895304512193, Validation Loss: 4.3910976666486725\n",
      "Epoch 100/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5499213987215217, Validation Loss: 4.394362491056714\n",
      "Epoch 101/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5498566728628681, Validation Loss: 4.399038029037096\n",
      "Epoch 102/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5497971240277417, Validation Loss: 4.404020071370722\n",
      "Epoch 103/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5497440280598193, Validation Loss: 4.409049566222145\n",
      "Epoch 104/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5497037225790083, Validation Loss: 4.41387826276319\n",
      "Epoch 105/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5496894209904527, Validation Loss: 4.416843401825397\n",
      "Epoch 106/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5497046236753254, Validation Loss: 4.414633757829978\n",
      "Epoch 107/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5497093132268392, Validation Loss: 4.4073601683477595\n",
      "Epoch 108/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5496728445210852, Validation Loss: 4.397566580267525\n",
      "Epoch 109/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5496255411421502, Validation Loss: 4.3847192341794745\n",
      "Epoch 110/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5495926281774919, Validation Loss: 4.366803996007582\n",
      "Epoch 111/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5495764948109608, Validation Loss: 4.345638393026758\n",
      "Epoch 112/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5495752482532779, Validation Loss: 4.321941412071184\n",
      "Epoch 113/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5495869761998738, Validation Loss: 4.297163859201176\n",
      "Epoch 114/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5496083434126247, Validation Loss: 4.272365994424664\n",
      "Epoch 115/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5496341747413847, Validation Loss: 4.249179441122242\n",
      "Epoch 116/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5496586540945233, Validation Loss: 4.2300854005590365\n",
      "Epoch 117/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5496772105296529, Validation Loss: 4.2158845623139545\n",
      "Epoch 118/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5496907763839635, Validation Loss: 4.208005963597417\n",
      "Epoch 119/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5497056441361352, Validation Loss: 4.2073575828841445\n",
      "Epoch 120/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5497282711718939, Validation Loss: 4.213115460395463\n",
      "Epoch 121/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5497607006865876, Validation Loss: 4.22529584545035\n",
      "Epoch 122/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5498041192665479, Validation Loss: 4.242837953559584\n",
      "Epoch 123/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5498615339946169, Validation Loss: 4.265987475530894\n",
      "Epoch 124/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5499397431524343, Validation Loss: 4.294700851595084\n",
      "Epoch 125/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5500468934564919, Validation Loss: 4.33028563482624\n",
      "Epoch 126/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5501911803236863, Validation Loss: 4.36956871774795\n",
      "Epoch 127/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5503967329371581, Validation Loss: 4.426840146187196\n",
      "Epoch 128/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5506657217705391, Validation Loss: 4.523329771417287\n",
      "Epoch 129/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5510101601599131, Validation Loss: 4.612393755821398\n",
      "Epoch 130/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5512209036373509, Validation Loss: 4.689179233440966\n",
      "Epoch 131/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5513680180904207, Validation Loss: 4.745677595649011\n",
      "Epoch 132/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5514951789993272, Validation Loss: 4.781787060710868\n",
      "Epoch 133/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5516000153415301, Validation Loss: 4.809373860158379\n",
      "Epoch 134/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5516779616328279, Validation Loss: 4.823263287551252\n",
      "Epoch 135/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.551723367036481, Validation Loss: 4.8237685862956345\n",
      "Epoch 136/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.551743220639496, Validation Loss: 4.812538803712053\n",
      "Epoch 137/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5517459009472522, Validation Loss: 4.791404592947662\n",
      "Epoch 138/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5517456645146697, Validation Loss: 4.762586489504743\n",
      "Epoch 139/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5517510956696007, Validation Loss: 4.725989265174927\n",
      "Epoch 140/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5517181567117327, Validation Loss: 4.673406945649885\n",
      "Epoch 141/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5514930455043403, Validation Loss: 4.587143573774328\n",
      "Epoch 142/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.551015734931078, Validation Loss: 4.429401855987901\n",
      "Epoch 143/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.550423332565278, Validation Loss: 4.275694425243656\n",
      "Epoch 144/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5502769176939117, Validation Loss: 4.172995637280297\n",
      "Epoch 145/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5502639130902782, Validation Loss: 4.116060916529091\n",
      "Epoch 146/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.55031073986107, Validation Loss: 4.076175876000556\n",
      "Epoch 147/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.550331196316758, Validation Loss: 4.045088957748649\n",
      "Epoch 148/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5503208313976061, Validation Loss: 4.019682048834003\n",
      "Epoch 149/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5502861488370479, Validation Loss: 3.9979364147009844\n",
      "Epoch 150/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5502395407676564, Validation Loss: 3.978881141245087\n",
      "Epoch 151/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.550187681169363, Validation Loss: 3.963055556726953\n",
      "Epoch 152/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5501345286818784, Validation Loss: 3.9517550084819435\n",
      "Epoch 153/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5500819922690972, Validation Loss: 3.9474776239070035\n",
      "Epoch 154/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5500301144310868, Validation Loss: 3.9516334448524306\n",
      "Epoch 155/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5499751577053731, Validation Loss: 3.963924722719605\n",
      "Epoch 156/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5499148279657807, Validation Loss: 3.9817148092060557\n",
      "Epoch 157/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5498517768940944, Validation Loss: 4.001136837812131\n",
      "Epoch 158/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5497904132743937, Validation Loss: 4.020061328859226\n",
      "Epoch 159/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5497328889511562, Validation Loss: 4.037039747774591\n",
      "Epoch 160/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5496793542905062, Validation Loss: 4.051419759612813\n",
      "Epoch 161/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5496286696231264, Validation Loss: 4.061981772851223\n",
      "Epoch 162/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5495804346492752, Validation Loss: 4.067907684802187\n",
      "Epoch 163/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5495342128970762, Validation Loss: 4.0683512595842535\n",
      "Epoch 164/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5494874952439974, Validation Loss: 4.063260430084104\n",
      "Epoch 165/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5494431087548746, Validation Loss: 4.0535155734282\n",
      "Epoch 166/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5494008708127137, Validation Loss: 4.0400727403878705\n",
      "Epoch 167/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5493593489218523, Validation Loss: 4.024911125811005\n",
      "Epoch 168/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5493178885138071, Validation Loss: 4.009355552509151\n",
      "Epoch 169/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5492770968907206, Validation Loss: 3.993256718137001\n",
      "Epoch 170/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5492358781236478, Validation Loss: 3.975507734500436\n",
      "Epoch 171/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5491926500626977, Validation Loss: 3.9561280994626524\n",
      "Epoch 172/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5491467805442591, Validation Loss: 3.9361225016282573\n",
      "Epoch 173/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5490986489342174, Validation Loss: 3.9164691460971524\n",
      "Epoch 174/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5490438341844722, Validation Loss: 3.8947763304164584\n",
      "Epoch 175/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5489787867451015, Validation Loss: 3.8685397345031727\n",
      "Epoch 176/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5489122602673631, Validation Loss: 3.841032454581654\n",
      "Epoch 177/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5488582964313696, Validation Loss: 3.8177443249879794\n",
      "Epoch 178/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5488203829074633, Validation Loss: 3.80066435318684\n",
      "Epoch 179/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.548797903261198, Validation Loss: 3.7886103630007386\n",
      "Epoch 180/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5487897271973319, Validation Loss: 3.780163052851637\n",
      "Epoch 181/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5487995642019885, Validation Loss: 3.773821672991529\n",
      "Epoch 182/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5488322347086347, Validation Loss: 3.768031021652069\n",
      "Epoch 183/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5488914490968545, Validation Loss: 3.7618196288298\n",
      "Epoch 184/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5490138568400474, Validation Loss: 3.7534737949634738\n",
      "Epoch 185/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5491668728768523, Validation Loss: 3.7410818575568463\n",
      "Epoch 186/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5492691986740059, Validation Loss: 3.7250890633037193\n",
      "Epoch 187/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5493248617929457, Validation Loss: 3.7063835430943155\n",
      "Epoch 188/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5493338137207276, Validation Loss: 3.688264298584027\n",
      "Epoch 189/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5493259192641159, Validation Loss: 3.6714926763317934\n",
      "Epoch 190/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5493161842612773, Validation Loss: 3.6554456693007875\n",
      "Epoch 191/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5493056189458239, Validation Loss: 3.6399207950309145\n",
      "Epoch 192/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5492932048160524, Validation Loss: 3.62490407940544\n",
      "Epoch 193/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5492769599814652, Validation Loss: 3.610014465030231\n",
      "Epoch 194/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5492553151367227, Validation Loss: 3.5952443169376065\n",
      "Epoch 195/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5492281895109615, Validation Loss: 3.5799561170584164\n",
      "Epoch 196/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5491983804019211, Validation Loss: 3.5630248033755567\n",
      "Epoch 197/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5491695217552838, Validation Loss: 3.543282693853081\n",
      "Epoch 198/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5491512750981964, Validation Loss: 3.520085436678808\n",
      "Epoch 199/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5491461506648342, Validation Loss: 3.4949853554165458\n",
      "Epoch 200/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5491440347323767, Validation Loss: 3.4718876679101287\n",
      "Epoch 201/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5491714290057285, Validation Loss: 3.462938217144266\n",
      "Epoch 202/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5492876578785856, Validation Loss: 3.4910917740173697\n",
      "Epoch 203/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5493735643027808, Validation Loss: 3.5637115866612574\n",
      "Epoch 204/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5494262986837516, Validation Loss: 3.6667389132777566\n",
      "Epoch 205/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5494520116356568, Validation Loss: 3.7865411800870117\n",
      "Epoch 206/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5494576904989773, Validation Loss: 3.9160056409739563\n",
      "Epoch 207/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5494537801001002, Validation Loss: 4.048648570663584\n",
      "Epoch 208/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5494393164714442, Validation Loss: 4.171702756717554\n",
      "Epoch 209/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5494010641376424, Validation Loss: 4.2659680244886315\n",
      "Epoch 210/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5493271802546497, Validation Loss: 4.321639796016965\n",
      "Epoch 211/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5492213551640154, Validation Loss: 4.346927171832363\n",
      "Epoch 212/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5491023786667769, Validation Loss: 4.355250699556781\n",
      "Epoch 213/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5489756444850814, Validation Loss: 4.358723987892701\n",
      "Epoch 214/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5488455064541202, Validation Loss: 4.361307167692445\n",
      "Epoch 215/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5487312407216015, Validation Loss: 4.364802274826219\n",
      "Epoch 216/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5486323169316281, Validation Loss: 4.360876406223848\n",
      "Epoch 217/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5485302603631943, Validation Loss: 4.352721118772718\n",
      "Epoch 218/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5484283180880846, Validation Loss: 4.344282974817583\n",
      "Epoch 219/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5483355624086317, Validation Loss: 4.335781867296876\n",
      "Epoch 220/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5482509799088878, Validation Loss: 4.3271111456932205\n",
      "Epoch 221/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5481736385841529, Validation Loss: 4.31790492473332\n",
      "Epoch 222/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5481012381598084, Validation Loss: 4.307839594105243\n",
      "Epoch 223/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5480320145103214, Validation Loss: 4.297135082389435\n",
      "Epoch 224/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.547965003688713, Validation Loss: 4.286455003960378\n",
      "Epoch 225/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5479006481575024, Validation Loss: 4.276222503113453\n",
      "Epoch 226/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5478395127328538, Validation Loss: 4.266734752000822\n",
      "Epoch 227/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5477810065187652, Validation Loss: 4.258118276971897\n",
      "Epoch 228/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5477249910589753, Validation Loss: 4.2506105354683\n",
      "Epoch 229/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5476705559457455, Validation Loss: 4.243967618855152\n",
      "Epoch 230/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5476174857015996, Validation Loss: 4.23920244238244\n",
      "Epoch 231/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5475648286546114, Validation Loss: 4.236549571900392\n",
      "Epoch 232/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5475119599650495, Validation Loss: 4.235821657772535\n",
      "Epoch 233/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5474586335750822, Validation Loss: 4.235937816136104\n",
      "Epoch 234/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.547410694450829, Validation Loss: 4.236917410493584\n",
      "Epoch 235/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5473731169190255, Validation Loss: 4.238967691745523\n",
      "Epoch 236/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5473426627365117, Validation Loss: 4.243424925129846\n",
      "Epoch 237/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5473165955580134, Validation Loss: 4.251494855296469\n",
      "Epoch 238/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5472947587610306, Validation Loss: 4.265259359287373\n",
      "Epoch 239/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5472774565646563, Validation Loss: 4.2851341424329785\n",
      "Epoch 240/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.547264405196389, Validation Loss: 4.309596544251803\n",
      "Epoch 241/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5472557760840725, Validation Loss: 4.335501144712666\n",
      "Epoch 242/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5472499299468028, Validation Loss: 4.359887446964155\n",
      "Epoch 243/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5472453051611446, Validation Loss: 4.3798816905620415\n",
      "Epoch 244/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5472406637109217, Validation Loss: 4.397000598096113\n",
      "Epoch 245/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5472364496689903, Validation Loss: 4.409728637562579\n",
      "Epoch 246/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5472331486024412, Validation Loss: 4.41781209503012\n",
      "Epoch 247/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5472330197191224, Validation Loss: 4.423306804198375\n",
      "Epoch 248/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5472371944525078, Validation Loss: 4.428346601534805\n",
      "Epoch 249/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5472438777625418, Validation Loss: 4.433597156081008\n",
      "Epoch 250/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5472514941988408, Validation Loss: 4.437824159894551\n",
      "Epoch 251/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5472591313344719, Validation Loss: 4.440723123472897\n",
      "Epoch 252/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5472670921390772, Validation Loss: 4.4422694236815055\n",
      "Epoch 253/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5472767840524099, Validation Loss: 4.4410060299978635\n",
      "Epoch 254/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5472889773255255, Validation Loss: 4.437017233654669\n",
      "Epoch 255/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5473024287149736, Validation Loss: 4.430166933030297\n",
      "Epoch 256/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5473160926279353, Validation Loss: 4.419926820412143\n",
      "Epoch 257/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5473298300339676, Validation Loss: 4.404810840910906\n",
      "Epoch 258/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5473432875943708, Validation Loss: 4.384057292404815\n",
      "Epoch 259/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5473561272589145, Validation Loss: 4.359708155798967\n",
      "Epoch 260/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5473675062052923, Validation Loss: 4.337615956303803\n",
      "Epoch 261/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5473920695361975, Validation Loss: 4.320974570212447\n",
      "Epoch 262/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5474851375444568, Validation Loss: 4.347847229779895\n",
      "Epoch 263/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5476428286460828, Validation Loss: 4.412904215489808\n",
      "Epoch 264/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5476460946001401, Validation Loss: 4.474541600319574\n",
      "Epoch 265/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.547628311288955, Validation Loss: 4.5323413913249855\n",
      "Epoch 266/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5476058558247306, Validation Loss: 4.596129799030367\n",
      "Epoch 267/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5475633954822382, Validation Loss: 4.641245993025521\n",
      "Epoch 268/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5475329770394066, Validation Loss: 4.6611662207142235\n",
      "Epoch 269/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5475274279142074, Validation Loss: 4.6766781763616265\n",
      "Epoch 270/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5475292132312427, Validation Loss: 4.688566635662953\n",
      "Epoch 271/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5475373650208616, Validation Loss: 4.687115674880016\n",
      "Epoch 272/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5475446726909364, Validation Loss: 4.6913849928176266\n",
      "Epoch 273/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5475561006409221, Validation Loss: 4.7007655165265145\n",
      "Epoch 274/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5475720347674985, Validation Loss: 4.7092695577583825\n",
      "Epoch 275/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.547593054877902, Validation Loss: 4.716026678633186\n",
      "Epoch 276/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5476191002922963, Validation Loss: 4.719114778366641\n",
      "Epoch 277/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5476501205100773, Validation Loss: 4.719165995856238\n",
      "Epoch 278/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5476856499856466, Validation Loss: 4.716973827220437\n",
      "Epoch 279/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5477183859417353, Validation Loss: 4.713556391575757\n",
      "Epoch 280/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5477395020668779, Validation Loss: 4.707628338008711\n",
      "Epoch 281/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5477836605759255, Validation Loss: 4.704363471995482\n",
      "Epoch 282/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5478440222750393, Validation Loss: 4.704405244852525\n",
      "Epoch 283/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5479049339933877, Validation Loss: 4.704026392104029\n",
      "Epoch 284/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5479577930577971, Validation Loss: 4.699337136232276\n",
      "Epoch 285/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5480102499006154, Validation Loss: 4.686039112048088\n",
      "Epoch 286/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5480481956905752, Validation Loss: 4.664671869465833\n",
      "Epoch 287/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5480573601527392, Validation Loss: 4.652032563830434\n",
      "Epoch 288/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5481038768885529, Validation Loss: 4.613627233654225\n",
      "Epoch 289/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5481838417481794, Validation Loss: 4.5941683674772\n",
      "Epoch 290/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5482150332152517, Validation Loss: 4.582520744206244\n",
      "Epoch 291/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5482296574030419, Validation Loss: 4.573785452103466\n",
      "Epoch 292/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5482732335019369, Validation Loss: 4.576868680472948\n",
      "Epoch 293/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5484112242698254, Validation Loss: 4.613202323160008\n",
      "Epoch 294/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.548908077425671, Validation Loss: 4.641454707459062\n",
      "Epoch 295/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5494809035655301, Validation Loss: 4.678388410803094\n",
      "Epoch 296/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5497772823342822, Validation Loss: 4.72402296145737\n",
      "Epoch 297/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5499220452617544, Validation Loss: 4.772263836663602\n",
      "Epoch 298/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5500305772743824, Validation Loss: 4.823929261682785\n",
      "Epoch 299/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5501473441069286, Validation Loss: 4.87648270535987\n",
      "Epoch 300/300, LR: 0.0001, Hidden size: 16, Training Loss: 0.5504065009130252, Validation Loss: 4.920536970421446\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs+1):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    \n",
    "    for x_train, y_train in train_loader:\n",
    "        x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "\n",
    "        prediction, hidden = model(x_train)\n",
    "        cost = criterion(prediction, y_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += cost.item()\n",
    "        \n",
    "    total_train_loss /= len(train_loader)\n",
    "    train_losses.append(total_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "\n",
    "            prediction, _ = model(x_val)\n",
    "            loss = criterion(prediction, y_val)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch}/{num_epochs}, LR: {lr}, Hidden size: {hidden_size}, Training Loss: {total_train_loss}, Validation Loss: {val_loss}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_path = f\"../데이터/Checkpoint/lstm_transaction_model.pth\"\n",
    "        torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_df = pd.DataFrame({\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses\n",
    "})\n",
    "\n",
    "losses_df.to_excel('lstm_transaction_losses.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_df = pd.read_excel('../데이터/Checkpoint/lstm_transaction_losses.xlsx')\n",
    "\n",
    "plt.figure(figsize=(12, 8)) \n",
    "plt.plot(losses_df['train_losses'], label='Training Loss')\n",
    "plt.plot(losses_df['val_losses'], label='Validation Loss')\n",
    "plt.title('Training and Validation Losses', color='white')\n",
    "plt.xlabel('Epoch', color='white')\n",
    "plt.ylabel('Loss', color='white')\n",
    "plt.xticks(color='white')\n",
    "plt.yticks(color='white')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8)) \n",
    "plt.plot(losses_df['train_losses'], label='Training Loss')\n",
    "plt.title('Training Losses', color='white')\n",
    "plt.xlabel('Epoch', color='white')\n",
    "plt.ylabel('Loss', color='white')\n",
    "plt.xticks(color='white')\n",
    "plt.yticks(color='white')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8)) \n",
    "plt.plot(losses_df['val_losses'], label='Validation Loss')\n",
    "plt.title('Validation Losses', color='white')\n",
    "plt.xlabel('Epoch', color='white')\n",
    "plt.ylabel('Loss', color='white')\n",
    "plt.xticks(color='white')\n",
    "plt.yticks(color='white')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NODE 부동산 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transaction_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hkyoo\\OneDrive\\바탕 화면\\SCI\\코드\\train.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hkyoo/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hkyoo/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m LSTM_Transaction_Dataset(transaction_df[transaction_df[\u001b[39m'\u001b[39m\u001b[39m계약년월\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m2022\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hkyoo/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hkyoo/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m val_dataset \u001b[39m=\u001b[39m LSTM_Transaction_Dataset(transaction_df[transaction_df[\u001b[39m'\u001b[39m\u001b[39m계약년월\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m2022\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transaction_df' is not defined"
     ]
    }
   ],
   "source": [
    "# 데이터 다운로드\n",
    "batch_size = 1\n",
    "transaction_df = pd.read_excel('../데이터/Transaction/transaction_final.xlsx', index_col=0)\n",
    "\n",
    "train_dataset = ODE_Transaction_Dataset(transaction_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 크기 : torch.Size([2, 5])\n",
      "Y 크기 : torch.Size([2, 5])\n",
      "Z 크기 : torch.Size([2, 1])\n",
      "W 크기 : torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "for x,y,z,w in train_loader:\n",
    "      print(\"X 크기 : {}\".format(x.shape))\n",
    "      print(\"Y 크기 : {}\".format(y.shape))\n",
    "      print(\"Z 크기 : {}\".format(z.shape))\n",
    "      print(\"W 크기 : {}\".format(w.shape))\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 크기 : torch.float32\n",
      "Y 크기 : torch.float32\n",
      "Z 크기 : torch.float32\n",
      "W 크기 : torch.float32\n"
     ]
    }
   ],
   "source": [
    "for x,y,z,w in train_loader:\n",
    "      print(\"X 크기 : {}\".format(x.dtype))\n",
    "      print(\"Y 크기 : {}\".format(y.dtype))\n",
    "      print(\"Z 크기 : {}\".format(z.dtype))\n",
    "      print(\"W 크기 : {}\".format(w.dtype))\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu is available\n",
      "작동하는지 실험\n",
      "(tensor([[[0.3200],\n",
      "         [0.3020]],\n",
      "\n",
      "        [[0.3165],\n",
      "         [0.3053]],\n",
      "\n",
      "        [[0.3074],\n",
      "         [0.3091]],\n",
      "\n",
      "        [[0.3381],\n",
      "         [0.3102]],\n",
      "\n",
      "        [[0.3129],\n",
      "         [0.3069]]], grad_fn=<SliceBackward0>), tensor([[[-1.9246,  0.9916,  0.5816,  ..., -0.0953, -0.4212,  0.2555],\n",
      "         [-0.3911,  0.1918,  0.4691,  ..., -0.1469, -0.4416,  0.2056]],\n",
      "\n",
      "        [[-2.3361,  0.9280,  0.5030,  ..., -0.3589, -0.1182, -0.2516],\n",
      "         [-0.4929,  0.1927, -0.0292,  ...,  0.3396, -0.2586, -0.0658]],\n",
      "\n",
      "        [[-3.3343,  0.8663,  0.3372,  ..., -1.4191,  0.6670, -1.6950],\n",
      "         [-0.4960,  0.2321, -0.2474,  ...,  0.5994, -0.1838, -0.2525]],\n",
      "\n",
      "        [[ 0.8072,  1.3306,  1.7080,  ..., -0.8884, -1.7589,  2.7519],\n",
      "         [ 0.6183,  0.7583,  1.9381,  ..., -1.3055, -0.8485,  0.8296]],\n",
      "\n",
      "        [[-2.7293,  0.8898,  0.4417,  ..., -0.7159,  0.1923, -0.7784],\n",
      "         [-0.5078,  0.2105, -0.1448,  ...,  0.4671, -0.2156, -0.1514]],\n",
      "\n",
      "        [[ 1.2445,  1.3671,  1.8943,  ..., -1.1631, -1.9350,  3.0802],\n",
      "         [ 1.2932,  1.1343,  2.5548,  ..., -1.4962, -0.9968,  1.2692]]],\n",
      "       grad_fn=<ViewBackward0>), tensor([[ 3.9042e+00,  1.1460e+00, -8.7323e-01, -2.0277e-01, -1.4232e+00,\n",
      "         -9.7355e-01, -2.9226e-01,  1.7025e+00,  4.9917e-01,  3.9957e-02,\n",
      "         -4.1916e-01,  2.4280e-01, -1.5132e-02,  1.7395e-01, -2.0224e+00,\n",
      "          5.1864e-01,  1.5203e+00,  3.3595e+00, -8.0708e-01, -6.4386e-01,\n",
      "         -9.6330e-02,  8.3888e-01,  2.4769e-02, -2.2345e-01,  5.3007e-01,\n",
      "          1.2489e+00,  1.1800e+00,  3.9309e-01, -8.7714e-01,  1.6921e+00,\n",
      "          1.0720e+00,  5.2080e-01,  3.8115e-01, -1.2353e+00, -2.2096e-01,\n",
      "          4.0373e-01,  1.1288e+00, -3.7577e-01,  9.0160e-01,  1.1998e+00,\n",
      "          1.8166e+00, -1.0170e+00,  2.2276e-01, -6.2415e-01, -5.6234e-01,\n",
      "         -2.6533e-01,  5.2966e-01,  3.7777e-01, -7.3276e-02, -6.4162e-01,\n",
      "          4.7550e-01,  2.0280e+00, -3.7411e-01,  4.5963e-01,  4.7883e-01,\n",
      "         -5.3839e-01,  2.2202e+00, -6.2920e-02, -1.1106e-01, -8.5488e-01,\n",
      "         -3.6557e-01, -2.5014e+00,  5.9920e-01, -3.6857e-01],\n",
      "        [ 1.0762e-01, -3.4166e-01, -1.0522e+00,  1.2187e+00, -5.6238e-01,\n",
      "          1.9079e+00, -7.6549e-01,  5.2089e-01,  7.2618e-01, -4.3966e-01,\n",
      "         -1.1483e+00,  4.7966e-01,  5.7630e-01,  8.1643e-01, -1.3576e+00,\n",
      "         -5.7244e-01,  7.6411e-01,  2.0117e-02,  2.5561e-01,  1.0010e+00,\n",
      "         -4.1395e-01, -8.5190e-01,  1.2236e-01, -6.9191e-01,  6.0380e-01,\n",
      "         -6.9187e-01,  1.0257e+00,  1.0411e+00, -8.2717e-01, -1.0858e+00,\n",
      "         -1.0233e+00, -1.1872e+00, -1.3930e+00, -1.4010e-01, -8.0047e-01,\n",
      "          2.9662e-01,  1.9676e+00,  4.4584e-01,  1.9046e+00, -2.1059e-01,\n",
      "          1.5923e+00,  2.0074e-01, -1.1612e-01,  1.8093e-01,  1.2243e+00,\n",
      "          2.7559e-03,  1.3391e-01, -5.1903e-01, -1.5699e+00, -3.7198e-02,\n",
      "         -6.9918e-02,  3.8163e-01, -7.4425e-02, -7.5933e-01, -8.8816e-01,\n",
      "          1.7010e-01, -7.8576e-01,  1.8825e+00,  8.4531e-01, -8.4700e-01,\n",
      "         -1.5939e+00, -3.7945e-01, -5.2958e-01, -7.4874e-01]],\n",
      "       grad_fn=<AddBackward0>), tensor([[-2.7978e-02,  2.1564e-02, -1.9743e-02, -3.8898e-03,  3.6597e-02,\n",
      "         -5.5066e-02,  4.0261e-02,  4.9608e-02, -2.5658e-03,  9.2387e-02,\n",
      "         -6.3146e-02, -2.8552e-02,  3.3216e-02,  8.2328e-02, -6.9201e-03,\n",
      "         -3.6288e-02,  3.0480e-02, -9.8560e-03,  3.6546e-02,  2.4284e-02,\n",
      "         -4.0156e-02,  7.7440e-03, -2.4673e-02,  1.6008e-02,  3.8246e-02,\n",
      "          1.9721e-02,  7.4606e-02,  6.3284e-02, -3.7887e-03,  5.4471e-02,\n",
      "         -4.2920e-02, -3.5930e-02,  6.2402e-02,  7.3542e-02, -4.3362e-02,\n",
      "         -9.6650e-03, -9.0047e-02,  7.8424e-02,  1.5734e-03, -2.0391e-03,\n",
      "          6.8380e-02, -4.2523e-03, -5.9703e-02,  7.0611e-02,  6.4688e-02,\n",
      "         -6.2975e-02,  3.0866e-02, -2.6243e-02,  3.6632e-03, -4.1667e-02,\n",
      "          2.1046e-02, -3.1662e-02,  1.5487e-03, -1.8984e-02, -2.9312e-02,\n",
      "         -1.0388e-01,  2.0708e-02, -1.3626e-02, -4.2216e-02, -6.5238e-02,\n",
      "          5.3133e-02,  7.1187e-02,  5.7235e-02,  4.6491e-02],\n",
      "        [-1.9571e-02,  4.3879e-02, -5.0783e-03, -9.8894e-03,  3.7486e-02,\n",
      "         -6.2992e-02,  5.2318e-02,  3.7628e-02, -1.7702e-02,  8.1281e-02,\n",
      "         -4.3972e-02, -1.0214e-02,  5.3238e-02,  8.3248e-02, -2.6203e-04,\n",
      "         -3.3267e-02,  2.7521e-02, -8.1333e-04,  2.3006e-02,  3.6827e-02,\n",
      "         -4.1841e-02,  1.3795e-02, -4.0217e-02,  5.5267e-03,  1.2294e-02,\n",
      "          4.9167e-05,  6.8872e-02,  5.1969e-02,  5.1642e-03,  4.4822e-02,\n",
      "         -3.3580e-02, -4.7620e-02,  7.9498e-02,  4.5254e-02, -4.0060e-02,\n",
      "          6.4915e-04, -6.5277e-02,  6.5421e-02, -1.5464e-02, -3.0794e-02,\n",
      "          8.9416e-02,  3.7358e-03, -6.7441e-02,  7.1974e-02,  5.4571e-02,\n",
      "         -5.3111e-02,  3.0609e-02, -2.8819e-02, -1.4598e-02, -4.4193e-02,\n",
      "          3.2197e-02, -5.3195e-02, -7.4973e-03, -1.8159e-02, -1.2066e-02,\n",
      "         -1.0746e-01,  3.8273e-02, -3.0866e-03, -3.4713e-02, -1.0123e-01,\n",
      "          4.4599e-02,  7.0794e-02,  5.9186e-02,  6.3070e-02]],\n",
      "       grad_fn=<SliceBackward0>), tensor([[ 0.0155, -0.0075, -0.0300, -0.0190, -0.0339, -0.0477, -0.0162, -0.0334,\n",
      "         -0.0602, -0.0859,  0.0301,  0.0979, -0.0266,  0.0036,  0.0496,  0.0510,\n",
      "          0.0025, -0.0809, -0.0337,  0.0760,  0.0043,  0.0196,  0.0543, -0.0725,\n",
      "         -0.0324,  0.0053,  0.0032,  0.0442, -0.0300,  0.0521, -0.0945,  0.0549,\n",
      "          0.0717, -0.1073, -0.0669,  0.0128, -0.0119,  0.0157,  0.0189,  0.0650,\n",
      "          0.0086, -0.0841,  0.0187,  0.0170,  0.0094, -0.0108, -0.0065,  0.0556,\n",
      "          0.0479,  0.0164, -0.0572, -0.0344, -0.0694,  0.0377, -0.0301, -0.0064,\n",
      "          0.1002, -0.0484,  0.0019, -0.0736, -0.0714,  0.0944, -0.0039, -0.0209],\n",
      "        [ 0.0579, -0.0116, -0.0288, -0.0306, -0.0371, -0.0444, -0.0290, -0.0292,\n",
      "         -0.0747, -0.0663,  0.0496,  0.0840, -0.0257,  0.0075,  0.0487,  0.0475,\n",
      "          0.0025, -0.0878, -0.0380,  0.0758,  0.0044,  0.0135,  0.0488, -0.0813,\n",
      "         -0.0364,  0.0076, -0.0205,  0.0302, -0.0115,  0.0511, -0.0846,  0.0607,\n",
      "          0.0711, -0.0939, -0.0811, -0.0033, -0.0092,  0.0347,  0.0322,  0.0580,\n",
      "         -0.0061, -0.0910, -0.0055,  0.0147,  0.0043, -0.0052, -0.0028,  0.0524,\n",
      "          0.0341,  0.0373, -0.0631, -0.0340, -0.0806,  0.0498, -0.0214, -0.0112,\n",
      "          0.1168, -0.0336, -0.0097, -0.0747, -0.0705,  0.1314,  0.0226, -0.0298]],\n",
      "       grad_fn=<SliceBackward0>), tensor([[0.3404],\n",
      "        [0.3172]], grad_fn=<SelectBackward0>))\n",
      "torch.Size([5, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# 데이터 & 모델에 device 붙임!!!\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # 윈도우 gpu\n",
    "# device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu') # 맥 gpu\n",
    "print(f'{device} is available')\n",
    "\n",
    "model = NODE(output_dim=1,hidden_dim=256,latent_dim=64).to(device)\n",
    "\n",
    "print('작동하는지 실험')\n",
    "basic_data = torch.rand((5,2,1)).to(device)  # window_size, batch_size, 1\n",
    "time = torch.FloatTensor([[1,2,3,6,10,12],[1,3,5,8,10,12]]).reshape(6,2,1).to(device) # window_size, batch_size, 1\n",
    "data = model(basic_data,time)\n",
    "print(data)\n",
    "print(data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "noise_std = 0.02\n",
    "optim = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), lr=0.001)\n",
    "\n",
    "num_epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0\n",
      "loss : 11219012608.0, best loss : 11219012608.0\n",
      "loss : 13275686912.0, best loss : 11219012608.0\n",
      "loss : 13226204160.0, best loss : 11219012608.0\n",
      "loss : 11485132800.0, best loss : 11219012608.0\n",
      "loss : 8940823552.0, best loss : 8940823552.0\n",
      "loss : 7942053888.0, best loss : 7942053888.0\n",
      "loss : 8093862912.0, best loss : 7942053888.0\n",
      "loss : 9578081280.0, best loss : 7942053888.0\n",
      "loss : 11440328704.0, best loss : 7942053888.0\n",
      "loss : 13386883072.0, best loss : 7942053888.0\n",
      "loss : 15193017344.0, best loss : 7942053888.0\n",
      "loss : 1.5491499810317402e+19, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kimhakhyun/Desktop/Git/sci/SCI/코드/train.ipynb 셀 17\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimhakhyun/Desktop/Git/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m window_size\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimhakhyun/Desktop/Git/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()   \u001b[39m############### 여기서 second 오류가 나는지 확인!!!!\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kimhakhyun/Desktop/Git/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimhakhyun/Desktop/Git/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X22sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m optim\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimhakhyun/Desktop/Git/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X22sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/function.py:274\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    273\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 274\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/Desktop/Git/sci/SCI/코드/Model/ODEF.py:124\u001b[0m, in \u001b[0;36mODEAdjoint.backward\u001b[0;34m(ctx, dLdz)\u001b[0m\n\u001b[1;32m    120\u001b[0m adj_t[i_t] \u001b[39m=\u001b[39m adj_t[i_t] \u001b[39m-\u001b[39m dLdt_i\n\u001b[1;32m    122\u001b[0m aug_z \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((z_i\u001b[39m.\u001b[39mview(bs, n_dim), adj_z, torch\u001b[39m.\u001b[39mzeros(bs, n_params)\u001b[39m.\u001b[39mto(z), adj_t[i_t]), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 124\u001b[0m aug_ans \u001b[39m=\u001b[39m ode_solve(aug_z, t_i, t[i_t\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], augmented_dynamics)\n\u001b[1;32m    126\u001b[0m adj_z[:] \u001b[39m=\u001b[39m aug_ans[:, n_dim:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mn_dim]\n\u001b[1;32m    127\u001b[0m adj_p[:] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m aug_ans[:, \u001b[39m2\u001b[39m\u001b[39m*\u001b[39mn_dim:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mn_dim \u001b[39m+\u001b[39m n_params]\n",
      "File \u001b[0;32m~/Desktop/Git/sci/SCI/코드/Model/ODEF.py:28\u001b[0m, in \u001b[0;36mode_solve\u001b[0;34m(z0, t0, t1, f)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m i_step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_steps):\n\u001b[1;32m     27\u001b[0m     z \u001b[39m=\u001b[39m z \u001b[39m+\u001b[39m h \u001b[39m*\u001b[39m f(z, t)\n\u001b[0;32m---> 28\u001b[0m     t \u001b[39m=\u001b[39m t \u001b[39m+\u001b[39m h\n\u001b[1;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m z\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_train_loss = float('inf') \n",
    "for epoch in range(num_epochs + 1):\n",
    "    print('epoch : ',epoch)\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for batch_idx, samples in enumerate(train_loader):\n",
    "        tran_x, time_x, tran_y, time_y = samples\n",
    "        \n",
    "        t = torch.cat((time_x,time_y),dim=1)  \n",
    "        tran_x = tran_x.transpose(0,1).unsqueeze(2).to(device)\n",
    "        t = t.transpose(0,1).unsqueeze(2).to(device)\n",
    "        \n",
    "        x_p, _, z, z_mean, z_log_var, pred = model(tran_x, t)\n",
    "        kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean**2 - torch.exp(z_log_var), -1)\n",
    "        loss = 0.5 * ((tran_x-x_p)**2).sum(-1).sum(0) / noise_std**2 + kl_loss\n",
    "        loss = torch.mean(loss)\n",
    "        loss /= window_size\n",
    "        optim.zero_grad()   ############### 여기서 second 오류가 나는지 확인!!!!\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if loss < best_train_loss:\n",
    "            best_train_loss = loss\n",
    "            torch.save(model.state_dict(), \"../데이터/checkpoint/best_ODE_transaction_model.pth\")\n",
    "        \n",
    "        print('loss : {}, best loss : {}'.format(loss, best_train_loss))\n",
    "    print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ODERNN 부동산 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "num_epochs = 300\n",
    "\n",
    "model = ODE_RNN(256,64,1,200,200,device).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\ODE_Transaction_Dataset.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'].astype(str), format='%Y%m')\n",
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\ODE_Transaction_Dataset.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'].astype(str), format='%Y%m')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "train_dataset = ODE_Transaction_Dataset(transaction_df[transaction_df['계약년월']//100 != 2022])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "val_dataset = ODE_Transaction_Dataset(transaction_df[transaction_df['계약년월']//100 == 2022])\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 크기 : torch.Size([1, 5])\n",
      "Y 크기 : torch.Size([1, 5])\n",
      "Z 크기 : torch.Size([1, 1])\n",
      "W 크기 : torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "for x,y,z,w in train_loader:\n",
    "      print(\"X 크기 : {}\".format(x.shape))\n",
    "      print(\"Y 크기 : {}\".format(y.shape))\n",
    "      print(\"Z 크기 : {}\".format(z.shape))\n",
    "      print(\"W 크기 : {}\".format(w.shape))\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작동하는지 실험\n",
      "tensor([[0., 0., 0., 0., 0., 0.]], device='cuda:0', grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "print('작동하는지 실험')\n",
    "t = torch.FloatTensor([[1.,2.,3.,5.,9.,12.]]).to(device)\n",
    "data = torch.FloatTensor([[123,126,266,279,300]]).to(device)\n",
    "\n",
    "out = model(data,t)\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 학습 되는지 파악하기!!!!!!!\n",
    "\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# print(f'{device} is available')\n",
    "\n",
    "# model = ODE_RNN(256,64,1,200,200,device).to(device)\n",
    "\n",
    "# print('작동하는지 실험')\n",
    "# t = torch.FloatTensor([[1.,2.,3.,5.,9.,12.]]).to(device)\n",
    "# data = torch.FloatTensor([[123,126,266,279,300]]).to(device)\n",
    "\n",
    "# out, _ = model(data,t)\n",
    "\n",
    "# loss = criterion(out[0][:5], torch.FloatTensor([2,5,6,7,8]))\n",
    "\n",
    "# loss.backward()\n",
    "\n",
    "# for name,param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name,param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:530: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/300, LR: 0.0001, Training Loss: 42.8354185386184, Validation Loss: 29.2708740234375\n",
      "Epoch 1/300, LR: 0.0001, Training Loss: 42.345011187520115, Validation Loss: 29.2708740234375\n",
      "Epoch 2/300, LR: 0.0001, Training Loss: 42.345011187520115, Validation Loss: 29.2708740234375\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs+1):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    for batch_idx, (tran_x, time_x, tran_y, time_y) in enumerate(train_loader):\n",
    "        tran_x, time_x, tran_y, time_y = tran_x.to(device), time_x.to(device), tran_y.to(device), time_y.to(device)\n",
    "        time_f = torch.cat([time_x,time_y], axis=1)\n",
    "\n",
    "        prediction, hidden = model(tran_x, time_f)\n",
    "        cost = criterion(prediction[0][:5], tran_x[0])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += cost.item()\n",
    "    \n",
    "    total_train_loss /= len(train_loader)\n",
    "    train_losses.append(total_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (tran_x, time_x, tran_y, time_y) in enumerate(val_loader):\n",
    "            tran_x, time_x, tran_y, time_y = tran_x.to(device), time_x.to(device), tran_y.to(device), time_y.to(device)\n",
    "            time_f = torch.cat([time_x,time_y], axis=1)\n",
    "            \n",
    "            prediction, hidden = model(tran_x, time_f)\n",
    "            loss = criterion(prediction, y_val)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch}/{num_epochs}, LR: {lr}, Training Loss: {total_train_loss}, Validation Loss: {val_loss}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_path = f\"../데이터/Checkpoint/best_ode_transaction_model(300 epoch_0.0001 lr).pth\"\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "save_path = f\"../데이터/Checkpoint/recent_ode_transaction_model(300 epoch_0.0001 lr).pth\"\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\ODE_Transaction_Dataset.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'].astype(str), format='%Y%m')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning_rate: 1e-07, Epoch 0/5, Training Loss: 422.15655517578125\n",
      "Learning_rate: 1e-07, Epoch 1/5, Training Loss: 422.1231384277344\n",
      "Learning_rate: 1e-07, Epoch 2/5, Training Loss: 422.0897521972656\n",
      "Learning_rate: 1e-07, Epoch 3/5, Training Loss: 422.0563659667969\n",
      "Learning_rate: 1e-07, Epoch 4/5, Training Loss: 422.02294921875\n",
      "Learning_rate: 1e-07, Epoch 5/5, Training Loss: 421.989501953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\ODE_Transaction_Dataset.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'].astype(str), format='%Y%m')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning_rate: 1e-06, Epoch 0/5, Training Loss: 421.86181640625\n",
      "Learning_rate: 1e-06, Epoch 1/5, Training Loss: 421.5279235839844\n",
      "Learning_rate: 1e-06, Epoch 2/5, Training Loss: 421.1943359375\n",
      "Learning_rate: 1e-06, Epoch 3/5, Training Loss: 420.86114501953125\n",
      "Learning_rate: 1e-06, Epoch 4/5, Training Loss: 420.5282287597656\n",
      "Learning_rate: 1e-06, Epoch 5/5, Training Loss: 420.1957092285156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\ODE_Transaction_Dataset.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'].astype(str), format='%Y%m')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning_rate: 1e-05, Epoch 0/5, Training Loss: 418.931640625\n",
      "Learning_rate: 1e-05, Epoch 1/5, Training Loss: 415.6735534667969\n",
      "Learning_rate: 1e-05, Epoch 2/5, Training Loss: 412.51904296875\n",
      "Learning_rate: 1e-05, Epoch 3/5, Training Loss: 409.501953125\n",
      "Learning_rate: 1e-05, Epoch 4/5, Training Loss: 406.6498718261719\n",
      "Learning_rate: 1e-05, Epoch 5/5, Training Loss: 403.9827880859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\ODE_Transaction_Dataset.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'].astype(str), format='%Y%m')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning_rate: 0.0001, Epoch 0/5, Training Loss: 396.4898376464844\n",
      "Learning_rate: 0.0001, Epoch 1/5, Training Loss: 386.22821044921875\n",
      "Learning_rate: 0.0001, Epoch 2/5, Training Loss: 383.25341796875\n",
      "Learning_rate: 0.0001, Epoch 3/5, Training Loss: 382.4549865722656\n",
      "Learning_rate: 0.0001, Epoch 4/5, Training Loss: 382.2449645996094\n",
      "Learning_rate: 0.0001, Epoch 5/5, Training Loss: 382.1899108886719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\ODE_Transaction_Dataset.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'].astype(str), format='%Y%m')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning_rate: 0.001, Epoch 0/5, Training Loss: 382.3034362792969\n",
      "Learning_rate: 0.001, Epoch 1/5, Training Loss: 382.17236328125\n",
      "Learning_rate: 0.001, Epoch 2/5, Training Loss: 382.1704406738281\n",
      "Learning_rate: 0.001, Epoch 3/5, Training Loss: 382.1704406738281\n",
      "Learning_rate: 0.001, Epoch 4/5, Training Loss: 382.1704406738281\n",
      "Learning_rate: 0.001, Epoch 5/5, Training Loss: 382.1704406738281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\ODE_Transaction_Dataset.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'].astype(str), format='%Y%m')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning_rate: 0.01, Epoch 0/5, Training Loss: 382.17230224609375\n",
      "Learning_rate: 0.01, Epoch 1/5, Training Loss: 382.1704406738281\n",
      "Learning_rate: 0.01, Epoch 2/5, Training Loss: 382.1704406738281\n",
      "Learning_rate: 0.01, Epoch 3/5, Training Loss: 382.1704406738281\n",
      "Learning_rate: 0.01, Epoch 4/5, Training Loss: 382.1704406738281\n",
      "Learning_rate: 0.01, Epoch 5/5, Training Loss: 382.1704406738281\n"
     ]
    }
   ],
   "source": [
    "for lr in [1e-7,1e-6,1e-5,1e-4,1e-3,1e-2]:\n",
    "    model = ODE_RNN(256,64,1,200,200,device).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_train_loss = float('inf') \n",
    "    for epoch in range(num_epochs + 1):\n",
    "        losses = []\n",
    "        model.train()\n",
    "        \n",
    "        for batch_idx, samples in enumerate(train_loader):\n",
    "            tran_x, time_x, tran_y, time_y = samples\n",
    "            tran_x, time_x, tran_y, time_y = tran_x.to(device), time_x.to(device), tran_y.to(device), time_y.to(device)\n",
    "            time_f = torch.cat([time_x,time_y],axis=1)\n",
    "            prediction, hidden = model(tran_x,time_f)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            cost = criterion(prediction[0][:5], tran_x[0])\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "        # for name,param in model.named_parameters():\n",
    "            # if param.requires_grad:\n",
    "            #     print(name,param.grad)\n",
    "            #     break\n",
    "        print(f'Learning_rate: {lr}, Epoch {epoch}/{num_epochs}, Training Loss: {cost.item()}')\n",
    "        # model.eval()\n",
    "        # val_loss = 0.0\n",
    "        # with torch.no_grad():\n",
    "        #     for batch_idx, samples in enumerate(transaction_val_loader):\n",
    "        #         x_val, y_val = samples\n",
    "\n",
    "        #         prediction, hidden = model(x_val)\n",
    "        #         loss = criterion(prediction, y_val)\n",
    "        #         val_loss += loss.item()\n",
    "\n",
    "        # val_loss /= len(transaction_val_loader)\n",
    "        # print(f'Epoch {epoch}/{num_epochs}, Training Loss: {cost.item()}, Validation Loss: {val_loss}')\n",
    "        \n",
    "        # if val_loss < best_val_loss:\n",
    "        #     best_val_loss = val_loss\n",
    "        #     torch.save(model.state_dict(), '../데이터/Checkpoint/best_rnn_transaction_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
