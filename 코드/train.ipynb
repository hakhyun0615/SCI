{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from Dataset.Economy_Dataset import EconomyDataset\n",
    "from Dataset.Transaction_Dataset import TransactionDataset\n",
    "\n",
    "from Model.LSTM import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5\n",
    "hidden_size = 256\n",
    "output_size = 1\n",
    "\n",
    "lr = 1e-3\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "economy_df = pd.read_excel('../데이터/Economy/economy_all.xlsx')\n",
    "economy_df = economy_df['국고채금리'].values\n",
    "transaction_df = pd.read_csv('../데이터/Transaction/transaction_final.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimhakhyun/Desktop/Git/sci/SCI/코드/Dataset/Economy_Dataset.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  self.x = torch.FloatTensor(X)\n"
     ]
    }
   ],
   "source": [
    "economy_train_size = int(len(economy_df)*0.6)\n",
    "economy_val_size = int(len(economy_df)*0.3)\n",
    "trainsaction_train_size = int(len(transaction_df)*0.6)\n",
    "trainsaction_val_size = int(len(transaction_df)*0.3)\n",
    "\n",
    "economy_train_dataset = EconomyDataset(economy_df[:economy_train_size])\n",
    "economy_train_loader = DataLoader(economy_train_dataset, batch_size=2)\n",
    "economy_val_dataset = EconomyDataset(economy_df[economy_train_size:economy_train_size+economy_val_size])\n",
    "economy_val_loader = DataLoader(economy_val_dataset, batch_size=2)\n",
    "economy_test_dataset = EconomyDataset(economy_df[economy_train_size+economy_val_size:])\n",
    "economy_test_loader = DataLoader(economy_test_dataset, batch_size=2)\n",
    "\n",
    "transaction_train_dataset = TransactionDataset(transaction_df[:trainsaction_train_size])\n",
    "transaction_train_loader = DataLoader(transaction_train_dataset, batch_size=2)\n",
    "transaction_val_dataset = TransactionDataset(transaction_df[trainsaction_train_size:trainsaction_train_size+trainsaction_val_size])\n",
    "transaction_val_loader = DataLoader(transaction_val_dataset, batch_size=2)\n",
    "transaction_test_dataset = TransactionDataset(transaction_df[trainsaction_train_size+trainsaction_val_size:])\n",
    "transaction_test_loader = DataLoader(transaction_test_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100, Training Loss: 0.584766149520874, Validation Loss: 1.7173492695604051\n",
      "Epoch 1/100, Training Loss: 0.029341816902160645, Validation Loss: 0.4915290773829578\n",
      "Epoch 2/100, Training Loss: 0.014836261048913002, Validation Loss: 0.673044593539089\n",
      "Epoch 3/100, Training Loss: 0.011310496367514133, Validation Loss: 0.7112700086685696\n",
      "Epoch 4/100, Training Loss: 0.00764611829072237, Validation Loss: 0.7607639563669052\n",
      "Epoch 5/100, Training Loss: 0.005434147082269192, Validation Loss: 0.8015894385586891\n",
      "Epoch 6/100, Training Loss: 0.0051312013529241085, Validation Loss: 0.8240756413766316\n",
      "Epoch 7/100, Training Loss: 0.005548834800720215, Validation Loss: 0.8355885456715312\n",
      "Epoch 8/100, Training Loss: 0.00619431771337986, Validation Loss: 0.8415670536591539\n",
      "Epoch 9/100, Training Loss: 0.006885802838951349, Validation Loss: 0.844067028391042\n",
      "Epoch 10/100, Training Loss: 0.0075322529301047325, Validation Loss: 0.8453299791019943\n",
      "Epoch 11/100, Training Loss: 0.008100266568362713, Validation Loss: 0.8443624642677605\n",
      "Epoch 12/100, Training Loss: 0.00857716053724289, Validation Loss: 0.842393031809479\n",
      "Epoch 13/100, Training Loss: 0.008959265425801277, Validation Loss: 0.83892971869292\n",
      "Epoch 14/100, Training Loss: 0.009255285374820232, Validation Loss: 0.8342103626179908\n",
      "Epoch 15/100, Training Loss: 0.009476643986999989, Validation Loss: 0.828186296192663\n",
      "Epoch 16/100, Training Loss: 0.009638863615691662, Validation Loss: 0.8209219521709851\n",
      "Epoch 17/100, Training Loss: 0.009755201637744904, Validation Loss: 0.8124867084303072\n",
      "Epoch 18/100, Training Loss: 0.00983947142958641, Validation Loss: 0.802844618447125\n",
      "Epoch 19/100, Training Loss: 0.009902267716825008, Validation Loss: 0.7920656908702638\n",
      "Epoch 20/100, Training Loss: 0.009953726083040237, Validation Loss: 0.780042652107243\n",
      "Epoch 21/100, Training Loss: 0.010000957176089287, Validation Loss: 0.7668131437551763\n",
      "Epoch 22/100, Training Loss: 0.010051335208117962, Validation Loss: 0.7522090746622\n",
      "Epoch 23/100, Training Loss: 0.010109065100550652, Validation Loss: 0.7362366866852555\n",
      "Epoch 24/100, Training Loss: 0.010180041193962097, Validation Loss: 0.7186880578353468\n",
      "Epoch 25/100, Training Loss: 0.010267031379044056, Validation Loss: 0.6995613683081631\n",
      "Epoch 26/100, Training Loss: 0.0103756133466959, Validation Loss: 0.6786751464741039\n",
      "Epoch 27/100, Training Loss: 0.01050804927945137, Validation Loss: 0.6560857342389811\n",
      "Epoch 28/100, Training Loss: 0.010669531300663948, Validation Loss: 0.6317678738518485\n",
      "Epoch 29/100, Training Loss: 0.010861445218324661, Validation Loss: 0.6059654083967742\n",
      "Epoch 30/100, Training Loss: 0.01108557265251875, Validation Loss: 0.578983433823201\n",
      "Epoch 31/100, Training Loss: 0.011336270719766617, Validation Loss: 0.5513864412330024\n",
      "Epoch 32/100, Training Loss: 0.011600208468735218, Validation Loss: 0.523851973422487\n",
      "Epoch 33/100, Training Loss: 0.011849584057927132, Validation Loss: 0.49718124667872743\n",
      "Epoch 34/100, Training Loss: 0.012045331299304962, Validation Loss: 0.47216527154003934\n",
      "Epoch 35/100, Training Loss: 0.012155107222497463, Validation Loss: 0.4495004728087224\n",
      "Epoch 36/100, Training Loss: 0.012203743681311607, Validation Loss: 0.4296825849284817\n",
      "Epoch 37/100, Training Loss: 0.012354274280369282, Validation Loss: 0.41294107128799495\n",
      "Epoch 38/100, Training Loss: 0.012939021922647953, Validation Loss: 0.3992476177850871\n",
      "Epoch 39/100, Training Loss: 0.014268896542489529, Validation Loss: 0.38832472728764905\n",
      "Epoch 40/100, Training Loss: 0.016171781346201897, Validation Loss: 0.3794257943934229\n",
      "Epoch 41/100, Training Loss: 0.01779109239578247, Validation Loss: 0.37115926232737756\n",
      "Epoch 42/100, Training Loss: 0.018297391012310982, Validation Loss: 0.36236073913044364\n",
      "Epoch 43/100, Training Loss: 0.01778344064950943, Validation Loss: 0.35313345542402075\n",
      "Epoch 44/100, Training Loss: 0.01697048917412758, Validation Loss: 0.34345195735555273\n",
      "Epoch 45/100, Training Loss: 0.016351493075489998, Validation Loss: 0.33259333305094124\n",
      "Epoch 46/100, Training Loss: 0.016160812228918076, Validation Loss: 0.32210507818464457\n",
      "Epoch 47/100, Training Loss: 0.01656467840075493, Validation Loss: 0.3138516948425344\n",
      "Epoch 48/100, Training Loss: 0.017611125484108925, Validation Loss: 0.30788037727221046\n",
      "Epoch 49/100, Training Loss: 0.019029829651117325, Validation Loss: 0.3041606275946833\n",
      "Epoch 50/100, Training Loss: 0.02024267427623272, Validation Loss: 0.30293439214749796\n",
      "Epoch 51/100, Training Loss: 0.02076978050172329, Validation Loss: 0.30358533017819617\n",
      "Epoch 52/100, Training Loss: 0.020562300458550453, Validation Loss: 0.3038439759277805\n",
      "Epoch 53/100, Training Loss: 0.0200125053524971, Validation Loss: 0.3016049316834791\n",
      "Epoch 54/100, Training Loss: 0.019637443125247955, Validation Loss: 0.29737297350740327\n",
      "Epoch 55/100, Training Loss: 0.019652962684631348, Validation Loss: 0.29312671101485777\n",
      "Epoch 56/100, Training Loss: 0.019937483593821526, Validation Loss: 0.29018719851280494\n",
      "Epoch 57/100, Training Loss: 0.02024749480187893, Validation Loss: 0.2888456844250738\n",
      "Epoch 58/100, Training Loss: 0.020393790677189827, Validation Loss: 0.28859357667222085\n",
      "Epoch 59/100, Training Loss: 0.020330533385276794, Validation Loss: 0.28845543836359866\n",
      "Epoch 60/100, Training Loss: 0.02014905773103237, Validation Loss: 0.2876097504766741\n",
      "Epoch 61/100, Training Loss: 0.019986042752861977, Validation Loss: 0.2859520299721875\n",
      "Epoch 62/100, Training Loss: 0.019918056204915047, Validation Loss: 0.28400814495086835\n",
      "Epoch 63/100, Training Loss: 0.01992940343916416, Validation Loss: 0.28239042623421745\n",
      "Epoch 64/100, Training Loss: 0.01995403692126274, Validation Loss: 0.2813727400695955\n",
      "Epoch 65/100, Training Loss: 0.01993441954255104, Validation Loss: 0.2808029445347659\n",
      "Epoch 66/100, Training Loss: 0.019856909289956093, Validation Loss: 0.28029632992547704\n",
      "Epoch 67/100, Training Loss: 0.019749488681554794, Validation Loss: 0.2795468580620114\n",
      "Epoch 68/100, Training Loss: 0.019652066752314568, Validation Loss: 0.2785302130100068\n",
      "Epoch 69/100, Training Loss: 0.019585447385907173, Validation Loss: 0.27745711947708124\n",
      "Epoch 70/100, Training Loss: 0.019542312249541283, Validation Loss: 0.2765634684131198\n",
      "Epoch 71/100, Training Loss: 0.019500181078910828, Validation Loss: 0.2759334913186779\n",
      "Epoch 72/100, Training Loss: 0.01944165863096714, Validation Loss: 0.27547278202525505\n",
      "Epoch 73/100, Training Loss: 0.019365541636943817, Validation Loss: 0.27501374206466217\n",
      "Epoch 74/100, Training Loss: 0.019284946843981743, Validation Loss: 0.27445657641302595\n",
      "Epoch 75/100, Training Loss: 0.019213801249861717, Validation Loss: 0.27383106687141534\n",
      "Epoch 76/100, Training Loss: 0.01915711537003517, Validation Loss: 0.2732483829292635\n",
      "Epoch 77/100, Training Loss: 0.019108839333057404, Validation Loss: 0.27279948763680295\n",
      "Epoch 78/100, Training Loss: 0.019059285521507263, Validation Loss: 0.2724886162749109\n",
      "Epoch 79/100, Training Loss: 0.019002996385097504, Validation Loss: 0.27224574453586164\n",
      "Epoch 80/100, Training Loss: 0.01894245855510235, Validation Loss: 0.27199482858590535\n",
      "Epoch 81/100, Training Loss: 0.018884781748056412, Validation Loss: 0.2717146172009442\n",
      "Epoch 82/100, Training Loss: 0.018835559487342834, Validation Loss: 0.27144658581060604\n",
      "Epoch 83/100, Training Loss: 0.018795059993863106, Validation Loss: 0.27125285941552385\n",
      "Epoch 84/100, Training Loss: 0.018758375197649002, Validation Loss: 0.27116132231042556\n",
      "Epoch 85/100, Training Loss: 0.01872073858976364, Validation Loss: 0.271150950352811\n",
      "Epoch 86/100, Training Loss: 0.018681427463889122, Validation Loss: 0.27117469969068353\n",
      "Epoch 87/100, Training Loss: 0.0186433307826519, Validation Loss: 0.27120172915497925\n",
      "Epoch 88/100, Training Loss: 0.018610995262861252, Validation Loss: 0.2712431451592628\n",
      "Epoch 89/100, Training Loss: 0.018586158752441406, Validation Loss: 0.27133502716086305\n",
      "Epoch 90/100, Training Loss: 0.018567262217402458, Validation Loss: 0.2715097669008953\n",
      "Epoch 91/100, Training Loss: 0.018550928682088852, Validation Loss: 0.27176890460915665\n",
      "Epoch 92/100, Training Loss: 0.018534965813159943, Validation Loss: 0.2720868973182015\n",
      "Epoch 93/100, Training Loss: 0.01852058619260788, Validation Loss: 0.27243731006456073\n",
      "Epoch 94/100, Training Loss: 0.018510224297642708, Validation Loss: 0.2728161461584802\n",
      "Epoch 95/100, Training Loss: 0.018506791442632675, Validation Loss: 0.27324443567652323\n",
      "Epoch 96/100, Training Loss: 0.018510276451706886, Validation Loss: 0.2737494652226035\n",
      "Epoch 97/100, Training Loss: 0.018518824130296707, Validation Loss: 0.2743432451076972\n",
      "Epoch 98/100, Training Loss: 0.01853056252002716, Validation Loss: 0.2750220635768658\n",
      "Epoch 99/100, Training Loss: 0.018544504418969154, Validation Loss: 0.27577173937087146\n",
      "Epoch 100/100, Training Loss: 0.018558399751782417, Validation Loss: 0.27656765180706444\n"
     ]
    }
   ],
   "source": [
    "# 경제 모델\n",
    "best_val_loss = float('inf') \n",
    "for epoch in range(num_epochs + 1):\n",
    "    model.train()\n",
    "    for batch_idx, samples in enumerate(economy_train_loader):\n",
    "        x_train, y_train = samples\n",
    "\n",
    "        prediction = model(x_train)\n",
    "        cost = criterion(prediction, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, samples in enumerate(economy_val_loader):\n",
    "            x_val, y_val = samples\n",
    "\n",
    "            prediction = model(x_val)\n",
    "            loss = criterion(prediction, y_val)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(economy_val_loader)\n",
    "    print(f'Epoch {epoch}/{num_epochs}, Training Loss: {cost.item()}, Validation Loss: {val_loss}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), '../코드/Checkpoint/best_economy_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kimhakhyun/Desktop/Git/sci/SCI/코드/train.ipynb 셀 8\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimhakhyun/Desktop/Git/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimhakhyun/Desktop/Git/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     cost\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kimhakhyun/Desktop/Git/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimhakhyun/Desktop/Git/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimhakhyun/Desktop/Git/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m val_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[1;32m    144\u001b[0m         exp_avgs,\n\u001b[1;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    147\u001b[0m         state_steps,\n\u001b[1;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mamsgrad\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39mbeta1,\n\u001b[1;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39mbeta2,\n\u001b[1;32m    151\u001b[0m         lr\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    153\u001b[0m         eps\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39meps\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mforeach\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mcapturable\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    158\u001b[0m         fused\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mfused\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgrad_scale\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfound_inf\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m func(params,\n\u001b[1;32m    282\u001b[0m      grads,\n\u001b[1;32m    283\u001b[0m      exp_avgs,\n\u001b[1;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    286\u001b[0m      state_steps,\n\u001b[1;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39mamsgrad,\n\u001b[1;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39mbeta1,\n\u001b[1;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39mbeta2,\n\u001b[1;32m    290\u001b[0m      lr\u001b[39m=\u001b[39mlr,\n\u001b[1;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39mweight_decay,\n\u001b[1;32m    292\u001b[0m      eps\u001b[39m=\u001b[39meps,\n\u001b[1;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39mmaximize,\n\u001b[1;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39mcapturable,\n\u001b[1;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39mdifferentiable,\n\u001b[1;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39mgrad_scale,\n\u001b[1;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39mfound_inf)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:345\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    344\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m--> 345\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n\u001b[1;32m    348\u001b[0m     step \u001b[39m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 부동산 모델\n",
    "best_val_loss = float('inf') \n",
    "for epoch in range(num_epochs + 1):\n",
    "    model.train()\n",
    "    for batch_idx, samples in enumerate(transaction_train_loader):\n",
    "        x_train, y_train = samples\n",
    "\n",
    "        prediction = model(x_train)\n",
    "        cost = criterion(prediction, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, samples in enumerate(transaction_val_loader):\n",
    "            x_val, y_val = samples\n",
    "\n",
    "            prediction = model(x_val)\n",
    "            loss = criterion(prediction, y_val)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(transaction_val_loader)\n",
    "    print(f'Epoch {epoch}/{num_epochs}, Training Loss: {cost.item()}, Validation Loss: {val_loss}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), '../코드/Checkpoint/best_transaction_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
