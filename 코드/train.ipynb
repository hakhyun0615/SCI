{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "from Dataset.Embedding_Dataset import Embedding_Dataset\n",
    "from Model.Embedding import Embedding\n",
    "\n",
    "from Dataset.LSTM_Dataset import LSTM_Dataset\n",
    "from Model.LSTM import LSTM\n",
    "from Model.NLinear import NLinear\n",
    "\n",
    "from Dataset.Attention_Dataset import Attention_Dataset\n",
    "from Model.Attention import LSTMSeq2Seq\n",
    "\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = torch.device('cpu') # CPU\n",
    "# DEVICE = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu') # MAC\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # WINDOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "\n",
    "embedding_lr = 0.001\n",
    "embedding_batch = 32\n",
    "embedding_epochs = 150\n",
    "encoder_dim_1 = 128\n",
    "encoder_dim_2 = 256\n",
    "encoder_dim_3 = 512\n",
    "embedding_dim = 1024\n",
    "decoder_dim_1 = 512\n",
    "decoder_dim_2 = 256\n",
    "decoder_dim_3 = 128\n",
    "\n",
    "lstm_lr = 0.01\n",
    "lstm_batch = 1\n",
    "lstm_epochs = 50\n",
    "window_size = 5\n",
    "hidden_dim = 128\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "connection_info = \"host=localhost dbname=postgres user=postgres password=hd219833 port=5432\"\n",
    "conn = psycopg2.connect(connection_info)\n",
    "table_1_query = '''\n",
    "    SELECT * FROM building\n",
    "    '''\n",
    "table_2_query = '''\n",
    "    SELECT * FROM economy\n",
    "    '''\n",
    "table_3_query = '''\n",
    "    SELECT * FROM building_price\n",
    "    '''\n",
    "table_1 = pd.read_sql(table_1_query,conn) \n",
    "table_2 = pd.read_sql(table_2_query,conn)\n",
    "table_3 = pd.read_sql(table_3_query,conn) \n",
    "\n",
    "# table_1 = pd.read_csv('../데이터/Table/table_1.csv') \n",
    "# table_2 = pd.read_csv('../데이터/Table/table_2.csv') \n",
    "# table_3 = pd.read_csv('../데이터/Table/table_3.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val_losses(train_losses, val_losses):\n",
    "    print(f'Min Validation Loss: {min(val_losses)}')\n",
    "    plt.plot(train_losses[1:], label='Training Loss')\n",
    "    plt.plot(val_losses[1:], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Embedding_Dataset(table_1, table_2, table_3)\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Train Loss: 5485944229.0342, Val Loss: 5847792593.0082\n",
      "Epoch [2/150], Train Loss: 5331781348.7715, Val Loss: 5680119831.4959\n"
     ]
    }
   ],
   "source": [
    "model = Embedding(encoder_dim_1, encoder_dim_2, encoder_dim_3, embedding_dim, decoder_dim_1, decoder_dim_2, decoder_dim_3).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=embedding_lr)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "consecutive_val_loss_increases = 0\n",
    "max_consecutive_val_loss_increases = 3\n",
    "for epoch in range(embedding_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        input = data[0].to(DEVICE)\n",
    "        target = data[1].to(DEVICE)\n",
    "        output = model(input).to(DEVICE)\n",
    "\n",
    "        train_loss = criterion(output, target)\n",
    "        total_train_loss += train_loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            input = data[0].to(DEVICE)\n",
    "            target = data[1].to(DEVICE)\n",
    "            output = model(input).to(DEVICE)\n",
    "\n",
    "            val_loss = criterion(output, target)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # if len(val_losses) > 1 and val_losses[-1] > val_losses[-2]:\n",
    "    #     consecutive_val_loss_increases += 1\n",
    "    #     if consecutive_val_loss_increases >= max_consecutive_val_loss_increases:\n",
    "    #         torch.save(model, f'../데이터/Checkpoint/embedding_tr_{train_ratio}_lr_{embedding_lr}_batch_{embedding_batch}_epochs_{embedding_epochs}_e1_{encoder_dim_1}_e2_{encoder_dim_1}_e3_{encoder_dim_3}_emb_{embedding_dim}_d1{decoder_dim_1}_d2_{decoder_dim_2}_d3_{decoder_dim_3}.pth')\n",
    "    #         print(f\"Early Stopping Triggered!\")\n",
    "    #         break\n",
    "    # else:\n",
    "    #     consecutive_val_loss_increases = 0\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        consecutive_val_loss_increases = 0  \n",
    "    else:\n",
    "        consecutive_val_loss_increases += 1 \n",
    "    if consecutive_val_loss_increases == max_consecutive_val_loss_increases:\n",
    "        torch.save(model, f'../데이터/Checkpoint/embedding_tr_{train_ratio}_lr_{embedding_lr}_batch_{embedding_batch}_epochs_{embedding_epochs}_e1_{encoder_dim_1}_e2_{encoder_dim_1}_e3_{encoder_dim_3}_emb_{embedding_dim}_d1{decoder_dim_1}_d2_{decoder_dim_2}_d3_{decoder_dim_3}.pth')\n",
    "        print(\"Early Stopping Triggered\")\n",
    "        break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{embedding_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Validation Loss: 2241089226.630137\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_train_val_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../데이터/Checkpoint/embedding_train_0.8_lr_0.01_batch_32_epochs_50_dim_6.pth')\n",
    "dataset = LSTM_Dataset(model, table_1, table_2, table_3, embedding_dim, window_size)\n",
    "\n",
    "train_ratio = 0.8\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 4457084913.2495\n",
      "Epoch [2/50], Train Loss: 2639799669.2864\n",
      "Epoch [3/50], Train Loss: 1889211181.3742\n",
      "Epoch [4/50], Train Loss: 1621113765.7466\n",
      "Epoch [5/50], Train Loss: 1536347630.7305\n",
      "Epoch [6/50], Train Loss: 1508256809.3546\n",
      "Epoch [7/50], Train Loss: 1501316050.4178\n",
      "Epoch [8/50], Train Loss: 1496720547.7443\n",
      "Epoch [9/50], Train Loss: 1493436904.9662\n",
      "Epoch [10/50], Train Loss: 1491849440.4544\n",
      "Epoch [11/50], Train Loss: 1490709939.9167\n",
      "Epoch [12/50], Train Loss: 1490298252.6531\n",
      "Epoch [13/50], Train Loss: 1490066771.7568\n",
      "Epoch [14/50], Train Loss: 1489904553.6412\n",
      "Epoch [15/50], Train Loss: 1489809264.2112\n",
      "Epoch [16/50], Train Loss: 1489719638.4178\n",
      "Epoch [17/50], Train Loss: 1489666388.2125\n",
      "Epoch [18/50], Train Loss: 1489614520.1380\n",
      "Epoch [19/50], Train Loss: 1489531656.4119\n",
      "Epoch [20/50], Train Loss: 1489476864.0525\n",
      "Epoch [21/50], Train Loss: 1489432332.4914\n",
      "Epoch [22/50], Train Loss: 1489389663.6376\n",
      "Epoch [23/50], Train Loss: 1489300576.0239\n",
      "Epoch [24/50], Train Loss: 1489243003.7281\n",
      "Epoch [25/50], Train Loss: 1489210289.8569\n",
      "Epoch [26/50], Train Loss: 1489147177.3672\n",
      "Epoch [27/50], Train Loss: 1489087240.8590\n",
      "Epoch [28/50], Train Loss: 1489049705.3208\n",
      "Epoch [29/50], Train Loss: 1488954457.3110\n",
      "Epoch [30/50], Train Loss: 1488918861.9109\n",
      "Epoch [31/50], Train Loss: 1488849718.9130\n",
      "Epoch [32/50], Train Loss: 1488789314.3850\n",
      "Epoch [33/50], Train Loss: 1488730489.2280\n",
      "Epoch [34/50], Train Loss: 1488664809.6480\n",
      "Epoch [35/50], Train Loss: 1488615758.3526\n",
      "Epoch [36/50], Train Loss: 1488567962.9942\n",
      "Epoch [37/50], Train Loss: 1488495349.4339\n",
      "Epoch [38/50], Train Loss: 1488466819.5393\n",
      "Epoch [39/50], Train Loss: 1488416725.0039\n",
      "Epoch [40/50], Train Loss: 1488351656.2833\n",
      "Epoch [41/50], Train Loss: 1488283501.6679\n",
      "Epoch [42/50], Train Loss: 1488228943.3389\n",
      "Epoch [43/50], Train Loss: 1488196455.7372\n",
      "Epoch [44/50], Train Loss: 1488117979.1185\n",
      "Epoch [45/50], Train Loss: 1488069540.7553\n",
      "Epoch [46/50], Train Loss: 1488010932.9404\n",
      "Epoch [47/50], Train Loss: 1487973938.8470\n",
      "Epoch [48/50], Train Loss: 1487893113.0089\n",
      "Epoch [49/50], Train Loss: 1487875749.7484\n",
      "Epoch [50/50], Train Loss: 1487800924.4878\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lstm_lr)\n",
    "\n",
    "model.train()\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(lstm_epochs):\n",
    "    total_train_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        src = data[0]\n",
    "        trg = data[1]\n",
    "\n",
    "        if trg.sum() != 0: \n",
    "            output = model(src)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0]\n",
    "            trg = data[1]\n",
    "\n",
    "            if trg.sum() != 0:\n",
    "                output = model(src)\n",
    "\n",
    "                val_loss = criterion(output, trg)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{lstm_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "torch.save(model, f'../데이터/Checkpoint/lstm_lr_{lstm_lr}_batch_{lstm_batch}_epochs_{lstm_epochs}_hdim_{hidden_dim}_odim_{output_dim}_ws_{window_size}.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../데이터/Checkpoint/embedding_tr_0.8_lr_0.001_batch_32_epochs_150_e1_128_e2_128_e3_512_emb_1024_d1512_d2_256_d3_128.pth')\n",
    "dataset = LSTM_Dataset(model, table_1, table_2, table_3, embedding_dim, window_size)\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=lstm_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=lstm_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     23\u001b[0m         train_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 24\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[1;32m     27\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     adam(\n\u001b[1;32m    164\u001b[0m         params_with_grad,\n\u001b[1;32m    165\u001b[0m         grads,\n\u001b[1;32m    166\u001b[0m         exp_avgs,\n\u001b[1;32m    167\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    168\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    169\u001b[0m         state_steps,\n\u001b[1;32m    170\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    171\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    172\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    173\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    174\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    175\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    176\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    177\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    178\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    179\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    180\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    181\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    182\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m func(params,\n\u001b[1;32m    312\u001b[0m      grads,\n\u001b[1;32m    313\u001b[0m      exp_avgs,\n\u001b[1;32m    314\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    315\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    316\u001b[0m      state_steps,\n\u001b[1;32m    317\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    318\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    319\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    320\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    321\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    322\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    323\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    324\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    325\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    326\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    327\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:385\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    384\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 385\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    388\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = NLinear(embedding_dim, window_size).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lstm_lr)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "consecutive_val_loss_increases = 0\n",
    "max_consecutive_val_loss_increases = 3\n",
    "for epoch in range(lstm_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        src = data[0].to(DEVICE)\n",
    "        trg = data[1].to(DEVICE)\n",
    "\n",
    "        if trg.sum() != 0:\n",
    "            output = model(src)\n",
    "\n",
    "            train_loss = criterion(output, trg)\n",
    "            total_train_loss += train_loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0].to(DEVICE)\n",
    "            trg = data[1].to(DEVICE)\n",
    "\n",
    "            if trg.sum() != 0:\n",
    "                output = model(src)\n",
    "\n",
    "                val_loss = criterion(output, trg)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    if len(val_losses) > 1 and val_losses[-1] > val_losses[-2]:\n",
    "        consecutive_val_loss_increases += 1\n",
    "        if consecutive_val_loss_increases >= max_consecutive_val_loss_increases:\n",
    "            torch.save(model, f'../데이터/Checkpoint/nlinear_tr_{train_ratio}_lr_{lstm_lr}_batch_{lstm_batch}_epochs_{lstm_epochs}_emb_{embedding_dim}_ws_{window_size}.pth')\n",
    "            print(f\"Early Stopping Triggered!\")\n",
    "            break\n",
    "    else:\n",
    "        consecutive_val_loss_increases = 0\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{lstm_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../데이터/Checkpoint/embedding_train_0.8_lr_0.01_batch_32_epochs_50_dim_6.pth')\n",
    "dataset = Attention_Dataset(model, table_1, table_2, table_3, embedding_dim, window_size)\n",
    "\n",
    "train_ratio = 0.8\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('C:/Users/hkyoo/OneDrive/바탕 화면/SCI/데이터/Checkpoint/embedding_lr_0.01_batch_32_epochs_50_dim_6.pth')\n",
    "dataset = Attention_Dataset(model, table_1, table_2, table_3, embedding_dim, window_size)\n",
    "dataloader = DataLoader(dataset, batch_size=lstm_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0254], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = 38\n",
    "window_size = 5\n",
    "mx_len = 1\n",
    "input = torch.randn(num, window_size, embedding_dim)\n",
    "\n",
    "model = LSTMSeq2Seq(embedding_dim, hidden_dim, output_dim, DEVICE)\n",
    "model(input,1, mx_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# 학습\n",
    "history = {'train_loss':[], 'val_loss':[], 'lr':[]}\n",
    "\n",
    "for epoch in range(att_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    num = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        src = batch[0][0].to(DEVICE)\n",
    "        max_len = batch[1][0].to(DEVICE)\n",
    "        anw = batch[2][0]\n",
    "        trg = batch[3][0].to(DEVICE)\n",
    "        \n",
    "        if len(anw)==0:\n",
    "            continue\n",
    "        \n",
    "        num += len(anw)\n",
    "        \n",
    "        dong_loss = 0\n",
    "        for index in anw:\n",
    "            index.to(DEVICE)\n",
    "            output = model(src, index, max_len)\n",
    "            loss = criterion(output, trg)\n",
    "            # dong_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # optimizer.zero_grad()\n",
    "        # # dong_loss /= len(anw)\n",
    "        # dong_loss = torch.tensor(dong_loss, requires_grad=True).to(DEVICE)\n",
    "        # dong_loss.backward()\n",
    "        # optimizer.step()\n",
    "    train_loss = epoch_loss / num\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    \n",
    "    \n",
    "\n",
    "#     if epoch%valid_every==0:\n",
    "#         print(\"==========================\")\n",
    "#         model.eval()\n",
    "#         epoch_loss = 0\n",
    "#         valid_num = 1e-9\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for i, batch in enumerate(val_loader):\n",
    "#                 src = batch[0].to(DEVICE)\n",
    "#                 trg = batch[1].to(DEVICE)\n",
    "#                 if(trg != 0):\n",
    "#                     output = model(src, trg)\n",
    "#                     loss = criterion(output, trg)\n",
    "#                     epoch_loss += loss.item()\n",
    "#                     valid_num += 1\n",
    "#         valid_loss = epoch_loss / valid_num\n",
    "            \n",
    "#         if valid_loss < best_valid_loss:\n",
    "#             best_valid_loss = valid_loss\n",
    "#             model.decoder.t=0\n",
    "#             torch.save(model.state_dict(), 'lstm-model.pt')\n",
    "#         print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
    "\n",
    "#         history['train_loss'].append(train_loss)\n",
    "#         history['val_loss'].append(valid_loss)\n",
    "#         history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('C:/Users/hkyoo/OneDrive/바탕 화면/SCI/데이터/Checkpoint/embedding_lr_0.01_batch_32_epochs_50_dim_6.pth')\n",
    "\n",
    "dataset = LSTM_Dataset(model, table_1, table_2, table_3, embedding_dim, window_size)\n",
    "dataloader = DataLoader(dataset, batch_size=lstm_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_dim, out_dim, nhead, nlayers\n",
    "model = TFModel(embedding_dim, window_size, output_dim, 2, 2)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lstm_lr)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(lstm_epochs):\n",
    "    epoch_loss = 0\n",
    "    train_num = 1e-9\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        src = batch[0].to(DEVICE)\n",
    "        trg = batch[1].to(DEVICE)\n",
    "        if(trg != 0):\n",
    "            train_num += 1\n",
    "            optimizer.zero_grad()\n",
    "            src_mask = model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "            output = model(src, src_mask)\n",
    "            loss = criterion(output[0], trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "    train_loss = epoch_loss / train_num\n",
    "    print(f'Epoch [{epoch+1}/{embedding_epochs}], Train Loss: {train_loss:.4f}')\n",
    "\n",
    "torch.save(model, f'../데이터/Checkpoint/lstm_lsr_{lstm_lr}_batch_{lstm_batch}_epochs_{lstm_epochs}_hdim_{hidden_dim}_odim_{output_dim}_ws{window_size}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
