{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from Dataset.Economy_Dataset import Economy_Dataset\n",
    "from Dataset.RNN_Transaction_Dataset import RNN_Transaction_Dataset\n",
    "from Dataset.NODE_Transaction_Dataset import NODE_Transaction_Dataset\n",
    "\n",
    "from Model.LSTM import LSTM\n",
    "from Model.NODE import NODE\n",
    "from Model.ODEF import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5\n",
    "hidden_size = 256\n",
    "output_size = 1\n",
    "\n",
    "lr = 1e-3\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 부동산 & 경제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df = pd.read_excel('../데이터/Transaction/transaction_final.xlsx', index_col=0)\n",
    "economy_df = pd.read_excel('../데이터/Economy/economy_all.xlsx')\n",
    "economy_df = economy_df['국고채금리']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\RNN_Transaction_Dataset.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'])\n",
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\RNN_Transaction_Dataset.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'])\n",
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\Economy_Dataset.py:11: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xe (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  self.economy_x = torch.FloatTensor(economy_x)\n"
     ]
    }
   ],
   "source": [
    "trainsaction_train_size = int(len(transaction_df)*0.7)\n",
    "trainsaction_val_size = int(len(transaction_df)*0.3)\n",
    "\n",
    "transaction_train_dataset = RNN_Transaction_Dataset(transaction_df[:trainsaction_train_size])\n",
    "transaction_train_loader = DataLoader(transaction_train_dataset, batch_size=2)\n",
    "transaction_val_dataset = RNN_Transaction_Dataset(transaction_df[trainsaction_train_size:])\n",
    "transaction_val_loader = DataLoader(transaction_val_dataset, batch_size=2)\n",
    "\n",
    "economy_train_size = int(len(economy_df)*0.7)\n",
    "economy_val_size = int(len(economy_df)*0.3)\n",
    "\n",
    "economy_train_dataset = Economy_Dataset(economy_df[:economy_train_size])\n",
    "economy_train_loader = DataLoader(economy_train_dataset, batch_size=2)\n",
    "economy_val_dataset = Economy_Dataset(economy_df[economy_train_size:])\n",
    "economy_val_loader = DataLoader(economy_val_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경제 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1, Training Loss: 0.002730900188907981, Validation Loss: 0.3143093644015106\n",
      "Epoch 1/1, Training Loss: 0.0027428490575402975, Validation Loss: 0.31638226120186774\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf') \n",
    "for epoch in range(num_epochs + 1):\n",
    "    model.train()\n",
    "    for batch_idx, samples in enumerate(economy_train_loader):\n",
    "        economy_x_train, economy_y_train = samples\n",
    "\n",
    "        prediction, hidden = model(economy_x_train)\n",
    "        cost = criterion(prediction, economy_y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, samples in enumerate(economy_val_loader):\n",
    "            economy_x_val, economy_y_val = samples\n",
    "\n",
    "            prediction, hidden = model(economy_x_val)\n",
    "            loss = criterion(prediction, economy_y_val)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(economy_val_loader)\n",
    "    print(f'Epoch {epoch}/{num_epochs}, Training Loss: {cost.item()}, Validation Loss: {val_loss}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), '../데이터/Checkpoint/best_rnn_economy_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 부동산 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1, Training Loss: 28219.5234375, Validation Loss: 5213844.961625429\n",
      "Epoch 1/1, Training Loss: 27884.662109375, Validation Loss: 5179137.586078938\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf') \n",
    "for epoch in range(num_epochs + 1):\n",
    "    model.train()\n",
    "    for batch_idx, samples in enumerate(transaction_train_loader):\n",
    "        dong_x_train, dong_y_train = samples\n",
    "\n",
    "        prediction, hidden = model(dong_x_train)\n",
    "        cost = criterion(prediction, dong_y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, samples in enumerate(transaction_val_loader):\n",
    "            x_val, y_val = samples\n",
    "\n",
    "            prediction, hidden = model(x_val)\n",
    "            loss = criterion(prediction, y_val)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(transaction_val_loader)\n",
    "    print(f'Epoch {epoch}/{num_epochs}, Training Loss: {cost.item()}, Validation Loss: {val_loss}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), '../데이터/Checkpoint/best_rnn_transaction_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NODE 부동산 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 다운로드\n",
    "batch_size = 2\n",
    "transaction_df = pd.read_excel('../데이터/Transaction/transaction_final.xlsx', index_col=0)[:300]\n",
    "\n",
    "train_dataset = NODE_Transaction_Dataset(transaction_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 크기 : torch.Size([2, 5])\n",
      "Y 크기 : torch.Size([2, 5])\n",
      "Z 크기 : torch.Size([2, 1])\n",
      "W 크기 : torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "for x,y,z,w in train_loader:\n",
    "  print(\"X 크기 : {}\".format(x.shape))\n",
    "  print(\"Y 크기 : {}\".format(y.shape))\n",
    "  print(\"Z 크기 : {}\".format(z.shape))\n",
    "  print(\"W 크기 : {}\".format(w.shape))\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu is available\n",
      "작동하는지 실험\n",
      "torch.Size([2, 64])\n",
      "(tensor([[[-0.2702],\n",
      "         [-0.8058]],\n",
      "\n",
      "        [[-0.2397],\n",
      "         [-0.6825]],\n",
      "\n",
      "        [[-0.4590],\n",
      "         [-0.5927]],\n",
      "\n",
      "        [[-0.3633],\n",
      "         [-1.0265]],\n",
      "\n",
      "        [[-0.2411],\n",
      "         [-0.6443]]], grad_fn=<SliceBackward0>), tensor([[[ 0.2714,  0.3164,  0.2589,  ..., -0.2647, -0.0451,  0.4345],\n",
      "         [-0.3265,  0.4037, -0.2012,  ..., -0.2224, -0.1503,  0.7907]],\n",
      "\n",
      "        [[ 0.3367,  0.1995,  0.2425,  ..., -0.3627, -0.1541,  0.3290],\n",
      "         [-0.0362,  0.4016, -0.8405,  ..., -0.4509, -0.1558,  0.9271]],\n",
      "\n",
      "        [[ 0.4387,  0.0225, -0.0503,  ..., -0.1694,  0.3831,  0.8487],\n",
      "         [ 0.0519,  0.2897, -1.3085,  ..., -0.4478,  0.0257,  1.1122]],\n",
      "\n",
      "        [[-0.7968,  1.4352, -0.5435,  ...,  0.1792,  1.5636,  1.4129],\n",
      "         [-1.9387,  0.4602, -0.2517,  ..., -0.8022,  1.0323,  0.4069]],\n",
      "\n",
      "        [[ 0.3737,  0.1110,  0.1716,  ..., -0.3896, -0.1225,  0.3373],\n",
      "         [ 0.0209,  0.3741, -1.0397,  ..., -0.4743, -0.1095,  0.9906]],\n",
      "\n",
      "        [[-1.0089,  1.6209, -0.6791,  ...,  0.1525,  1.7598,  1.4345],\n",
      "         [-2.7352,  0.7620, -0.5518,  ..., -1.9089,  1.3595, -0.1818]]],\n",
      "       grad_fn=<ViewBackward0>), tensor([[ 0.4039, -1.2989, -1.8978,  0.9210,  0.6373, -0.4396, -1.7212,  0.6259,\n",
      "         -0.7029, -1.7821,  1.3166,  0.3121, -0.1767, -0.5921,  1.0282, -0.3416,\n",
      "          0.8324,  2.1637,  1.1038, -1.2187, -1.4006,  0.5053, -0.7415, -0.1048,\n",
      "         -2.1153,  0.3959,  2.3859, -1.0316, -0.4465, -0.0625,  1.6550,  0.3629,\n",
      "         -0.6610,  0.6915,  1.2188, -0.9100, -1.9658, -0.9596, -0.9504, -2.1754,\n",
      "         -1.7132, -0.5715, -0.6395,  0.3003, -0.4945, -0.3164,  1.6091,  0.8823,\n",
      "         -0.6507,  1.3642,  0.9939, -1.4382,  0.0192,  0.0204,  0.3755,  0.8328,\n",
      "         -0.1299,  0.3320,  2.3088, -0.1316,  0.5798,  1.0740,  0.7091,  0.1462],\n",
      "        [ 0.2103,  2.0691, -0.6182, -0.7594,  0.2932, -0.6649, -2.1245, -0.5463,\n",
      "         -0.2861, -1.8788,  0.4512, -0.7492,  0.2802,  0.5789, -0.1494, -0.0645,\n",
      "          0.3448,  0.0842,  2.2251, -0.0264,  0.6390,  0.0425, -1.3368,  0.8189,\n",
      "         -0.5037, -2.2106, -0.7587, -0.8075,  0.3866, -0.5187,  1.5151, -1.3502,\n",
      "          0.6478, -0.4202, -0.3920, -0.0925, -0.4576, -0.1416, -0.3025, -1.2982,\n",
      "         -0.4554, -1.0432,  0.3583,  0.1248,  1.1273, -0.9836,  0.7923, -0.5398,\n",
      "          0.8829, -0.5035,  0.7747, -1.3999, -0.1552, -0.5416,  0.4232, -0.1402,\n",
      "          1.5043,  1.3255,  0.2805,  0.0983, -0.1966,  0.2914,  0.5269, -0.6434]],\n",
      "       grad_fn=<AddBackward0>), tensor([[-7.2039e-03, -2.3928e-02, -1.4698e-02,  3.4406e-02, -3.4206e-02,\n",
      "         -5.3296e-02, -7.2349e-02, -6.3346e-02,  4.3040e-02, -2.1609e-02,\n",
      "          4.4425e-03, -7.3611e-02, -8.8960e-02, -1.0373e-04, -7.7189e-02,\n",
      "         -6.7130e-02,  7.1359e-02, -1.0155e-01,  5.3104e-03,  6.7438e-03,\n",
      "          2.5960e-02,  7.0365e-02, -4.9010e-02, -1.3001e-02, -6.5872e-02,\n",
      "         -8.4367e-02, -5.4697e-02, -8.7525e-02,  3.8977e-04, -6.1695e-02,\n",
      "          2.2402e-03,  7.5598e-02, -7.4927e-02,  8.9828e-02,  7.5181e-02,\n",
      "         -1.5466e-01,  3.6124e-02,  5.6678e-02, -1.3474e-02,  3.2019e-03,\n",
      "         -9.7761e-03, -7.3951e-02,  3.1893e-02, -8.2908e-02,  9.4935e-03,\n",
      "          9.2950e-02, -4.8523e-02,  4.4647e-02, -6.4956e-02,  4.0541e-02,\n",
      "         -6.0558e-03,  1.1887e-02,  8.5946e-02,  5.5893e-02,  2.2946e-02,\n",
      "         -1.0622e-02,  4.6680e-02, -6.6108e-02,  5.2542e-02,  1.8601e-02,\n",
      "          2.7363e-02, -3.3097e-02, -4.4607e-02,  6.5851e-02],\n",
      "        [ 3.6065e-03, -4.0661e-02, -2.5358e-03,  4.6481e-02, -3.5518e-02,\n",
      "         -7.3111e-02, -5.8327e-02, -5.4220e-02,  3.7546e-02, -2.8194e-03,\n",
      "          6.6105e-04, -8.6890e-02, -8.0858e-02, -1.3091e-02, -9.2022e-02,\n",
      "         -5.0368e-02,  6.0544e-02, -7.6083e-02, -1.6917e-02, -5.9011e-04,\n",
      "          2.0328e-02,  6.2211e-02, -5.2939e-02,  3.1317e-03, -5.4861e-02,\n",
      "         -7.1554e-02, -6.4045e-02, -1.0131e-01,  1.2762e-02, -4.5672e-02,\n",
      "          2.0350e-02,  1.0375e-01, -9.1428e-02,  8.6247e-02,  8.7272e-02,\n",
      "         -1.5666e-01,  3.2125e-02,  4.7771e-02, -8.0353e-03, -1.5418e-03,\n",
      "         -1.1909e-03, -7.6765e-02,  4.5595e-02, -7.8818e-02,  3.2948e-02,\n",
      "          7.3848e-02, -3.8930e-02,  4.6706e-02, -5.5438e-02,  3.8816e-02,\n",
      "          3.0704e-02,  2.5870e-02,  8.9649e-02,  5.2753e-02,  2.7719e-02,\n",
      "         -7.0265e-03,  5.6682e-02, -6.0452e-02,  7.2199e-02,  2.2508e-02,\n",
      "          4.5901e-02, -3.3239e-02, -4.2394e-02,  6.5454e-02]],\n",
      "       grad_fn=<SliceBackward0>), tensor([[-0.0172,  0.0487,  0.0369,  0.0007,  0.0582,  0.0139,  0.0976, -0.0317,\n",
      "         -0.0546, -0.0180,  0.0568, -0.0293, -0.0701, -0.0277, -0.0665,  0.0405,\n",
      "         -0.0644,  0.0255,  0.0919, -0.0484, -0.0295,  0.0364,  0.0477, -0.0204,\n",
      "          0.0138, -0.0277,  0.0641,  0.0532,  0.0002, -0.1170,  0.0526,  0.1510,\n",
      "         -0.0292, -0.1127, -0.0584,  0.0247,  0.0105,  0.0517,  0.0119,  0.0312,\n",
      "         -0.0397,  0.0843, -0.0165,  0.0003, -0.0202, -0.0469,  0.0122, -0.0822,\n",
      "         -0.0059, -0.0437,  0.0197, -0.0691, -0.0430,  0.0710,  0.0074,  0.0311,\n",
      "          0.0077, -0.0365, -0.0361, -0.0283, -0.0583,  0.0169, -0.0492, -0.0758],\n",
      "        [-0.0133,  0.0453,  0.0224,  0.0005,  0.0557,  0.0237,  0.1173, -0.0272,\n",
      "         -0.0753, -0.0468,  0.0691, -0.0350, -0.0634, -0.0184, -0.0905,  0.0234,\n",
      "         -0.0756,  0.0278,  0.1056, -0.0543, -0.0271,  0.0575,  0.0438, -0.0399,\n",
      "          0.0256, -0.0237,  0.0379,  0.0578,  0.0128, -0.0966,  0.0629,  0.1466,\n",
      "         -0.0245, -0.1033, -0.0659,  0.0039,  0.0072,  0.0381,  0.0191,  0.0069,\n",
      "         -0.0421,  0.0823, -0.0196, -0.0010, -0.0372, -0.0588,  0.0090, -0.0771,\n",
      "         -0.0066, -0.0461,  0.0388, -0.0660, -0.0439,  0.0674,  0.0082,  0.0251,\n",
      "          0.0176, -0.0294, -0.0184, -0.0153, -0.0728,  0.0189, -0.0551, -0.0603]],\n",
      "       grad_fn=<SliceBackward0>), tensor([[-0.3449],\n",
      "        [-1.0859]], grad_fn=<SelectBackward0>))\n",
      "torch.Size([5, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# 데이터 & 모델에 device 붙임!!!\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device} is available')\n",
    "\n",
    "model = NODE(output_dim=1,hidden_dim=256,latent_dim=64).to(device)\n",
    "\n",
    "print('작동하는지 실험')\n",
    "basic_data = torch.rand((5,2,1))  # window_size, batch_size, 1\n",
    "time = torch.FloatTensor([[1,2,3,6,10,12],[1,3,5,8,10,12]]).reshape(6,2,1) # window_size, batch_size, 1\n",
    "data = model(basic_data,time)\n",
    "print(data)\n",
    "print(data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "noise_std = 0.02\n",
    "optim = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), lr=0.001)\n",
    "\n",
    "num_epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0\n",
      "torch.Size([2, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ../데이터/checkpoint does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[39mif\u001b[39;00m loss \u001b[39m<\u001b[39m best_train_loss:\n\u001b[0;32m     23\u001b[0m         best_train_loss \u001b[39m=\u001b[39m loss\n\u001b[1;32m---> 24\u001b[0m         torch\u001b[39m.\u001b[39;49msave(model\u001b[39m.\u001b[39;49mstate_dict(), \u001b[39m'\u001b[39;49m\u001b[39m../데이터/checkpoint/best_ODE_transaction_model.pth\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     26\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mloss : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, best loss : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(loss, train_best_loss))\n\u001b[0;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-----------------------------------------------------\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hkyoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 440\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hkyoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     container \u001b[39m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m container(name_or_buffer)\n",
      "File \u001b[1;32mc:\\Users\\hkyoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileWriter(\u001b[39mstr\u001b[39;49m(name)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory ../데이터/checkpoint does not exist."
     ]
    }
   ],
   "source": [
    "best_train_loss = float('inf') \n",
    "for epoch in range(num_epochs + 1):\n",
    "    print('epoch : ',epoch)\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for batch_idx, samples in enumerate(train_loader):\n",
    "        tran_x, time_x, tran_y, time_y = samples\n",
    "        \n",
    "        t = torch.cat((time_x,time_y),dim=1)  \n",
    "        tran_x = tran_x.transpose(0,1).unsqueeze(2).to(device)\n",
    "        t = t.transpose(0,1).unsqueeze(2).to(device)\n",
    "        \n",
    "        x_p, _, z, z_mean, z_log_var, pred = model(tran_x, t)\n",
    "        kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean**2 - torch.exp(z_log_var), -1)\n",
    "        loss = 0.5 * ((x-x_p)**2).sum(-1).sum(0) / noise_std**2 + kl_loss\n",
    "        loss = torch.mean(loss)\n",
    "        loss /= window_size\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if loss < best_train_loss:\n",
    "            best_train_loss = loss\n",
    "            torch.save(model.state_dict(), \"../데이터/checkpoint/best_ODE_transaction_model.pth\")\n",
    "        \n",
    "        print('loss : {}, best loss : {}'.format(loss, train_best_loss))\n",
    "    print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 100, 1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.7250e+03,  0.0000e+00],\n",
      "         [ 2.7500e+03,  0.0000e+00]],\n",
      "\n",
      "        [[ 2.7500e+03,  2.5936e-01],\n",
      "         [ 2.8333e+03, -6.5739e-01]],\n",
      "\n",
      "        [[ 2.8333e+03, -6.3059e-01],\n",
      "         [ 3.3958e+03, -7.7510e-02]],\n",
      "\n",
      "        [[ 3.3958e+03,  1.8642e-01],\n",
      "         [ 3.6667e+03,  6.2826e-01]],\n",
      "\n",
      "        [[ 3.6667e+03,  3.8002e-01],\n",
      "         [ 3.7500e+03, -2.6122e-01]]])\n"
     ]
    }
   ],
   "source": [
    "x = tran_x\n",
    "t = torch.rand((5,2,1))\n",
    "t[1:] = t[:-1] - t[1:]\n",
    "t[0] = 0.\n",
    "xt = torch.cat((x, t), dim=-1)\n",
    "print(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 2])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 2])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt.flip((0,)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nn.GRU(2,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0092,  0.0062,  0.0061,  ...,  0.0027, -0.0018, -0.0036],\n",
       "          [-0.0110,  0.0203,  0.0324,  ..., -0.0099,  0.0145, -0.0037]],\n",
       " \n",
       "         [[-0.0114,  0.0226,  0.0304,  ..., -0.0139,  0.0178,  0.0106],\n",
       "          [-0.0110,  0.0228,  0.0388,  ..., -0.0168,  0.0199,  0.0093]],\n",
       " \n",
       "         [[-0.0142,  0.0212,  0.0260,  ..., -0.0132,  0.0153,  0.0155],\n",
       "          [-0.0204,  0.0326,  0.0490,  ..., -0.0218,  0.0256,  0.0067]],\n",
       " \n",
       "         [[-0.0174,  0.0304,  0.0374,  ..., -0.0209,  0.0242,  0.0200],\n",
       "          [-0.0094,  0.0228,  0.0363,  ..., -0.0228,  0.0216,  0.0292]],\n",
       " \n",
       "         [[-0.0182,  0.0416,  0.0585,  ..., -0.0329,  0.0378,  0.0254],\n",
       "          [-0.0159,  0.0261,  0.0343,  ..., -0.0204,  0.0196,  0.0249]]],\n",
       "        grad_fn=<StackBackward0>),\n",
       " tensor([[[-1.8189e-02,  4.1593e-02,  5.8491e-02, -2.7725e-02, -8.1069e-02,\n",
       "           -5.3336e-02,  9.4525e-02,  4.7108e-02,  5.6965e-02,  7.0807e-02,\n",
       "           -1.0543e-02,  2.9052e-02,  2.8545e-02,  1.3226e-02, -5.2441e-02,\n",
       "           -2.5634e-02,  6.4114e-02,  1.0975e-02,  6.6405e-02, -5.7458e-02,\n",
       "           -6.9070e-02,  9.7897e-03, -7.0158e-02,  9.1960e-02,  7.9110e-03,\n",
       "            5.9716e-02,  5.0264e-02,  4.3460e-02,  6.9637e-02,  7.1133e-02,\n",
       "            6.3734e-02, -6.1091e-02,  1.4101e-02,  4.5826e-03, -9.2994e-02,\n",
       "           -2.5479e-02, -6.8129e-02,  3.8282e-02,  7.6770e-02,  2.3878e-02,\n",
       "           -4.5916e-02,  4.8254e-02, -2.4576e-02,  4.6858e-02, -6.1449e-02,\n",
       "            2.4680e-02,  3.8379e-02,  7.0066e-02, -2.9567e-02, -3.7746e-02,\n",
       "           -4.6941e-02, -2.2197e-02,  5.1930e-02, -5.2322e-02,  1.0817e-02,\n",
       "            2.8738e-02, -5.8614e-02, -5.1175e-02,  2.0866e-05,  8.1490e-02,\n",
       "            6.3430e-02,  5.0322e-02,  1.3180e-02,  2.6476e-02,  4.3784e-02,\n",
       "           -1.5527e-02,  5.9795e-02,  9.1787e-02, -4.5471e-02, -3.4049e-02,\n",
       "           -4.8281e-02,  2.6433e-03, -4.0980e-02,  6.3828e-02, -1.8466e-03,\n",
       "           -5.1639e-02, -1.8156e-02, -5.1133e-02, -6.4160e-02,  9.7291e-03,\n",
       "            4.9911e-02,  4.1145e-02, -2.6470e-04,  3.9405e-03, -7.4764e-02,\n",
       "            1.4958e-04, -9.6749e-03, -2.1184e-02,  1.4111e-02, -2.8397e-02,\n",
       "           -5.4265e-03, -4.6981e-02,  8.9801e-02, -2.2412e-02, -3.9786e-02,\n",
       "            7.0367e-02, -1.2709e-02, -7.5032e-03,  1.6910e-02,  9.8245e-03,\n",
       "           -3.1946e-02, -2.2876e-02,  1.6230e-03,  5.1723e-02,  6.5664e-02,\n",
       "           -1.0852e-02, -5.5896e-02,  4.5133e-02,  9.4286e-02,  6.4588e-03,\n",
       "           -1.5984e-02,  1.7812e-02, -6.5955e-02,  3.8493e-02,  7.3120e-02,\n",
       "           -4.6608e-02,  6.8459e-02, -1.0520e-02,  3.8727e-02, -3.8020e-02,\n",
       "            8.4364e-03, -1.6113e-02, -5.5877e-02, -1.8302e-02,  9.8978e-03,\n",
       "           -1.5827e-02, -2.4826e-02,  1.8441e-02,  3.1106e-02, -9.6584e-03,\n",
       "           -9.9158e-03,  6.3701e-02,  6.0246e-02,  3.3244e-02, -5.0820e-02,\n",
       "           -7.2079e-02,  2.6335e-02,  1.0257e-03,  3.9707e-02,  7.9325e-02,\n",
       "           -3.9423e-02, -5.5166e-02,  3.6036e-02, -4.3186e-02,  8.3471e-02,\n",
       "           -8.8476e-02, -4.5980e-02,  1.9840e-02, -6.8017e-02, -7.1010e-03,\n",
       "            3.6329e-02, -8.8197e-03,  3.6395e-02, -6.2977e-02, -6.0227e-03,\n",
       "            2.4500e-03,  6.8318e-02, -7.3251e-02,  1.0697e-02, -4.5303e-02,\n",
       "            1.0407e-01,  1.2019e-04,  8.2298e-02, -3.2868e-02,  4.3680e-02,\n",
       "           -3.5388e-03,  1.5587e-02,  1.9842e-02, -5.8227e-03, -2.1758e-02,\n",
       "           -8.3712e-02,  2.1010e-02, -6.5820e-02,  1.9694e-03,  7.5669e-02,\n",
       "            4.5811e-02,  5.2446e-02, -1.5372e-02, -8.9316e-02,  7.4299e-02,\n",
       "           -1.0428e-01, -7.9332e-02,  4.4477e-02,  7.2041e-02,  4.2917e-02,\n",
       "            5.1750e-04,  5.3118e-03,  4.2499e-02, -8.0363e-02, -3.9629e-02,\n",
       "            4.7676e-02, -2.3312e-02,  6.5039e-02,  2.0650e-02,  9.9026e-02,\n",
       "           -5.1129e-02,  1.6648e-02, -3.0429e-02,  4.0325e-02, -4.1543e-04,\n",
       "            7.5920e-02,  9.6948e-03,  7.9110e-03,  5.8990e-02, -5.9367e-02,\n",
       "           -6.7706e-02,  5.7167e-02,  3.9120e-04, -7.1231e-02,  1.0330e-02,\n",
       "            1.6509e-02,  4.2951e-02,  4.0556e-02,  4.0014e-02,  1.4745e-02,\n",
       "            6.0040e-02,  8.2229e-02,  1.4669e-02,  3.3364e-02,  6.6880e-03,\n",
       "           -2.1914e-02,  5.7243e-02, -2.9495e-02,  3.0988e-02, -5.1971e-02,\n",
       "            4.6434e-02, -3.6973e-02,  9.6637e-04,  6.1390e-02,  7.5268e-03,\n",
       "           -4.3817e-02,  4.1665e-02,  1.0645e-01,  8.8602e-02,  4.7561e-02,\n",
       "           -9.4170e-03,  1.6569e-03,  8.9971e-02, -2.9422e-02,  3.2403e-02,\n",
       "            5.6647e-03,  9.1883e-02,  5.7968e-02, -5.5408e-03,  1.6489e-02,\n",
       "           -7.4748e-02, -5.8511e-02,  3.5868e-02, -4.2977e-02,  5.0882e-02,\n",
       "           -2.9160e-02, -2.2428e-02, -6.5194e-03, -3.2949e-02,  3.7838e-02,\n",
       "            2.5408e-02],\n",
       "          [-1.5856e-02,  2.6076e-02,  3.4251e-02, -3.0859e-03, -8.9907e-02,\n",
       "           -4.0120e-02,  1.0694e-01,  2.2494e-02,  6.3136e-02,  6.8625e-02,\n",
       "           -2.2017e-02,  5.0109e-02,  4.2298e-02,  4.2821e-02, -5.0363e-02,\n",
       "           -2.6692e-02,  7.3129e-02, -1.2037e-02,  6.1456e-02, -6.5086e-02,\n",
       "           -8.6476e-02,  2.4356e-02, -8.1821e-02,  9.9901e-02,  5.7194e-03,\n",
       "            6.9051e-02,  5.3411e-02,  2.1015e-02,  7.9932e-02,  6.9914e-02,\n",
       "            5.7684e-02, -3.5888e-02,  3.0320e-02, -2.1279e-02, -6.2508e-02,\n",
       "           -1.0526e-02, -5.2558e-02,  2.7704e-02,  6.2754e-02,  1.5556e-02,\n",
       "           -5.1795e-02,  7.3231e-02, -1.7279e-02,  4.9615e-02, -5.6468e-02,\n",
       "            4.7434e-02,  3.4584e-02,  7.5380e-02, -3.9321e-02, -5.1472e-02,\n",
       "           -3.4226e-02, -3.6271e-02,  4.5065e-02, -8.1967e-02,  1.1836e-03,\n",
       "            1.3929e-02, -7.0080e-02, -4.0305e-02, -7.7254e-03,  9.5891e-02,\n",
       "            8.3084e-02,  3.4822e-02,  2.8862e-02,  2.0425e-02,  4.2647e-02,\n",
       "           -3.1545e-02,  5.8915e-02,  7.9426e-02, -5.1371e-02, -2.8920e-02,\n",
       "           -3.1850e-02,  7.3429e-04, -2.5097e-02,  6.5786e-02, -5.8929e-03,\n",
       "           -8.1068e-02, -2.4907e-02, -6.3437e-02, -6.5055e-02,  2.1012e-02,\n",
       "            5.6731e-02,  2.2977e-02, -2.2119e-02, -5.3104e-03, -5.9596e-02,\n",
       "           -1.1109e-02,  3.0138e-03, -1.9038e-02,  2.7178e-02, -1.1010e-02,\n",
       "            1.4184e-03, -3.6613e-02,  6.2395e-02, -2.6465e-02, -3.8747e-02,\n",
       "            7.1514e-02,  9.4744e-04,  1.1945e-02,  2.6728e-02,  5.9610e-03,\n",
       "           -3.5058e-02, -2.7799e-02, -1.1184e-02,  6.5136e-02,  6.3321e-02,\n",
       "           -3.3794e-02, -3.8079e-02,  4.8455e-02,  8.8637e-02,  2.3057e-03,\n",
       "           -4.3432e-02, -3.4368e-04, -8.4933e-02,  4.8009e-02,  6.8672e-02,\n",
       "           -4.3541e-02,  6.3047e-02,  6.9483e-03,  3.3394e-02, -4.6063e-02,\n",
       "            1.0676e-02, -4.1408e-02, -2.7498e-02, -2.1215e-02, -1.2560e-02,\n",
       "            9.8463e-03, -8.9837e-03,  1.2792e-02,  3.3083e-02,  1.9658e-02,\n",
       "            1.5626e-02,  5.8978e-02,  5.1050e-02,  2.9315e-02, -3.1090e-02,\n",
       "           -5.7728e-02,  1.4604e-03,  5.8761e-03,  2.3137e-02,  6.0778e-02,\n",
       "           -2.8307e-02, -4.5979e-02,  4.0733e-02, -1.6386e-02,  8.0323e-02,\n",
       "           -7.6742e-02, -6.3977e-02,  2.1935e-02, -7.4559e-02,  5.3528e-05,\n",
       "            2.4745e-02,  6.4394e-03,  4.2652e-02, -8.8328e-02, -8.0726e-03,\n",
       "            1.9814e-02,  6.7267e-02, -5.9715e-02,  2.0054e-03, -6.7188e-02,\n",
       "            8.6961e-02,  1.2493e-02,  5.3244e-02, -3.2098e-03,  4.5317e-02,\n",
       "            1.4894e-02,  2.8173e-02,  1.4273e-02, -2.1570e-02, -5.0713e-02,\n",
       "           -7.7807e-02, -7.3034e-03, -6.5787e-02, -2.7509e-04,  4.9929e-02,\n",
       "            5.5646e-02,  4.8904e-02, -2.2266e-02, -6.2193e-02,  7.6994e-02,\n",
       "           -9.6648e-02, -6.9988e-02,  5.1577e-02,  7.6404e-02,  3.7155e-02,\n",
       "           -3.2745e-03, -1.0444e-02,  3.3302e-02, -5.7303e-02, -4.3786e-02,\n",
       "            4.4427e-02, -3.0399e-02,  4.1257e-02,  2.5118e-02,  8.7324e-02,\n",
       "           -5.0485e-02,  2.0375e-02, -3.6302e-02,  3.8918e-02,  1.1209e-02,\n",
       "            7.1755e-02,  6.7851e-03,  1.7929e-02,  5.2257e-02, -6.2323e-02,\n",
       "           -3.8909e-02,  5.3312e-02,  6.4540e-03, -6.2726e-02, -1.4996e-02,\n",
       "            1.8309e-02,  3.4071e-02,  3.4478e-02,  4.3086e-02,  2.3039e-02,\n",
       "            7.1072e-02,  6.7976e-02,  2.2748e-02,  2.5000e-02,  1.1841e-02,\n",
       "           -2.0471e-02,  6.0345e-02, -2.9728e-02,  5.5299e-02, -4.7065e-02,\n",
       "            3.7371e-02, -4.6198e-02,  2.2668e-02,  7.4638e-02,  9.3044e-04,\n",
       "           -4.1819e-02,  4.3232e-02,  9.2980e-02,  5.9042e-02,  3.5437e-02,\n",
       "           -4.7743e-03,  2.5691e-03,  1.0239e-01, -2.9327e-02,  3.9665e-02,\n",
       "            2.8455e-02,  1.0108e-01,  5.5887e-02,  1.3241e-02,  1.3612e-02,\n",
       "           -8.5477e-02, -3.3875e-02,  2.0270e-02, -2.5716e-02,  5.6822e-02,\n",
       "           -2.6539e-02, -2.5445e-02, -3.3774e-02, -2.0443e-02,  1.9551e-02,\n",
       "            2.4871e-02]]], grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt = torch.rand((5,2,2))\n",
    "g(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt.flip((0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m m \u001b[39m=\u001b[39m RNNEncoder(\u001b[39m1\u001b[39m,\u001b[39m256\u001b[39m,\u001b[39m64\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m m(tran_x,t[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\hkyoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[12], line 18\u001b[0m, in \u001b[0;36mRNNEncoder.forward\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     15\u001b[0m t[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[0;32m     16\u001b[0m xt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x, t), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m _, h0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(xt\u001b[39m.\u001b[39;49mflip((\u001b[39m0\u001b[39;49m,)))  \u001b[39m# Reversed\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m# Compute latent dimension\u001b[39;00m\n\u001b[0;32m     20\u001b[0m z0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhid2lat(h0[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\hkyoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hkyoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:998\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    996\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    997\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 998\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mgru(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    999\u001b[0m                      \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m   1000\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1001\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mgru(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m   1002\u001b[0m                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "m = RNNEncoder(1,256,64)\n",
    "m(tran_x,t[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tran_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(tran_x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ODE 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn  import functional as F \n",
    "from torch.autograd import Variable\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEF(nn.Module):\n",
    "    def forward_with_grad(self, z, t, grad_outputs):\n",
    "        \"\"\"Compute f and a df/dz, a df/dp, a df/dt\"\"\"\n",
    "        batch_size = z.shape[0]\n",
    "\n",
    "        out = self.forward(z, t)\n",
    "\n",
    "        a = grad_outputs\n",
    "        adfdz, adfdt, *adfdp = torch.autograd.grad(\n",
    "            (out,), (z, t) + tuple(self.parameters()), grad_outputs=(a),\n",
    "            allow_unused=True, retain_graph=True\n",
    "        )\n",
    "        # grad method automatically sums gradients for batch items, we have to expand them back \n",
    "        if adfdp is not None:\n",
    "            adfdp = torch.cat([p_grad.flatten() for p_grad in adfdp]).unsqueeze(0)\n",
    "            adfdp = adfdp.expand(batch_size, -1) / batch_size\n",
    "        if adfdt is not None:\n",
    "            adfdt = adfdt.expand(batch_size, 1) / batch_size\n",
    "        return out, adfdz, adfdt, adfdp\n",
    "\n",
    "    def flatten_parameters(self):\n",
    "        p_shapes = []\n",
    "        flat_parameters = []\n",
    "        for p in self.parameters():\n",
    "            p_shapes.append(p.size())\n",
    "            flat_parameters.append(p.flatten())\n",
    "        return torch.cat(flat_parameters)\n",
    "    \n",
    "class LinearODEF(ODEF):\n",
    "    def __init__(self, W):\n",
    "        super(LinearODEF, self).__init__()\n",
    "        self.lin = nn.Linear(2, 2, bias=False)\n",
    "        self.lin.weight = nn.Parameter(W)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return self.lin(x)\n",
    "    \n",
    "class NeuralODE(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super(NeuralODE, self).__init__()\n",
    "        assert isinstance(func, ODEF)\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, z0, t=Tensor([0., 1.]), return_whole_sequence=False):\n",
    "        t = t.to(z0)\n",
    "        z = ODEAdjoint.apply(z0, t, self.func.flatten_parameters(), self.func)\n",
    "        if return_whole_sequence:\n",
    "            return z\n",
    "        else:\n",
    "            return z[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEAdjoint(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, z0, t, flat_parameters, func):\n",
    "        assert isinstance(func, ODEF)\n",
    "        bs, *z_shape = z0.size()\n",
    "        time_len = t.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = torch.zeros(time_len, bs, *z_shape).to(z0)\n",
    "            z[0] = z0\n",
    "            for i_t in range(time_len - 1):\n",
    "                z0 = ode_solve(z0, t[i_t], t[i_t+1], func)\n",
    "                z[i_t+1] = z0\n",
    "\n",
    "        ctx.func = func\n",
    "        ctx.save_for_backward(t, z.clone(), flat_parameters)\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dLdz):\n",
    "        \"\"\"\n",
    "        dLdz shape: time_len, batch_size, *z_shape\n",
    "        \"\"\"\n",
    "        func = ctx.func\n",
    "        t, z, flat_parameters = ctx.saved_tensors\n",
    "        time_len, bs, *z_shape = z.size()\n",
    "        n_dim = np.prod(z_shape)\n",
    "        n_params = flat_parameters.size(0)\n",
    "\n",
    "        # Dynamics of augmented system to be calculated backwards in time\n",
    "        def augmented_dynamics(aug_z_i, t_i):\n",
    "            \"\"\"\n",
    "            tensors here are temporal slices\n",
    "            t_i - is tensor with size: bs, 1\n",
    "            aug_z_i - is tensor with size: bs, n_dim*2 + n_params + 1\n",
    "            \"\"\"\n",
    "            z_i, a = aug_z_i[:, :n_dim], aug_z_i[:, n_dim:2*n_dim]  # ignore parameters and time\n",
    "\n",
    "            # Unflatten z and a\n",
    "            z_i = z_i.view(bs, *z_shape)\n",
    "            a = a.view(bs, *z_shape)\n",
    "            with torch.set_grad_enabled(True):\n",
    "                t_i = t_i.detach().requires_grad_(True)\n",
    "                z_i = z_i.detach().requires_grad_(True)\n",
    "                func_eval, adfdz, adfdt, adfdp = func.forward_with_grad(z_i, t_i, grad_outputs=a)  # bs, *z_shape\n",
    "                adfdz = adfdz.to(z_i) if adfdz is not None else torch.zeros(bs, *z_shape).to(z_i)\n",
    "                adfdp = adfdp.to(z_i) if adfdp is not None else torch.zeros(bs, n_params).to(z_i)\n",
    "                adfdt = adfdt.to(z_i) if adfdt is not None else torch.zeros(bs, 1).to(z_i)\n",
    "\n",
    "            # Flatten f and adfdz\n",
    "            func_eval = func_eval.view(bs, n_dim)\n",
    "            adfdz = adfdz.view(bs, n_dim) \n",
    "            return torch.cat((func_eval, -adfdz, -adfdp, -adfdt), dim=1)\n",
    "\n",
    "        dLdz = dLdz.view(time_len, bs, n_dim)  # flatten dLdz for convenience\n",
    "        with torch.no_grad():\n",
    "            ## Create placeholders for output gradients\n",
    "            # Prev computed backwards adjoints to be adjusted by direct gradients\n",
    "            adj_z = torch.zeros(bs, n_dim).to(dLdz)\n",
    "            adj_p = torch.zeros(bs, n_params).to(dLdz)\n",
    "            # In contrast to z and p we need to return gradients for all times\n",
    "            adj_t = torch.zeros(time_len, bs, 1).to(dLdz)\n",
    "\n",
    "            for i_t in range(time_len-1, 0, -1):\n",
    "                z_i = z[i_t]\n",
    "                t_i = t[i_t]\n",
    "                f_i = func(z_i, t_i).view(bs, n_dim)\n",
    "\n",
    "                # Compute direct gradients\n",
    "                dLdz_i = dLdz[i_t]\n",
    "                dLdt_i = torch.bmm(torch.transpose(dLdz_i.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0]\n",
    "\n",
    "                # Adjusting adjoints with direct gradients\n",
    "                adj_z += dLdz_i\n",
    "                adj_t[i_t] = adj_t[i_t] - dLdt_i\n",
    "\n",
    "                # Pack augmented variable\n",
    "                aug_z = torch.cat((z_i.view(bs, n_dim), adj_z, torch.zeros(bs, n_params).to(z), adj_t[i_t]), dim=-1)\n",
    "\n",
    "                # Solve augmented system backwards\n",
    "                aug_ans = ode_solve(aug_z, t_i, t[i_t-1], augmented_dynamics)\n",
    "\n",
    "                # Unpack solved backwards augmented system\n",
    "                adj_z[:] = aug_ans[:, n_dim:2*n_dim]\n",
    "                adj_p[:] += aug_ans[:, 2*n_dim:2*n_dim + n_params]\n",
    "                adj_t[i_t-1] = aug_ans[:, 2*n_dim + n_params:]\n",
    "\n",
    "                del aug_z, aug_ans\n",
    "\n",
    "            ## Adjust 0 time adjoint with direct gradients\n",
    "            # Compute direct gradients \n",
    "            dLdz_0 = dLdz[0]\n",
    "            dLdt_0 = torch.bmm(torch.transpose(dLdz_0.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0]\n",
    "\n",
    "            # Adjust adjoints\n",
    "            adj_z += dLdz_0\n",
    "            adj_t[0] = adj_t[0] - dLdt_0\n",
    "        return adj_z.view(bs, *z_shape), adj_t, adj_p, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_solve(z0, t0, t1, f):\n",
    "    \"\"\"\n",
    "    Simplest Euler ODE initial value solver\n",
    "    \"\"\"\n",
    "    h_max = 0.05\n",
    "    n_steps = math.ceil((abs(t1 - t0)/h_max).max().item())\n",
    "\n",
    "    h = (t1 - t0)/n_steps\n",
    "    t = t0\n",
    "    z = z0\n",
    "\n",
    "    for i_step in range(n_steps):\n",
    "        z = z + h * f(z, t)\n",
    "        t = t + h\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as npr\n",
    "\n",
    "def gen_batch(batch_size, n_sample=100):\n",
    "    n_batches = samp_trajs.shape[1] // batch_size\n",
    "    time_len = samp_trajs.shape[0]\n",
    "    n_sample = min(n_sample, time_len)\n",
    "    for i in range(n_batches):\n",
    "        if n_sample > 0:\n",
    "            t0_idx = npr.multinomial(1, [1. / (time_len - n_sample)] * (time_len - n_sample))\n",
    "            t0_idx = np.argmax(t0_idx)\n",
    "            tM_idx = t0_idx + n_sample\n",
    "        else:\n",
    "            t0_idx = 0\n",
    "            tM_idx = time_len\n",
    "\n",
    "        frm, to = batch_size*i, batch_size*(i+1)\n",
    "        yield samp_trajs[t0_idx:tM_idx, frm:to], samp_ts[t0_idx:tM_idx, frm:to]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hkyoo\\AppData\\Local\\Temp\\ipykernel_27012\\2661987758.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  index_np = np.arange(0, n_points, 1, dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "t_max = 6.29*5\n",
    "n_points = 200\n",
    "noise_std = 0.02\n",
    "\n",
    "num_spirals = 1000\n",
    "\n",
    "index_np = np.arange(0, n_points, 1, dtype=np.int)\n",
    "index_np = np.hstack([index_np[:, None]])\n",
    "times_np = np.linspace(0, t_max, num=n_points)\n",
    "times_np = np.hstack([times_np[:, None]] * num_spirals)\n",
    "times = torch.from_numpy(times_np[:, :, None]).to(torch.float32)\n",
    "\n",
    "# Generate random spirals parameters\n",
    "normal01 = torch.distributions.Normal(0, 1.0)\n",
    "\n",
    "x0 = Variable(normal01.sample((num_spirals, 2))) * 2.0  \n",
    "\n",
    "W11 = -0.1 * normal01.sample((num_spirals,)).abs() - 0.05\n",
    "W22 = -0.1 * normal01.sample((num_spirals,)).abs() - 0.05\n",
    "W21 = -1.0 * normal01.sample((num_spirals,)).abs()\n",
    "W12 =  1.0 * normal01.sample((num_spirals,)).abs()\n",
    "\n",
    "xs_list = []\n",
    "for i in range(num_spirals):\n",
    "    if i % 2 == 1: #  Make it counter-clockwise\n",
    "        W21, W12 = W12, W21\n",
    "\n",
    "    func = LinearODEF(Tensor([[W11[i], W12[i]], [W21[i], W22[i]]]))\n",
    "    ode = NeuralODE(func)\n",
    "\n",
    "    xs = ode(x0[i:i+1], times[:, i:i+1], return_whole_sequence=True)\n",
    "    xs_list.append(xs)\n",
    "\n",
    "\n",
    "orig_trajs = torch.cat(xs_list, dim=1).detach()\n",
    "samp_trajs = orig_trajs + torch.randn_like(orig_trajs) * noise_std\n",
    "samp_ts = times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "preload = False\n",
    "n_epochs = 20000\n",
    "batch_size = 100\n",
    "\n",
    "plot_traj_idx = 1\n",
    "plot_traj = orig_trajs[:, plot_traj_idx:plot_traj_idx+1]\n",
    "plot_obs = samp_trajs[:, plot_traj_idx:plot_traj_idx+1]\n",
    "plot_ts = samp_ts[:, plot_traj_idx:plot_traj_idx+1]\n",
    "\n",
    "for epoch_idx in range(n_epochs):\n",
    "    losses = []\n",
    "    train_iter = gen_batch(batch_size)\n",
    "    for x, t in train_iter:\n",
    "\n",
    "        max_len = np.random.choice([30, 50, 100])\n",
    "        permutation = np.random.permutation(t.shape[0])\n",
    "        np.random.shuffle(permutation)\n",
    "        permutation = np.sort(permutation[:max_len])\n",
    "\n",
    "        x, t = x[permutation], t[permutation]\n",
    "        \n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 100, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 100, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NODE(output_dim=2,hidden_dim=256,latent_dim=64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nn.GRU(3,256)\n",
    "t[1:] = t[:-1] - t[1:]\n",
    "t[0] = 0.\n",
    "xt = torch.cat((x, t), dim=-1)\n",
    "\n",
    "_, h0 = g(xt.flip((0,)))  # Reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 100, 3])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt.flip((0,)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 30 but got size 29 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model(x,t)\n",
      "File \u001b[1;32mc:\\Users\\hkyoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hkyoo\\OneDrive\\바탕 화면\\SCI\\코드\\Model\\NODE.py:65\u001b[0m, in \u001b[0;36mNODE.forward\u001b[1;34m(self, x, t, MAP)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, t, MAP\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 65\u001b[0m     z_mean, z_log_var\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x, t[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[0;32m     66\u001b[0m     \u001b[39mif\u001b[39;00m MAP:\n\u001b[0;32m     67\u001b[0m         z \u001b[39m=\u001b[39m z_mean\n",
      "File \u001b[1;32mc:\\Users\\hkyoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hkyoo\\OneDrive\\바탕 화면\\SCI\\코드\\Model\\NODE.py:20\u001b[0m, in \u001b[0;36mRNNEncoder.forward\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     18\u001b[0m t[\u001b[39m1\u001b[39m:] \u001b[39m=\u001b[39m t[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m t[\u001b[39m1\u001b[39m:]\n\u001b[0;32m     19\u001b[0m t[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[1;32m---> 20\u001b[0m xt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((x, t), dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     22\u001b[0m _, h0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(xt\u001b[39m.\u001b[39mflip((\u001b[39m0\u001b[39m,)))  \u001b[39m# Reversed\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m# Compute latent dimension\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 30 but got size 29 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "model(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
