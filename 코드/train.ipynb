{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Model\\ODEF.py:145: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xe (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  def forward(self, z0, t=Tensor([0., 1.]), return_whole_sequence=False):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from Dataset.Economy_Dataset import Economy_Dataset\n",
    "from Dataset.RNN_Transaction_Dataset import RNN_Transaction_Dataset\n",
    "from Dataset.NODE_Transaction_Dataset import NODE_Transaction_Dataset\n",
    "\n",
    "from Model.LSTM import LSTM\n",
    "from Model.NODE import NODE\n",
    "from Model.ODEF import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5\n",
    "hidden_size = 256\n",
    "output_size = 1\n",
    "\n",
    "lr = 1e-3\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 부동산 & 경제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df = pd.read_excel('../데이터/Transaction/transaction_final.xlsx', index_col=0)\n",
    "economy_df = pd.read_excel('../데이터/Economy/economy_all.xlsx')\n",
    "economy_df = economy_df['국고채금리']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\RNN_Transaction_Dataset.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'])\n",
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\RNN_Transaction_Dataset.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'])\n",
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\Economy_Dataset.py:11: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xe (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  self.economy_x = torch.FloatTensor(economy_x)\n"
     ]
    }
   ],
   "source": [
    "trainsaction_train_size = int(len(transaction_df)*0.7)\n",
    "trainsaction_val_size = int(len(transaction_df)*0.3)\n",
    "\n",
    "transaction_train_dataset = RNN_Transaction_Dataset(transaction_df[:trainsaction_train_size])\n",
    "transaction_train_loader = DataLoader(transaction_train_dataset, batch_size=2)\n",
    "transaction_val_dataset = RNN_Transaction_Dataset(transaction_df[trainsaction_train_size:])\n",
    "transaction_val_loader = DataLoader(transaction_val_dataset, batch_size=2)\n",
    "\n",
    "economy_train_size = int(len(economy_df)*0.7)\n",
    "economy_val_size = int(len(economy_df)*0.3)\n",
    "\n",
    "economy_train_dataset = Economy_Dataset(economy_df[:economy_train_size])\n",
    "economy_train_loader = DataLoader(economy_train_dataset, batch_size=2)\n",
    "economy_val_dataset = Economy_Dataset(economy_df[economy_train_size:])\n",
    "economy_val_loader = DataLoader(economy_val_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경제 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1, Training Loss: 0.002730900188907981, Validation Loss: 0.3143093644015106\n",
      "Epoch 1/1, Training Loss: 0.0027428490575402975, Validation Loss: 0.31638226120186774\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf') \n",
    "for epoch in range(num_epochs + 1):\n",
    "    model.train()\n",
    "    for batch_idx, samples in enumerate(economy_train_loader):\n",
    "        economy_x_train, economy_y_train = samples\n",
    "\n",
    "        prediction, hidden = model(economy_x_train)\n",
    "        cost = criterion(prediction, economy_y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, samples in enumerate(economy_val_loader):\n",
    "            economy_x_val, economy_y_val = samples\n",
    "\n",
    "            prediction, hidden = model(economy_x_val)\n",
    "            loss = criterion(prediction, economy_y_val)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(economy_val_loader)\n",
    "    print(f'Epoch {epoch}/{num_epochs}, Training Loss: {cost.item()}, Validation Loss: {val_loss}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), '../데이터/Checkpoint/best_rnn_economy_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 부동산 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1, Training Loss: 28219.5234375, Validation Loss: 5213844.961625429\n",
      "Epoch 1/1, Training Loss: 27884.662109375, Validation Loss: 5179137.586078938\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf') \n",
    "for epoch in range(num_epochs + 1):\n",
    "    model.train()\n",
    "    for batch_idx, samples in enumerate(transaction_train_loader):\n",
    "        dong_x_train, dong_y_train = samples\n",
    "\n",
    "        prediction, hidden = model(dong_x_train)\n",
    "        cost = criterion(prediction, dong_y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, samples in enumerate(transaction_val_loader):\n",
    "            x_val, y_val = samples\n",
    "\n",
    "            prediction, hidden = model(x_val)\n",
    "            loss = criterion(prediction, y_val)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(transaction_val_loader)\n",
    "    print(f'Epoch {epoch}/{num_epochs}, Training Loss: {cost.item()}, Validation Loss: {val_loss}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), '../데이터/Checkpoint/best_rnn_transaction_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NODE 부동산 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 다운로드\n",
    "batch_size = 2\n",
    "transaction_df = pd.read_excel('../데이터/Transaction/transaction_final.xlsx', index_col=0)[:300]\n",
    "\n",
    "train_dataset = NODE_Transaction_Dataset(transaction_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 크기 : torch.Size([2, 5])\n",
      "Y 크기 : torch.Size([2, 5])\n",
      "Z 크기 : torch.Size([2, 1])\n",
      "W 크기 : torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "for x,y,z,w in train_loader:\n",
    "      print(\"X 크기 : {}\".format(x.shape))\n",
    "      print(\"Y 크기 : {}\".format(y.shape))\n",
    "      print(\"Z 크기 : {}\".format(z.shape))\n",
    "      print(\"W 크기 : {}\".format(w.shape))\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 크기 : torch.float32\n",
      "Y 크기 : torch.float32\n",
      "Z 크기 : torch.float32\n",
      "W 크기 : torch.float32\n"
     ]
    }
   ],
   "source": [
    "for x,y,z,w in train_loader:\n",
    "      print(\"X 크기 : {}\".format(x.dtype))\n",
    "      print(\"Y 크기 : {}\".format(y.dtype))\n",
    "      print(\"Z 크기 : {}\".format(z.dtype))\n",
    "      print(\"W 크기 : {}\".format(w.dtype))\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 is available\n",
      "작동하는지 실험\n",
      "(tensor([[[-0.3383],\n",
      "         [-0.5031]],\n",
      "\n",
      "        [[-0.3987],\n",
      "         [-0.5111]],\n",
      "\n",
      "        [[-0.5568],\n",
      "         [-0.5314]],\n",
      "\n",
      "        [[-0.1567],\n",
      "         [-0.3575]],\n",
      "\n",
      "        [[-0.4635],\n",
      "         [-0.5185]]], device='cuda:0', grad_fn=<SliceBackward0>), tensor([[[-4.2405e-01,  3.8913e-01, -1.8534e-01,  ..., -3.6342e-01,\n",
      "          -6.2650e-01,  2.5097e-01],\n",
      "         [-7.6890e-01,  5.2417e-01, -5.7313e-01,  ..., -1.5994e-01,\n",
      "           7.9747e-01, -7.0562e-01]],\n",
      "\n",
      "        [[-1.7614e-01,  2.2344e-01, -2.8449e-01,  ...,  1.0261e-01,\n",
      "          -7.2467e-01,  7.0099e-02],\n",
      "         [-6.9962e-01,  1.5217e-03, -5.1395e-01,  ...,  4.7271e-01,\n",
      "           6.5689e-01, -7.7950e-01]],\n",
      "\n",
      "        [[ 5.1198e-01,  1.4697e-02, -8.1318e-01,  ...,  4.6904e-01,\n",
      "          -9.3604e-01, -1.0604e-01],\n",
      "         [-5.8893e-01, -2.2069e-01, -6.2990e-01,  ...,  7.5617e-01,\n",
      "           6.7859e-01, -8.5812e-01]],\n",
      "\n",
      "        [[-9.2816e-01,  5.0963e-01, -1.0582e+00,  ..., -5.2520e+00,\n",
      "          -8.9159e-01,  1.7572e+00],\n",
      "         [-5.6407e-01,  1.2765e+00, -2.0001e+00,  ..., -2.0877e+00,\n",
      "           1.4543e+00, -5.3960e-01]],\n",
      "\n",
      "        [[ 9.7336e-02,  8.6790e-02, -4.5413e-01,  ...,  3.9487e-01,\n",
      "          -8.2866e-01, -6.6539e-02],\n",
      "         [-6.5840e-01, -1.1774e-01, -5.4921e-01,  ...,  6.1741e-01,\n",
      "           6.4323e-01, -8.1809e-01]],\n",
      "\n",
      "        [[-9.5510e-01,  4.5907e-01, -1.2367e+00,  ..., -6.0169e+00,\n",
      "          -1.0159e+00,  1.9947e+00],\n",
      "         [-5.9719e-01,  1.3070e+00, -2.7899e+00,  ..., -2.8990e+00,\n",
      "           1.6975e+00, -3.3405e-01]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[ 0.5070,  0.7481,  0.4266, -1.4168, -0.6128,  0.6923,  0.4179,  0.1904,\n",
      "          0.4573, -0.4059,  0.9528, -1.0766, -0.1560,  1.0815,  0.3327, -0.5579,\n",
      "          1.3269, -0.7944, -2.9973,  1.5342, -1.4209, -0.6003, -1.7030, -0.3625,\n",
      "         -0.0662,  0.3889,  2.1313, -0.8612,  0.2735, -0.4476,  1.6229, -1.2084,\n",
      "         -1.0859, -0.2613, -0.8000,  1.0146,  0.2534,  1.1187, -0.2913, -0.9273,\n",
      "         -1.0234, -1.2049, -1.5812,  0.2130,  0.9640, -0.6108, -0.5443,  0.3408,\n",
      "         -0.2514,  0.3401, -0.3831,  2.0152,  1.4352, -1.2642, -0.1542, -1.0826,\n",
      "         -0.2033,  0.1667,  0.2186, -0.9486,  0.5908,  1.2977,  1.6672,  1.8112],\n",
      "        [ 0.3932, -1.2249,  2.7488,  0.6706, -0.0946,  0.7410, -0.2890,  0.0186,\n",
      "         -0.7381, -0.6645,  0.6488,  1.1155,  1.0158,  1.0392,  0.0545, -0.4058,\n",
      "         -0.6071, -0.0337,  2.5212, -0.6659,  1.0415, -1.0656, -1.0680,  0.4484,\n",
      "          1.5806, -2.1216,  0.7588, -0.1057,  2.4480, -0.6215,  0.5172,  0.3292,\n",
      "         -1.0774, -0.6823,  0.6935,  0.1864, -0.2807, -0.3150, -0.0360,  2.5118,\n",
      "         -1.3290,  1.7038,  0.6060, -1.9662,  0.4118,  0.0344,  0.7174,  1.2581,\n",
      "         -0.6289, -0.7734,  1.8380,  0.4735, -0.7011,  0.4709,  0.8778, -1.1247,\n",
      "          0.0049,  0.2649,  1.2490,  0.5174, -1.1203, -0.2169,  0.7045, -1.9023]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[-0.0094, -0.0052,  0.0625, -0.0273,  0.0168,  0.0073,  0.0815,  0.0694,\n",
      "         -0.0017,  0.0207, -0.0489,  0.0710,  0.0058,  0.0368,  0.0229, -0.0638,\n",
      "         -0.0474, -0.0279,  0.0154, -0.0023, -0.0308,  0.0121, -0.0419, -0.0760,\n",
      "         -0.0343, -0.0229, -0.0169, -0.0071, -0.0666,  0.0661,  0.0289, -0.1160,\n",
      "         -0.0431,  0.0101, -0.0605,  0.0678, -0.0667,  0.0443,  0.0327, -0.0371,\n",
      "          0.1006,  0.0764, -0.0682,  0.0352, -0.0253, -0.0091, -0.0367,  0.0147,\n",
      "          0.0528,  0.0892, -0.0174,  0.0428,  0.0668,  0.0134,  0.0332, -0.0182,\n",
      "         -0.0298,  0.0059, -0.0410, -0.0187,  0.0314,  0.0271,  0.0306, -0.0048],\n",
      "        [-0.0142, -0.0097,  0.0685, -0.0209,  0.0332,  0.0165,  0.0809,  0.0737,\n",
      "         -0.0093, -0.0008, -0.0459,  0.0879, -0.0025,  0.0427,  0.0349, -0.0763,\n",
      "         -0.0503, -0.0418,  0.0205, -0.0140, -0.0301, -0.0044, -0.0534, -0.0720,\n",
      "         -0.0369, -0.0237, -0.0133, -0.0154, -0.0715,  0.0686,  0.0408, -0.1357,\n",
      "         -0.0561,  0.0292, -0.0792,  0.0807, -0.0701,  0.0614,  0.0330, -0.0364,\n",
      "          0.1096,  0.0713, -0.0788,  0.0267, -0.0223, -0.0240, -0.0363,  0.0298,\n",
      "          0.0605,  0.0954, -0.0250,  0.0573,  0.0746,  0.0152,  0.0388, -0.0136,\n",
      "         -0.0397,  0.0129, -0.0269, -0.0316,  0.0301,  0.0331,  0.0376,  0.0078]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>), tensor([[-0.0415,  0.0054,  0.0580, -0.0549,  0.0065, -0.0061, -0.0556,  0.0550,\n",
      "         -0.0350, -0.0363, -0.0057, -0.0116, -0.0254,  0.0969, -0.0521,  0.0535,\n",
      "         -0.0475,  0.0749,  0.0240,  0.0196,  0.0205,  0.0782,  0.0185, -0.0856,\n",
      "          0.0290,  0.1193,  0.0104, -0.0856,  0.0070,  0.0810, -0.0362,  0.0265,\n",
      "          0.0025,  0.0186, -0.0096, -0.0775, -0.0307, -0.0301,  0.0371,  0.0274,\n",
      "         -0.0046, -0.0240, -0.0640, -0.0162, -0.0162, -0.0230,  0.0443, -0.0287,\n",
      "         -0.0217, -0.0222, -0.0507, -0.0373,  0.0018,  0.0183,  0.0128,  0.0259,\n",
      "          0.0351,  0.0281, -0.0477, -0.0803, -0.0112,  0.0257, -0.0078,  0.0464],\n",
      "        [-0.0442, -0.0131,  0.0574, -0.0675,  0.0239, -0.0099, -0.0511,  0.0462,\n",
      "         -0.0331, -0.0325, -0.0093, -0.0198, -0.0195,  0.1073, -0.0572,  0.0593,\n",
      "         -0.0465,  0.0660,  0.0317,  0.0155,  0.0236,  0.0806,  0.0131, -0.0818,\n",
      "          0.0213,  0.1363, -0.0083, -0.0970,  0.0036,  0.0929, -0.0442,  0.0338,\n",
      "         -0.0116,  0.0185, -0.0187, -0.0870, -0.0345, -0.0320,  0.0426,  0.0196,\n",
      "         -0.0036, -0.0159, -0.0778, -0.0083, -0.0164, -0.0255,  0.0481, -0.0399,\n",
      "         -0.0278, -0.0129, -0.0566, -0.0405,  0.0145,  0.0089,  0.0275,  0.0239,\n",
      "          0.0370,  0.0268, -0.0574, -0.0863, -0.0177,  0.0324, -0.0108,  0.0530]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>), tensor([[-0.1392],\n",
      "        [-0.2223]], device='cuda:0', grad_fn=<SelectBackward0>))\n",
      "torch.Size([5, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# 데이터 & 모델에 device 붙임!!!\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device} is available')\n",
    "\n",
    "model = NODE(output_dim=1,hidden_dim=256,latent_dim=64).to(device)\n",
    "\n",
    "print('작동하는지 실험')\n",
    "basic_data = torch.rand((5,2,1)).to(device)  # window_size, batch_size, 1\n",
    "time = torch.FloatTensor([[1,2,3,6,10,12],[1,3,5,8,10,12]]).reshape(6,2,1).to(device) # window_size, batch_size, 1\n",
    "data = model(basic_data,time)\n",
    "print(data)\n",
    "print(data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "noise_std = 0.02\n",
    "optim = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), lr=0.001)\n",
    "\n",
    "num_epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0\n",
      "loss : 12830812160.0, best loss : 12830812160.0\n",
      "loss : 22191441920.0, best loss : 12830812160.0\n",
      "loss : 24720576512.0, best loss : 12830812160.0\n",
      "loss : 26226360320.0, best loss : 12830812160.0\n",
      "loss : 20375556096.0, best loss : 12830812160.0\n",
      "loss : 24648525824.0, best loss : 12830812160.0\n",
      "loss : 13970219008.0, best loss : 12830812160.0\n",
      "loss : 13985061888.0, best loss : 12830812160.0\n",
      "loss : 13622090752.0, best loss : 12830812160.0\n",
      "loss : 12672044032.0, best loss : 12672044032.0\n",
      "loss : 24678682624.0, best loss : 12672044032.0\n",
      "loss : 38977490944.0, best loss : 12672044032.0\n",
      "loss : 10655381504.0, best loss : 10655381504.0\n",
      "loss : 10576839680.0, best loss : 10576839680.0\n",
      "loss : 9264247808.0, best loss : 9264247808.0\n",
      "loss : 5754091008.0, best loss : 5754091008.0\n",
      "loss : 7854706176.0, best loss : 5754091008.0\n",
      "loss : 18351620096.0, best loss : 5754091008.0\n",
      "loss : 6241753600.0, best loss : 5754091008.0\n",
      "loss : 2375976192.0, best loss : 2375976192.0\n",
      "loss : 2577966848.0, best loss : 2375976192.0\n",
      "loss : 3023938048.0, best loss : 2375976192.0\n",
      "loss : 5393776640.0, best loss : 2375976192.0\n",
      "loss : 5793871360.0, best loss : 2375976192.0\n",
      "loss : 33890527232.0, best loss : 2375976192.0\n",
      "loss : 18930100224.0, best loss : 2375976192.0\n",
      "loss : 21992126464.0, best loss : 2375976192.0\n",
      "loss : 12381723648.0, best loss : 2375976192.0\n",
      "loss : 13685398528.0, best loss : 2375976192.0\n",
      "loss : 9917001728.0, best loss : 2375976192.0\n",
      "loss : 11658651648.0, best loss : 2375976192.0\n",
      "loss : 19599595520.0, best loss : 2375976192.0\n",
      "loss : 21536772096.0, best loss : 2375976192.0\n",
      "loss : 21932963840.0, best loss : 2375976192.0\n",
      "loss : 23863945216.0, best loss : 2375976192.0\n",
      "loss : 22608384000.0, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  1\n",
      "loss : 12819766272.0, best loss : 2375976192.0\n",
      "loss : 22176544768.0, best loss : 2375976192.0\n",
      "loss : 24702822400.0, best loss : 2375976192.0\n",
      "loss : 26226884608.0, best loss : 2375976192.0\n",
      "loss : 20373753856.0, best loss : 2375976192.0\n",
      "loss : 24658511872.0, best loss : 2375976192.0\n",
      "loss : 13987760128.0, best loss : 2375976192.0\n",
      "loss : 14014693376.0, best loss : 2375976192.0\n",
      "loss : 13660433408.0, best loss : 2375976192.0\n",
      "loss : 12735908864.0, best loss : 2375976192.0\n",
      "loss : 25086199808.0, best loss : 2375976192.0\n",
      "loss : 40451014656.0, best loss : 2375976192.0\n",
      "loss : 10786851840.0, best loss : 2375976192.0\n",
      "loss : 11069931520.0, best loss : 2375976192.0\n",
      "loss : 10052529152.0, best loss : 2375976192.0\n",
      "loss : 7164113920.0, best loss : 2375976192.0\n",
      "loss : 6478907904.0, best loss : 2375976192.0\n",
      "loss : 7012304384.0, best loss : 2375976192.0\n",
      "loss : 2514063616.0, best loss : 2375976192.0\n",
      "loss : 2611733504.0, best loss : 2375976192.0\n",
      "loss : 2928310016.0, best loss : 2375976192.0\n",
      "loss : 3254315776.0, best loss : 2375976192.0\n",
      "loss : 6358660096.0, best loss : 2375976192.0\n",
      "loss : 5915730944.0, best loss : 2375976192.0\n",
      "loss : 40161976320.0, best loss : 2375976192.0\n",
      "loss : 19335008256.0, best loss : 2375976192.0\n",
      "loss : 24623855616.0, best loss : 2375976192.0\n",
      "loss : 12563651584.0, best loss : 2375976192.0\n",
      "loss : 13801490432.0, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  2\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  3\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  4\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  5\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  6\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  7\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  8\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  9\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  10\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  11\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  12\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  13\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  14\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  15\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  16\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  17\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  18\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  19\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  20\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  21\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  22\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  23\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  24\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  25\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  26\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  27\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  28\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  29\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  30\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  31\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  32\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  33\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  34\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  35\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  36\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  37\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  38\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  39\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  40\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  41\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  42\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  43\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  44\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  45\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  46\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  47\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  48\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  49\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n",
      "epoch :  50\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "loss : nan, best loss : 2375976192.0\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_train_loss = float('inf') \n",
    "for epoch in range(num_epochs + 1):\n",
    "    print('epoch : ',epoch)\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for batch_idx, samples in enumerate(train_loader):\n",
    "        tran_x, time_x, tran_y, time_y = samples\n",
    "        \n",
    "        t = torch.cat((time_x,time_y),dim=1)  \n",
    "        tran_x = tran_x.transpose(0,1).unsqueeze(2).to(device)\n",
    "        t = t.transpose(0,1).unsqueeze(2).to(device)\n",
    "        \n",
    "        x_p, _, z, z_mean, z_log_var, pred = model(tran_x, t)\n",
    "        kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean**2 - torch.exp(z_log_var), -1)\n",
    "        loss = 0.5 * ((tran_x-x_p)**2).sum(-1).sum(0) / noise_std**2 + kl_loss\n",
    "        loss = torch.mean(loss)\n",
    "        loss /= window_size\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if loss < best_train_loss:\n",
    "            best_train_loss = loss\n",
    "            torch.save(model.state_dict(), \"../데이터/checkpoint/best_ODE_transaction_model.pth\")\n",
    "        \n",
    "        print('loss : {}, best loss : {}'.format(loss, best_train_loss))\n",
    "    print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ODE 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn  import functional as F \n",
    "from torch.autograd import Variable\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEF(nn.Module):\n",
    "    def forward_with_grad(self, z, t, grad_outputs):\n",
    "        \"\"\"Compute f and a df/dz, a df/dp, a df/dt\"\"\"\n",
    "        batch_size = z.shape[0]\n",
    "\n",
    "        out = self.forward(z, t)\n",
    "\n",
    "        a = grad_outputs\n",
    "        adfdz, adfdt, *adfdp = torch.autograd.grad(\n",
    "            (out,), (z, t) + tuple(self.parameters()), grad_outputs=(a),\n",
    "            allow_unused=True, retain_graph=True\n",
    "        )\n",
    "        # grad method automatically sums gradients for batch items, we have to expand them back \n",
    "        if adfdp is not None:\n",
    "            adfdp = torch.cat([p_grad.flatten() for p_grad in adfdp]).unsqueeze(0)\n",
    "            adfdp = adfdp.expand(batch_size, -1) / batch_size\n",
    "        if adfdt is not None:\n",
    "            adfdt = adfdt.expand(batch_size, 1) / batch_size\n",
    "        return out, adfdz, adfdt, adfdp\n",
    "\n",
    "    def flatten_parameters(self):\n",
    "        p_shapes = []\n",
    "        flat_parameters = []\n",
    "        for p in self.parameters():\n",
    "            p_shapes.append(p.size())\n",
    "            flat_parameters.append(p.flatten())\n",
    "        return torch.cat(flat_parameters)\n",
    "    \n",
    "class LinearODEF(ODEF):\n",
    "    def __init__(self, W):\n",
    "        super(LinearODEF, self).__init__()\n",
    "        self.lin = nn.Linear(2, 2, bias=False)\n",
    "        self.lin.weight = nn.Parameter(W)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return self.lin(x)\n",
    "    \n",
    "class NeuralODE(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super(NeuralODE, self).__init__()\n",
    "        assert isinstance(func, ODEF)\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, z0, t=Tensor([0., 1.]), return_whole_sequence=False):\n",
    "        t = t.to(z0)\n",
    "        z = ODEAdjoint.apply(z0, t, self.func.flatten_parameters(), self.func)\n",
    "        if return_whole_sequence:\n",
    "            return z\n",
    "        else:\n",
    "            return z[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEAdjoint(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, z0, t, flat_parameters, func):\n",
    "        assert isinstance(func, ODEF)\n",
    "        bs, *z_shape = z0.size()\n",
    "        time_len = t.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = torch.zeros(time_len, bs, *z_shape).to(z0)\n",
    "            z[0] = z0\n",
    "            for i_t in range(time_len - 1):\n",
    "                z0 = ode_solve(z0, t[i_t], t[i_t+1], func)\n",
    "                z[i_t+1] = z0\n",
    "\n",
    "        ctx.func = func\n",
    "        ctx.save_for_backward(t, z.clone(), flat_parameters)\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dLdz):\n",
    "        \"\"\"\n",
    "        dLdz shape: time_len, batch_size, *z_shape\n",
    "        \"\"\"\n",
    "        func = ctx.func\n",
    "        t, z, flat_parameters = ctx.saved_tensors\n",
    "        time_len, bs, *z_shape = z.size()\n",
    "        n_dim = np.prod(z_shape)\n",
    "        n_params = flat_parameters.size(0)\n",
    "\n",
    "        # Dynamics of augmented system to be calculated backwards in time\n",
    "        def augmented_dynamics(aug_z_i, t_i):\n",
    "            \"\"\"\n",
    "            tensors here are temporal slices\n",
    "            t_i - is tensor with size: bs, 1\n",
    "            aug_z_i - is tensor with size: bs, n_dim*2 + n_params + 1\n",
    "            \"\"\"\n",
    "            z_i, a = aug_z_i[:, :n_dim], aug_z_i[:, n_dim:2*n_dim]  # ignore parameters and time\n",
    "\n",
    "            # Unflatten z and a\n",
    "            z_i = z_i.view(bs, *z_shape)\n",
    "            a = a.view(bs, *z_shape)\n",
    "            with torch.set_grad_enabled(True):\n",
    "                t_i = t_i.detach().requires_grad_(True)\n",
    "                z_i = z_i.detach().requires_grad_(True)\n",
    "                func_eval, adfdz, adfdt, adfdp = func.forward_with_grad(z_i, t_i, grad_outputs=a)  # bs, *z_shape\n",
    "                adfdz = adfdz.to(z_i) if adfdz is not None else torch.zeros(bs, *z_shape).to(z_i)\n",
    "                adfdp = adfdp.to(z_i) if adfdp is not None else torch.zeros(bs, n_params).to(z_i)\n",
    "                adfdt = adfdt.to(z_i) if adfdt is not None else torch.zeros(bs, 1).to(z_i)\n",
    "\n",
    "            # Flatten f and adfdz\n",
    "            func_eval = func_eval.view(bs, n_dim)\n",
    "            adfdz = adfdz.view(bs, n_dim) \n",
    "            return torch.cat((func_eval, -adfdz, -adfdp, -adfdt), dim=1)\n",
    "\n",
    "        dLdz = dLdz.view(time_len, bs, n_dim)  # flatten dLdz for convenience\n",
    "        with torch.no_grad():\n",
    "            ## Create placeholders for output gradients\n",
    "            # Prev computed backwards adjoints to be adjusted by direct gradients\n",
    "            adj_z = torch.zeros(bs, n_dim).to(dLdz)\n",
    "            adj_p = torch.zeros(bs, n_params).to(dLdz)\n",
    "            # In contrast to z and p we need to return gradients for all times\n",
    "            adj_t = torch.zeros(time_len, bs, 1).to(dLdz)\n",
    "\n",
    "            for i_t in range(time_len-1, 0, -1):\n",
    "                z_i = z[i_t]\n",
    "                t_i = t[i_t]\n",
    "                f_i = func(z_i, t_i).view(bs, n_dim)\n",
    "\n",
    "                # Compute direct gradients\n",
    "                dLdz_i = dLdz[i_t]\n",
    "                dLdt_i = torch.bmm(torch.transpose(dLdz_i.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0]\n",
    "\n",
    "                # Adjusting adjoints with direct gradients\n",
    "                adj_z += dLdz_i\n",
    "                adj_t[i_t] = adj_t[i_t] - dLdt_i\n",
    "\n",
    "                # Pack augmented variable\n",
    "                aug_z = torch.cat((z_i.view(bs, n_dim), adj_z, torch.zeros(bs, n_params).to(z), adj_t[i_t]), dim=-1)\n",
    "\n",
    "                # Solve augmented system backwards\n",
    "                aug_ans = ode_solve(aug_z, t_i, t[i_t-1], augmented_dynamics)\n",
    "\n",
    "                # Unpack solved backwards augmented system\n",
    "                adj_z[:] = aug_ans[:, n_dim:2*n_dim]\n",
    "                adj_p[:] += aug_ans[:, 2*n_dim:2*n_dim + n_params]\n",
    "                adj_t[i_t-1] = aug_ans[:, 2*n_dim + n_params:]\n",
    "\n",
    "                del aug_z, aug_ans\n",
    "\n",
    "            ## Adjust 0 time adjoint with direct gradients\n",
    "            # Compute direct gradients \n",
    "            dLdz_0 = dLdz[0]\n",
    "            dLdt_0 = torch.bmm(torch.transpose(dLdz_0.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0]\n",
    "\n",
    "            # Adjust adjoints\n",
    "            adj_z += dLdz_0\n",
    "            adj_t[0] = adj_t[0] - dLdt_0\n",
    "        return adj_z.view(bs, *z_shape), adj_t, adj_p, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_solve(z0, t0, t1, f):\n",
    "    \"\"\"\n",
    "    Simplest Euler ODE initial value solver\n",
    "    \"\"\"\n",
    "    h_max = 0.05\n",
    "    n_steps = math.ceil((abs(t1 - t0)/h_max).max().item())\n",
    "\n",
    "    h = (t1 - t0)/n_steps\n",
    "    t = t0\n",
    "    z = z0\n",
    "\n",
    "    for i_step in range(n_steps):\n",
    "        z = z + h * f(z, t)\n",
    "        t = t + h\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as npr\n",
    "\n",
    "def gen_batch(batch_size, n_sample=100):\n",
    "    n_batches = samp_trajs.shape[1] // batch_size\n",
    "    time_len = samp_trajs.shape[0]\n",
    "    n_sample = min(n_sample, time_len)\n",
    "    for i in range(n_batches):\n",
    "        if n_sample > 0:\n",
    "            t0_idx = npr.multinomial(1, [1. / (time_len - n_sample)] * (time_len - n_sample))\n",
    "            t0_idx = np.argmax(t0_idx)\n",
    "            tM_idx = t0_idx + n_sample\n",
    "        else:\n",
    "            t0_idx = 0\n",
    "            tM_idx = time_len\n",
    "\n",
    "        frm, to = batch_size*i, batch_size*(i+1)\n",
    "        yield samp_trajs[t0_idx:tM_idx, frm:to], samp_ts[t0_idx:tM_idx, frm:to]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hkyoo\\AppData\\Local\\Temp\\ipykernel_27012\\2661987758.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  index_np = np.arange(0, n_points, 1, dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "t_max = 6.29*5\n",
    "n_points = 200\n",
    "noise_std = 0.02\n",
    "\n",
    "num_spirals = 1000\n",
    "\n",
    "index_np = np.arange(0, n_points, 1, dtype=np.int)\n",
    "index_np = np.hstack([index_np[:, None]])\n",
    "times_np = np.linspace(0, t_max, num=n_points)\n",
    "times_np = np.hstack([times_np[:, None]] * num_spirals)\n",
    "times = torch.from_numpy(times_np[:, :, None]).to(torch.float32)\n",
    "\n",
    "# Generate random spirals parameters\n",
    "normal01 = torch.distributions.Normal(0, 1.0)\n",
    "\n",
    "x0 = Variable(normal01.sample((num_spirals, 2))) * 2.0  \n",
    "\n",
    "W11 = -0.1 * normal01.sample((num_spirals,)).abs() - 0.05\n",
    "W22 = -0.1 * normal01.sample((num_spirals,)).abs() - 0.05\n",
    "W21 = -1.0 * normal01.sample((num_spirals,)).abs()\n",
    "W12 =  1.0 * normal01.sample((num_spirals,)).abs()\n",
    "\n",
    "xs_list = []\n",
    "for i in range(num_spirals):\n",
    "    if i % 2 == 1: #  Make it counter-clockwise\n",
    "        W21, W12 = W12, W21\n",
    "\n",
    "    func = LinearODEF(Tensor([[W11[i], W12[i]], [W21[i], W22[i]]]))\n",
    "    ode = NeuralODE(func)\n",
    "\n",
    "    xs = ode(x0[i:i+1], times[:, i:i+1], return_whole_sequence=True)\n",
    "    xs_list.append(xs)\n",
    "\n",
    "\n",
    "orig_trajs = torch.cat(xs_list, dim=1).detach()\n",
    "samp_trajs = orig_trajs + torch.randn_like(orig_trajs) * noise_std\n",
    "samp_ts = times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "preload = False\n",
    "n_epochs = 20000\n",
    "batch_size = 100\n",
    "\n",
    "plot_traj_idx = 1\n",
    "plot_traj = orig_trajs[:, plot_traj_idx:plot_traj_idx+1]\n",
    "plot_obs = samp_trajs[:, plot_traj_idx:plot_traj_idx+1]\n",
    "plot_ts = samp_ts[:, plot_traj_idx:plot_traj_idx+1]\n",
    "\n",
    "for epoch_idx in range(n_epochs):\n",
    "    losses = []\n",
    "    train_iter = gen_batch(batch_size)\n",
    "    for x, t in train_iter:\n",
    "\n",
    "        max_len = np.random.choice([30, 50, 100])\n",
    "        permutation = np.random.permutation(t.shape[0])\n",
    "        np.random.shuffle(permutation)\n",
    "        permutation = np.sort(permutation[:max_len])\n",
    "\n",
    "        x, t = x[permutation], t[permutation]\n",
    "        \n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 100, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 100, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NODE(output_dim=2,hidden_dim=256,latent_dim=64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nn.GRU(3,256)\n",
    "t[1:] = t[:-1] - t[1:]\n",
    "t[0] = 0.\n",
    "xt = torch.cat((x, t), dim=-1)\n",
    "\n",
    "_, h0 = g(xt.flip((0,)))  # Reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 100, 3])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt.flip((0,)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 30 but got size 29 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model(x,t)\n",
      "File \u001b[1;32mc:\\Users\\hkyoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hkyoo\\OneDrive\\바탕 화면\\SCI\\코드\\Model\\NODE.py:65\u001b[0m, in \u001b[0;36mNODE.forward\u001b[1;34m(self, x, t, MAP)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, t, MAP\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 65\u001b[0m     z_mean, z_log_var\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x, t[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[0;32m     66\u001b[0m     \u001b[39mif\u001b[39;00m MAP:\n\u001b[0;32m     67\u001b[0m         z \u001b[39m=\u001b[39m z_mean\n",
      "File \u001b[1;32mc:\\Users\\hkyoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hkyoo\\OneDrive\\바탕 화면\\SCI\\코드\\Model\\NODE.py:20\u001b[0m, in \u001b[0;36mRNNEncoder.forward\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     18\u001b[0m t[\u001b[39m1\u001b[39m:] \u001b[39m=\u001b[39m t[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m t[\u001b[39m1\u001b[39m:]\n\u001b[0;32m     19\u001b[0m t[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[1;32m---> 20\u001b[0m xt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((x, t), dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     22\u001b[0m _, h0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(xt\u001b[39m.\u001b[39mflip((\u001b[39m0\u001b[39m,)))  \u001b[39m# Reversed\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m# Compute latent dimension\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 30 but got size 29 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "model(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
