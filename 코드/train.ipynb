{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "from Dataset.Embedding_Dataset import Embedding_Dataset\n",
    "from Model.Embedding import Embedding\n",
    "\n",
    "from Dataset.LSTM_Dataset import LSTM_Dataset\n",
    "from Model.LSTM import LSTM\n",
    "\n",
    "from Dataset.Attention_Dataset import Attention_Dataset\n",
    "from Model.Attention import LSTMSeq2Seq\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = torch.device('cpu') # CPU\n",
    "# DEVICE = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu') # MAC\n",
    "# DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') # WINDOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_lr = 0.001\n",
    "embedding_batch = 32\n",
    "embedding_epochs = 150\n",
    "encoder_dim_1 = 128\n",
    "encoder_dim_2 = 256\n",
    "encoder_dim_3 = 512\n",
    "embedding_dim = 1024\n",
    "decoder_dim_1 = 512\n",
    "decoder_dim_2 = 256\n",
    "decoder_dim_3 = 128\n",
    "\n",
    "lstm_lr = 0.01\n",
    "lstm_batch = 1\n",
    "lstm_epochs = 50\n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "window_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "connection_info = \"host=localhost dbname=postgres user=postgres password=hd219833 port=5432\"\n",
    "conn = psycopg2.connect(connection_info)\n",
    "table_1_query = '''\n",
    "    SELECT * FROM building\n",
    "    '''\n",
    "table_2_query = '''\n",
    "    SELECT * FROM economy\n",
    "    '''\n",
    "table_3_query = '''\n",
    "    SELECT * FROM building_price\n",
    "    '''\n",
    "table_1 = pd.read_sql(table_1_query,conn) \n",
    "table_2 = pd.read_sql(table_2_query,conn)\n",
    "table_3 = pd.read_sql(table_3_query,conn) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "\n",
    "dataset = Embedding_Dataset(table_1, table_2, table_3)\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Train Loss: 785903355.3788, Val Loss: 6238460992.6575\n",
      "Epoch [2/150], Train Loss: 702467608.2184, Val Loss: 7516355360.7452\n",
      "Epoch [3/150], Train Loss: 699553889.3157, Val Loss: 2641651220.7342\n",
      "Epoch [4/150], Train Loss: 707483966.1206, Val Loss: 1826492170.8192\n",
      "Epoch [5/150], Train Loss: 680823940.8449, Val Loss: 1689277989.6315\n",
      "Epoch [6/150], Train Loss: 669948296.3576, Val Loss: 1586708175.0021\n",
      "Epoch [7/150], Train Loss: 666433912.8904, Val Loss: 1554239166.7589\n",
      "Epoch [8/150], Train Loss: 665180423.6211, Val Loss: 1494868364.1822\n",
      "Epoch [9/150], Train Loss: 648209590.7122, Val Loss: 1419356940.7753\n",
      "Epoch [10/150], Train Loss: 627768417.8184, Val Loss: 1399161696.5068\n",
      "Epoch [11/150], Train Loss: 616896064.2194, Val Loss: 4062176374.9479\n",
      "Epoch [12/150], Train Loss: 592124075.2579, Val Loss: 1355497848.8384\n",
      "Epoch [13/150], Train Loss: 619059033.2042, Val Loss: 1370666424.4678\n",
      "Epoch [14/150], Train Loss: 611892641.0426, Val Loss: 1321885126.4493\n",
      "Epoch [15/150], Train Loss: 609789319.5554, Val Loss: 1292466185.4596\n",
      "Epoch [16/150], Train Loss: 606772056.4944, Val Loss: 1368374193.6425\n",
      "Epoch [17/150], Train Loss: 598664344.2650, Val Loss: 1249904502.5192\n",
      "Epoch [18/150], Train Loss: 595580595.3478, Val Loss: 1233556296.8534\n",
      "Epoch [19/150], Train Loss: 596966254.5395, Val Loss: 1229441430.4151\n",
      "Epoch [20/150], Train Loss: 587758668.0357, Val Loss: 1214759239.2103\n",
      "Epoch [21/150], Train Loss: 590320246.4241, Val Loss: 1223874085.4219\n",
      "Epoch [22/150], Train Loss: 591423947.2734, Val Loss: 1216188148.3616\n",
      "Epoch [23/150], Train Loss: 582545005.8824, Val Loss: 2324135718.0055\n",
      "Epoch [24/150], Train Loss: 596689342.9742, Val Loss: 1198260752.4027\n",
      "Epoch [25/150], Train Loss: 588048618.2009, Val Loss: 1183115828.4897\n",
      "Epoch [26/150], Train Loss: 577804599.6835, Val Loss: 1157262196.0815\n",
      "Epoch [27/150], Train Loss: 587587239.1359, Val Loss: 1159464819.2288\n",
      "Epoch [28/150], Train Loss: 578153400.4573, Val Loss: 1144982029.1753\n",
      "Epoch [29/150], Train Loss: 570422399.5038, Val Loss: 1183500418.0712\n",
      "Epoch [30/150], Train Loss: 587556625.2956, Val Loss: 1163769927.5041\n",
      "Epoch [31/150], Train Loss: 594208688.2946, Val Loss: 1069062685.3781\n",
      "Epoch [32/150], Train Loss: 575633036.5214, Val Loss: 1072757817.0110\n",
      "Epoch [33/150], Train Loss: 573522857.0386, Val Loss: 1072460070.3452\n",
      "Epoch [34/150], Train Loss: 570155178.6996, Val Loss: 1064474137.7041\n",
      "Epoch [35/150], Train Loss: 573162504.6282, Val Loss: 1063479217.3781\n",
      "Epoch [36/150], Train Loss: 569315798.4099, Val Loss: 1057477836.6000\n",
      "Epoch [37/150], Train Loss: 579953740.6637, Val Loss: 1038037178.0466\n",
      "Epoch [38/150], Train Loss: 568450975.5641, Val Loss: 1039079611.9041\n",
      "Epoch [39/150], Train Loss: 572046127.6130, Val Loss: 1039997724.4301\n",
      "Early Stopping Triggered!\n"
     ]
    }
   ],
   "source": [
    "model = Embedding(encoder_dim_1, encoder_dim_2, encoder_dim_3, embedding_dim, decoder_dim_1, decoder_dim_2, decoder_dim_3).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=embedding_lr)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "consecutive_val_loss_increases = 0\n",
    "max_consecutive_val_loss_increases = 3\n",
    "for epoch in range(embedding_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        input = data[0].to(DEVICE)\n",
    "        target = data[1].to(DEVICE)\n",
    "        output = model(input).to(DEVICE)\n",
    "\n",
    "        train_loss = criterion(output, target)\n",
    "        total_train_loss += train_loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            input = data[0].to(DEVICE)\n",
    "            target = data[1].to(DEVICE)\n",
    "            output = model(input).to(DEVICE)\n",
    "\n",
    "            val_loss = criterion(output, target)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    if len(val_losses) > 1 and val_losses[-1] > val_losses[-2]:\n",
    "        consecutive_val_loss_increases += 1\n",
    "        if consecutive_val_loss_increases >= max_consecutive_val_loss_increases:\n",
    "            torch.save(model, f'../데이터/Checkpoint/embedding_tr_{train_ratio}_lr_{embedding_lr}_batch_{embedding_batch}_epochs_{embedding_epochs}_e1_{encoder_dim_1}_e2_{encoder_dim_1}_e3_{encoder_dim_3}_emb_{embedding_dim}_d1{decoder_dim_1}_d2_{decoder_dim_2}_d3_{decoder_dim_3}.pth')\n",
    "            print(f\"Early Stopping Triggered!\")\n",
    "            break\n",
    "    else:\n",
    "        consecutive_val_loss_increases = 0\n",
    "\n",
    "    # if avg_val_loss < best_val_loss:\n",
    "    #     best_val_loss = avg_val_loss\n",
    "    #     consecutive_val_loss_increases = 0  \n",
    "    # else:\n",
    "    #     consecutive_val_loss_increases += 1 \n",
    "    # if consecutive_val_loss_increases == max_consecutive_val_loss_increases:\n",
    "    #     # torch.save(model, f'../데이터/Checkpoint/embedding_train_{train_ratio}_lr_{embedding_lr}_batch_{embedding_batch}_epochs_{embedding_epochs}_e1_{encoder_dim_1}_e2_{encoder_dim_1}_emb_{embedding_dim}_d1{decoder_dim_1}_d2_{decoder_dim_2}.pth')\n",
    "    #     print(\"Early Stopping Triggered\")\n",
    "    #     break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{embedding_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Validation Loss: 1038037178.0465753\n"
     ]
    }
   ],
   "source": [
    "def plot_train_val_losses(train_losses, val_losses):\n",
    "    plt.plot(train_losses[1:], label='Training Loss')\n",
    "    plt.plot(val_losses[1:], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_train_val_losses(train_losses, val_losses)\n",
    "print(f'Min Validation Loss: {min(val_losses)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../데이터/Checkpoint/embedding_train_0.8_lr_0.01_batch_32_epochs_50_dim_6.pth')\n",
    "dataset = LSTM_Dataset(model, table_1, table_2, table_3, embedding_dim, window_size)\n",
    "\n",
    "train_ratio = 0.8\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 4457084913.2495\n",
      "Epoch [2/50], Train Loss: 2639799669.2864\n",
      "Epoch [3/50], Train Loss: 1889211181.3742\n",
      "Epoch [4/50], Train Loss: 1621113765.7466\n",
      "Epoch [5/50], Train Loss: 1536347630.7305\n",
      "Epoch [6/50], Train Loss: 1508256809.3546\n",
      "Epoch [7/50], Train Loss: 1501316050.4178\n",
      "Epoch [8/50], Train Loss: 1496720547.7443\n",
      "Epoch [9/50], Train Loss: 1493436904.9662\n",
      "Epoch [10/50], Train Loss: 1491849440.4544\n",
      "Epoch [11/50], Train Loss: 1490709939.9167\n",
      "Epoch [12/50], Train Loss: 1490298252.6531\n",
      "Epoch [13/50], Train Loss: 1490066771.7568\n",
      "Epoch [14/50], Train Loss: 1489904553.6412\n",
      "Epoch [15/50], Train Loss: 1489809264.2112\n",
      "Epoch [16/50], Train Loss: 1489719638.4178\n",
      "Epoch [17/50], Train Loss: 1489666388.2125\n",
      "Epoch [18/50], Train Loss: 1489614520.1380\n",
      "Epoch [19/50], Train Loss: 1489531656.4119\n",
      "Epoch [20/50], Train Loss: 1489476864.0525\n",
      "Epoch [21/50], Train Loss: 1489432332.4914\n",
      "Epoch [22/50], Train Loss: 1489389663.6376\n",
      "Epoch [23/50], Train Loss: 1489300576.0239\n",
      "Epoch [24/50], Train Loss: 1489243003.7281\n",
      "Epoch [25/50], Train Loss: 1489210289.8569\n",
      "Epoch [26/50], Train Loss: 1489147177.3672\n",
      "Epoch [27/50], Train Loss: 1489087240.8590\n",
      "Epoch [28/50], Train Loss: 1489049705.3208\n",
      "Epoch [29/50], Train Loss: 1488954457.3110\n",
      "Epoch [30/50], Train Loss: 1488918861.9109\n",
      "Epoch [31/50], Train Loss: 1488849718.9130\n",
      "Epoch [32/50], Train Loss: 1488789314.3850\n",
      "Epoch [33/50], Train Loss: 1488730489.2280\n",
      "Epoch [34/50], Train Loss: 1488664809.6480\n",
      "Epoch [35/50], Train Loss: 1488615758.3526\n",
      "Epoch [36/50], Train Loss: 1488567962.9942\n",
      "Epoch [37/50], Train Loss: 1488495349.4339\n",
      "Epoch [38/50], Train Loss: 1488466819.5393\n",
      "Epoch [39/50], Train Loss: 1488416725.0039\n",
      "Epoch [40/50], Train Loss: 1488351656.2833\n",
      "Epoch [41/50], Train Loss: 1488283501.6679\n",
      "Epoch [42/50], Train Loss: 1488228943.3389\n",
      "Epoch [43/50], Train Loss: 1488196455.7372\n",
      "Epoch [44/50], Train Loss: 1488117979.1185\n",
      "Epoch [45/50], Train Loss: 1488069540.7553\n",
      "Epoch [46/50], Train Loss: 1488010932.9404\n",
      "Epoch [47/50], Train Loss: 1487973938.8470\n",
      "Epoch [48/50], Train Loss: 1487893113.0089\n",
      "Epoch [49/50], Train Loss: 1487875749.7484\n",
      "Epoch [50/50], Train Loss: 1487800924.4878\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lstm_lr)\n",
    "\n",
    "model.train()\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(lstm_epochs):\n",
    "    total_train_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        src = data[0]\n",
    "        trg = data[1]\n",
    "\n",
    "        if trg.sum() != 0: \n",
    "            output = model(src)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0]\n",
    "            trg = data[1]\n",
    "\n",
    "            if trg.sum() != 0:\n",
    "                output = model(src)\n",
    "\n",
    "                val_loss = criterion(output, trg)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{lstm_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "torch.save(model, f'../데이터/Checkpoint/lstm_lr_{lstm_lr}_batch_{lstm_batch}_epochs_{lstm_epochs}_hdim_{hidden_dim}_odim_{output_dim}_ws_{window_size}.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../데이터/Checkpoint/embedding_train_0.8_lr_0.01_batch_32_epochs_50_dim_6.pth')\n",
    "dataset = Attention_Dataset(model, table_1, table_2, table_3, embedding_dim, window_size)\n",
    "\n",
    "train_ratio = 0.8\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('C:/Users/hkyoo/OneDrive/바탕 화면/SCI/데이터/Checkpoint/embedding_lr_0.01_batch_32_epochs_50_dim_6.pth')\n",
    "dataset = Attention_Dataset(model, table_1, table_2, table_3, embedding_dim, window_size)\n",
    "dataloader = DataLoader(dataset, batch_size=lstm_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0254], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = 38\n",
    "window_size = 5\n",
    "mx_len = 1\n",
    "input = torch.randn(num, window_size, embedding_dim)\n",
    "\n",
    "model = LSTMSeq2Seq(embedding_dim, hidden_dim, output_dim, DEVICE)\n",
    "model(input,1, mx_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# 학습\n",
    "history = {'train_loss':[], 'val_loss':[], 'lr':[]}\n",
    "\n",
    "for epoch in range(att_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    num = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        src = batch[0][0].to(DEVICE)\n",
    "        max_len = batch[1][0].to(DEVICE)\n",
    "        anw = batch[2][0]\n",
    "        trg = batch[3][0].to(DEVICE)\n",
    "        \n",
    "        if len(anw)==0:\n",
    "            continue\n",
    "        \n",
    "        num += len(anw)\n",
    "        \n",
    "        dong_loss = 0\n",
    "        for index in anw:\n",
    "            index.to(DEVICE)\n",
    "            output = model(src, index, max_len)\n",
    "            loss = criterion(output, trg)\n",
    "            # dong_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # optimizer.zero_grad()\n",
    "        # # dong_loss /= len(anw)\n",
    "        # dong_loss = torch.tensor(dong_loss, requires_grad=True).to(DEVICE)\n",
    "        # dong_loss.backward()\n",
    "        # optimizer.step()\n",
    "    train_loss = epoch_loss / num\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    \n",
    "    \n",
    "\n",
    "#     if epoch%valid_every==0:\n",
    "#         print(\"==========================\")\n",
    "#         model.eval()\n",
    "#         epoch_loss = 0\n",
    "#         valid_num = 1e-9\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for i, batch in enumerate(val_loader):\n",
    "#                 src = batch[0].to(DEVICE)\n",
    "#                 trg = batch[1].to(DEVICE)\n",
    "#                 if(trg != 0):\n",
    "#                     output = model(src, trg)\n",
    "#                     loss = criterion(output, trg)\n",
    "#                     epoch_loss += loss.item()\n",
    "#                     valid_num += 1\n",
    "#         valid_loss = epoch_loss / valid_num\n",
    "            \n",
    "#         if valid_loss < best_valid_loss:\n",
    "#             best_valid_loss = valid_loss\n",
    "#             model.decoder.t=0\n",
    "#             torch.save(model.state_dict(), 'lstm-model.pt')\n",
    "#         print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
    "\n",
    "#         history['train_loss'].append(train_loss)\n",
    "#         history['val_loss'].append(valid_loss)\n",
    "#         history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('C:/Users/hkyoo/OneDrive/바탕 화면/SCI/데이터/Checkpoint/embedding_lr_0.01_batch_32_epochs_50_dim_6.pth')\n",
    "\n",
    "dataset = LSTM_Dataset(model, table_1, table_2, table_3, embedding_dim, window_size)\n",
    "dataloader = DataLoader(dataset, batch_size=lstm_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_dim, out_dim, nhead, nlayers\n",
    "model = TFModel(embedding_dim, window_size, output_dim, 2, 2)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lstm_lr)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(lstm_epochs):\n",
    "    epoch_loss = 0\n",
    "    train_num = 1e-9\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        src = batch[0].to(DEVICE)\n",
    "        trg = batch[1].to(DEVICE)\n",
    "        if(trg != 0):\n",
    "            train_num += 1\n",
    "            optimizer.zero_grad()\n",
    "            src_mask = model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "            output = model(src, src_mask)\n",
    "            loss = criterion(output[0], trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "    train_loss = epoch_loss / train_num\n",
    "    print(f'Epoch [{epoch+1}/{embedding_epochs}], Train Loss: {train_loss:.4f}')\n",
    "\n",
    "torch.save(model, f'../데이터/Checkpoint/lstm_lsr_{lstm_lr}_batch_{lstm_batch}_epochs_{lstm_epochs}_hdim_{hidden_dim}_odim_{output_dim}_ws{window_size}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Embedding' object has no attribute 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../데이터/Checkpoint/embedding_train_0.8_lr_0.0001_batch_512_epochs_150_e1_256_e2_256_emb_1024_d1512_d2256.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m LSTM_Dataset(model, table_1, table_2, table_3, embedding_dim, window_size)\n\u001b[1;32m      5\u001b[0m dataset_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset)\n\u001b[1;32m      6\u001b[0m split_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(train_ratio \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset))\n",
      "File \u001b[0;32m~/Desktop/Git/sci/SCI/코드/Dataset/LSTM_Dataset.py:25\u001b[0m, in \u001b[0;36mLSTM_Dataset.__init__\u001b[0;34m(self, model, table_1, table_2, table_3, embedding_dim, window_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(encoder_input_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m---> 25\u001b[0m         apartment_complex_embedding_vector \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencoder(encoder_input_tensor[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     26\u001b[0m         apartment_complex_embedding_matrix[i] \u001b[38;5;241m=\u001b[39m apartment_complex_embedding_vector\n\u001b[1;32m     27\u001b[0m apartment_complex_embedding_matrix_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(apartment_complex_embedding_matrix)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Embedding' object has no attribute 'encoder'"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.8\n",
    "\n",
    "model = torch.load('../데이터/Checkpoint/embedding_train_0.8_lr_0.0001_batch_512_epochs_150_e1_256_e2_256_emb_1024_d1512_d2_256.pth')\n",
    "dataset = LSTM_Dataset(model, table_1, table_2, table_3, embedding_dim, window_size)\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NLinear(embedding_dim, window_size, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lstm_lr)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(lstm_epochs):\n",
    "    epoch_loss = 0\n",
    "    train_num = 1e-9\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        src = batch[0].to(DEVICE)\n",
    "        trg = batch[1].to(DEVICE)\n",
    "        if(trg != 0):\n",
    "            train_num += 1\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src)\n",
    "            loss = criterion(output[0], trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "    train_loss = epoch_loss / train_num\n",
    "    print(f'Epoch [{epoch+1}/{embedding_epochs}], Train Loss: {train_loss:.4f}')\n",
    "\n",
    "torch.save(model, f'../데이터/Checkpoint/lstm_lsr_{lstm_lr}_batch_{lstm_batch}_epochs_{lstm_epochs}_hdim_{hidden_dim}_odim_{output_dim}_ws{window_size}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
