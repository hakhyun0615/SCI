{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Model\\ODEF.py:145: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xe (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  def forward(self, z0, t=Tensor([0., 1.]), return_whole_sequence=False):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from Dataset.Economy_Dataset import Economy_Dataset\n",
    "from Dataset.LSTM_Transaction_Train_Dataset import LSTM_Transaction_Train_Dataset\n",
    "from Dataset.ODE_Transaction_Dataset import ODE_Transaction_Dataset\n",
    "\n",
    "from Model.LSTM import LSTM\n",
    "from Model.NODE import NODE\n",
    "from Model.ODERNN import *\n",
    "from Model.ODEF import *\n",
    "\n",
    "from utils import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 is available\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device} is available')\n",
    "\n",
    "transaction_df = pd.read_csv('../데이터/Transaction/transaction_all.csv')\n",
    "economy_df = pd.read_excel('../데이터/Economy/economy_all.xlsx')\n",
    "\n",
    "transaction_df, economy_df = preprocess(transaction_df, economy_df, window_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경제 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1, Training Loss: 0.002730900188907981, Validation Loss: 0.3143093644015106\n",
      "Epoch 1/1, Training Loss: 0.0027428490575402975, Validation Loss: 0.31638226120186774\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf') \n",
    "for epoch in range(num_epochs + 1):\n",
    "    model.train()\n",
    "    for batch_idx, samples in enumerate(economy_train_loader):\n",
    "        economy_x_train, economy_y_train = samples\n",
    "\n",
    "        prediction, hidden = model(economy_x_train)\n",
    "        cost = criterion(prediction, economy_y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, samples in enumerate(economy_val_loader):\n",
    "            economy_x_val, economy_y_val = samples\n",
    "\n",
    "            prediction, hidden = model(economy_x_val)\n",
    "            loss = criterion(prediction, economy_y_val)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(economy_val_loader)\n",
    "    print(f'Epoch {epoch}/{num_epochs}, Training Loss: {cost.item()}, Validation Loss: {val_loss}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), '../데이터/Checkpoint/best_rnn_economy_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 부동산 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5\n",
    "hidden_size = 16\n",
    "output_size = 1\n",
    "\n",
    "lr = 1e-4\n",
    "num_epochs = 300\n",
    "\n",
    "model = LSTM(input_size=input_size, hidden_size=hidden_size, output_size=output_size, device=device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\LSTM_Transaction_Train_Dataset.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['계약년월'] = pd.to_datetime(df['계약년월'].astype(str), format='%Y%m')\n",
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\LSTM_Transaction_Train_Dataset.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['계약년월'] = pd.to_datetime(df['계약년월'].astype(str), format='%Y%m')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 크기 : torch.Size([1, 5])\n",
      "Y 크기 : torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "train_dataset = LSTM_Transaction_Train_Dataset(transaction_df[transaction_df['계약년월']//100 != 2022])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "val_dataset = LSTM_Transaction_Train_Dataset(transaction_df[transaction_df['계약년월']//100 == 2022])\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "for x,y in train_loader:\n",
    "      print(\"X 크기 : {}\".format(x.shape))\n",
    "      print(\"Y 크기 : {}\".format(y.shape))\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/300, Training Loss: 0.8823492953859482\n",
      "Epoch 1/300, Training Loss: 0.17963628630959538\n",
      "Epoch 2/300, Training Loss: 0.16968892930227597\n",
      "Epoch 3/300, Training Loss: 0.16612966451157676\n",
      "Epoch 4/300, Training Loss: 0.16316348854310248\n",
      "Epoch 5/300, Training Loss: 0.1621046671194399\n",
      "Epoch 6/300, Training Loss: 0.15932035175085682\n",
      "Epoch 7/300, Training Loss: 0.16134757294320795\n",
      "Epoch 8/300, Training Loss: 0.16035716731819313\n",
      "Epoch 9/300, Training Loss: 0.160125290477425\n",
      "Epoch 10/300, Training Loss: 0.15791949076810408\n",
      "Epoch 11/300, Training Loss: 0.15401207396001282\n",
      "Epoch 12/300, Training Loss: 0.1520675809292226\n",
      "Epoch 13/300, Training Loss: 0.15159820044819347\n",
      "Epoch 14/300, Training Loss: 0.15249401557658646\n",
      "Epoch 15/300, Training Loss: 0.15201810566741072\n",
      "Epoch 16/300, Training Loss: 0.15145841510364189\n",
      "Epoch 17/300, Training Loss: 0.15133124272583248\n",
      "Epoch 18/300, Training Loss: 0.15053684409646303\n",
      "Epoch 19/300, Training Loss: 0.14819120832728475\n",
      "Epoch 20/300, Training Loss: 0.14800069705764604\n",
      "Epoch 21/300, Training Loss: 0.14872578736608227\n",
      "Epoch 22/300, Training Loss: 0.14769913609128021\n",
      "Epoch 23/300, Training Loss: 0.14643378888818623\n",
      "Epoch 24/300, Training Loss: 0.14615973668833987\n",
      "Epoch 25/300, Training Loss: 0.14601787214273926\n",
      "Epoch 26/300, Training Loss: 0.1457160344431237\n",
      "Epoch 27/300, Training Loss: 0.14542051578320603\n",
      "Epoch 28/300, Training Loss: 0.14493701264031364\n",
      "Epoch 29/300, Training Loss: 0.14455731724414383\n",
      "Epoch 30/300, Training Loss: 0.14473592350257225\n",
      "Epoch 31/300, Training Loss: 0.14444447798953167\n",
      "Epoch 32/300, Training Loss: 0.14446608758358023\n",
      "Epoch 33/300, Training Loss: 0.14445517653640344\n",
      "Epoch 34/300, Training Loss: 0.14434610170115764\n",
      "Epoch 35/300, Training Loss: 0.14420806316205015\n",
      "Epoch 36/300, Training Loss: 0.14410143728962507\n",
      "Epoch 37/300, Training Loss: 0.14408623906538207\n",
      "Epoch 38/300, Training Loss: 0.1441804025562033\n",
      "Epoch 39/300, Training Loss: 0.14443269169637818\n",
      "Epoch 40/300, Training Loss: 0.14450033432000461\n",
      "Epoch 41/300, Training Loss: 0.14462027698472024\n",
      "Epoch 42/300, Training Loss: 0.14475091177966284\n",
      "Epoch 43/300, Training Loss: 0.14529159997881916\n",
      "Epoch 44/300, Training Loss: 0.14518474215998228\n",
      "Epoch 45/300, Training Loss: 0.1450744509080002\n",
      "Epoch 46/300, Training Loss: 0.14496151098804932\n",
      "Epoch 47/300, Training Loss: 0.14527429372741885\n",
      "Epoch 48/300, Training Loss: 0.14553017736328036\n",
      "Epoch 49/300, Training Loss: 0.14535958672970026\n",
      "Epoch 50/300, Training Loss: 0.14531994753035052\n",
      "Epoch 51/300, Training Loss: 0.14518628368371111\n",
      "Epoch 52/300, Training Loss: 0.14510827241066365\n",
      "Epoch 53/300, Training Loss: 0.14512805827968003\n",
      "Epoch 54/300, Training Loss: 0.14506104896276653\n",
      "Epoch 55/300, Training Loss: 0.14499622597813797\n",
      "Epoch 56/300, Training Loss: 0.1449551547474417\n",
      "Epoch 57/300, Training Loss: 0.1449544721439135\n",
      "Epoch 58/300, Training Loss: 0.14481240483228325\n",
      "Epoch 59/300, Training Loss: 0.14475422971661325\n",
      "Epoch 60/300, Training Loss: 0.1449543844136519\n",
      "Epoch 61/300, Training Loss: 0.1448992880539627\n",
      "Epoch 62/300, Training Loss: 0.14459793555710648\n",
      "Epoch 63/300, Training Loss: 0.14460331820359734\n",
      "Epoch 64/300, Training Loss: 0.14454367716744754\n",
      "Epoch 65/300, Training Loss: 0.14433894179971682\n",
      "Epoch 66/300, Training Loss: 0.1442451877430211\n",
      "Epoch 67/300, Training Loss: 0.14428271917598734\n",
      "Epoch 68/300, Training Loss: 0.14424199612252994\n",
      "Epoch 69/300, Training Loss: 0.14422036925520232\n",
      "Epoch 70/300, Training Loss: 0.14421503836005653\n",
      "Epoch 71/300, Training Loss: 0.14421694925318287\n",
      "Epoch 72/300, Training Loss: 0.1442671348834802\n",
      "Epoch 73/300, Training Loss: 0.1442409403649946\n",
      "Epoch 74/300, Training Loss: 0.14421544397328626\n",
      "Epoch 75/300, Training Loss: 0.1445375094953651\n",
      "Epoch 76/300, Training Loss: 0.14505483467339583\n",
      "Epoch 77/300, Training Loss: 0.1450321300258277\n",
      "Epoch 78/300, Training Loss: 0.14503578643762594\n",
      "Epoch 79/300, Training Loss: 0.14509369555321303\n",
      "Epoch 80/300, Training Loss: 0.1452187256652909\n",
      "Epoch 81/300, Training Loss: 0.14530075231206696\n",
      "Epoch 82/300, Training Loss: 0.14537589451190494\n",
      "Epoch 83/300, Training Loss: 0.14543607354249885\n",
      "Epoch 84/300, Training Loss: 0.14553109259864883\n",
      "Epoch 85/300, Training Loss: 0.14547472190541264\n",
      "Epoch 86/300, Training Loss: 0.1450367705712604\n",
      "Epoch 87/300, Training Loss: 0.14501616596201064\n",
      "Epoch 88/300, Training Loss: 0.14613442115362796\n",
      "Epoch 89/300, Training Loss: 0.1457977517963656\n",
      "Epoch 90/300, Training Loss: 0.14571621154325776\n",
      "Epoch 91/300, Training Loss: 0.14562755896722218\n",
      "Epoch 92/300, Training Loss: 0.14543966958532048\n",
      "Epoch 93/300, Training Loss: 0.14521402897203942\n",
      "Epoch 94/300, Training Loss: 0.1450845597802083\n",
      "Epoch 95/300, Training Loss: 0.1449819479134877\n",
      "Epoch 96/300, Training Loss: 0.14494424309913964\n",
      "Epoch 97/300, Training Loss: 0.1448935446346727\n",
      "Epoch 98/300, Training Loss: 0.14474952608926617\n",
      "Epoch 99/300, Training Loss: 0.14455657062559002\n",
      "Epoch 100/300, Training Loss: 0.14450176574181062\n",
      "Epoch 101/300, Training Loss: 0.1444076354238098\n",
      "Epoch 102/300, Training Loss: 0.14435999082220008\n",
      "Epoch 103/300, Training Loss: 0.14429277438053836\n",
      "Epoch 104/300, Training Loss: 0.1442935502768789\n",
      "Epoch 105/300, Training Loss: 0.14426740392696955\n",
      "Epoch 106/300, Training Loss: 0.14421570345000342\n",
      "Epoch 107/300, Training Loss: 0.1443713712950886\n",
      "Epoch 108/300, Training Loss: 0.14461177645315734\n",
      "Epoch 109/300, Training Loss: 0.14662508988054462\n",
      "Epoch 110/300, Training Loss: 0.14700839233497726\n",
      "Epoch 111/300, Training Loss: 0.14700078624167387\n",
      "Epoch 112/300, Training Loss: 0.14665136103807613\n",
      "Epoch 113/300, Training Loss: 0.1453137460528208\n",
      "Epoch 114/300, Training Loss: 0.14506375326269205\n",
      "Epoch 115/300, Training Loss: 0.14515911324119674\n",
      "Epoch 116/300, Training Loss: 0.14529093914762\n",
      "Epoch 117/300, Training Loss: 0.14545605089171895\n",
      "Epoch 118/300, Training Loss: 0.14572212614977\n",
      "Epoch 119/300, Training Loss: 0.14561906121152735\n",
      "Epoch 120/300, Training Loss: 0.14599556084624632\n",
      "Epoch 121/300, Training Loss: 0.14651788532749482\n",
      "Epoch 122/300, Training Loss: 0.14719385783187466\n",
      "Epoch 123/300, Training Loss: 0.14727424423474506\n",
      "Epoch 124/300, Training Loss: 0.14702156275929792\n",
      "Epoch 125/300, Training Loss: 0.14681298796471345\n",
      "Epoch 126/300, Training Loss: 0.146667259686918\n",
      "Epoch 127/300, Training Loss: 0.14656099232596198\n",
      "Epoch 128/300, Training Loss: 0.14639631564786235\n",
      "Epoch 129/300, Training Loss: 0.1462420048495552\n",
      "Epoch 130/300, Training Loss: 0.14609886091565194\n",
      "Epoch 131/300, Training Loss: 0.14595558165753103\n",
      "Epoch 132/300, Training Loss: 0.14583245690986024\n",
      "Epoch 133/300, Training Loss: 0.14569436454552143\n",
      "Epoch 134/300, Training Loss: 0.14560363714462998\n",
      "Epoch 135/300, Training Loss: 0.14552869964884527\n",
      "Epoch 136/300, Training Loss: 0.14559038272650104\n",
      "Epoch 137/300, Training Loss: 0.14557359193815078\n",
      "Epoch 138/300, Training Loss: 0.14555452311299324\n",
      "Epoch 139/300, Training Loss: 0.1455451856944521\n",
      "Epoch 140/300, Training Loss: 0.14554790283114546\n",
      "Epoch 141/300, Training Loss: 0.14566341377616435\n",
      "Epoch 142/300, Training Loss: 0.14578419810535656\n",
      "Epoch 143/300, Training Loss: 0.14597606081174347\n",
      "Epoch 144/300, Training Loss: 0.14634066364470258\n",
      "Epoch 145/300, Training Loss: 0.1466104067603018\n",
      "Epoch 146/300, Training Loss: 0.14732690273391272\n",
      "Epoch 147/300, Training Loss: 0.14759142830662622\n",
      "Epoch 148/300, Training Loss: 0.14869951135848783\n",
      "Epoch 149/300, Training Loss: 0.14838854044221464\n",
      "Epoch 150/300, Training Loss: 0.14768032001133116\n",
      "Epoch 151/300, Training Loss: 0.14755863436281408\n",
      "Epoch 152/300, Training Loss: 0.14796654898042216\n",
      "Epoch 153/300, Training Loss: 0.14848283551426583\n",
      "Epoch 154/300, Training Loss: 0.14883200737824223\n",
      "Epoch 155/300, Training Loss: 0.14894191279477156\n",
      "Epoch 156/300, Training Loss: 0.14902600309863095\n",
      "Epoch 157/300, Training Loss: 0.14878617179858283\n",
      "Epoch 158/300, Training Loss: 0.14857153538662737\n",
      "Epoch 159/300, Training Loss: 0.14807398892943405\n",
      "Epoch 160/300, Training Loss: 0.14793059279662493\n",
      "Epoch 161/300, Training Loss: 0.14792591324811993\n",
      "Epoch 162/300, Training Loss: 0.1476450884468537\n",
      "Epoch 163/300, Training Loss: 0.1474292309722401\n",
      "Epoch 164/300, Training Loss: 0.1471594255457599\n",
      "Epoch 165/300, Training Loss: 0.14689460047796613\n",
      "Epoch 166/300, Training Loss: 0.14665470191352414\n",
      "Epoch 167/300, Training Loss: 0.14657517740259007\n",
      "Epoch 168/300, Training Loss: 0.14657907121757197\n",
      "Epoch 169/300, Training Loss: 0.1463112474556003\n",
      "Epoch 170/300, Training Loss: 0.1466718043470398\n",
      "Epoch 171/300, Training Loss: 0.14671440493588542\n",
      "Epoch 172/300, Training Loss: 0.14648440017211106\n",
      "Epoch 173/300, Training Loss: 0.146266159592549\n",
      "Epoch 174/300, Training Loss: 0.14609688780245572\n",
      "Epoch 175/300, Training Loss: 0.14598608999885487\n",
      "Epoch 176/300, Training Loss: 0.1458989430623533\n",
      "Epoch 177/300, Training Loss: 0.14582366685273176\n",
      "Epoch 178/300, Training Loss: 0.14571295735456727\n",
      "Epoch 179/300, Training Loss: 0.14557103234020058\n",
      "Epoch 180/300, Training Loss: 0.1454064657486732\n",
      "Epoch 181/300, Training Loss: 0.14524110827978928\n",
      "Epoch 182/300, Training Loss: 0.1451698540320053\n",
      "Epoch 183/300, Training Loss: 0.1451324869339507\n",
      "Epoch 184/300, Training Loss: 0.14513742921004852\n",
      "Epoch 185/300, Training Loss: 0.14517033277916305\n",
      "Epoch 186/300, Training Loss: 0.14495969674460718\n",
      "Epoch 187/300, Training Loss: 0.14488423532087866\n",
      "Epoch 188/300, Training Loss: 0.14482176447613995\n",
      "Epoch 189/300, Training Loss: 0.14472949718722025\n",
      "Epoch 190/300, Training Loss: 0.1446255394796929\n",
      "Epoch 191/300, Training Loss: 0.14452841485308188\n",
      "Epoch 192/300, Training Loss: 0.14443402763810265\n",
      "Epoch 193/300, Training Loss: 0.14433781379541968\n",
      "Epoch 194/300, Training Loss: 0.1442428313670439\n",
      "Epoch 195/300, Training Loss: 0.14414909793763048\n",
      "Epoch 196/300, Training Loss: 0.14406106523684167\n",
      "Epoch 197/300, Training Loss: 0.14399220789763695\n",
      "Epoch 198/300, Training Loss: 0.14393331750120145\n",
      "Epoch 199/300, Training Loss: 0.1438741625428982\n",
      "Epoch 200/300, Training Loss: 0.1438153659131457\n",
      "Epoch 201/300, Training Loss: 0.14376337770838385\n",
      "Epoch 202/300, Training Loss: 0.14372269692082443\n",
      "Epoch 203/300, Training Loss: 0.14368960613677254\n",
      "Epoch 204/300, Training Loss: 0.14366330302857844\n",
      "Epoch 205/300, Training Loss: 0.14364433296288598\n",
      "Epoch 206/300, Training Loss: 0.14363286563593813\n",
      "Epoch 207/300, Training Loss: 0.14362871439562694\n",
      "Epoch 208/300, Training Loss: 0.14362958698314057\n",
      "Epoch 209/300, Training Loss: 0.14363425658950868\n",
      "Epoch 210/300, Training Loss: 0.1436410730056914\n",
      "Epoch 211/300, Training Loss: 0.1436500585279891\n",
      "Epoch 212/300, Training Loss: 0.1436594650321684\n",
      "Epoch 213/300, Training Loss: 0.14366824907103196\n",
      "Epoch 214/300, Training Loss: 0.14367734481085534\n",
      "Epoch 215/300, Training Loss: 0.1436875437042093\n",
      "Epoch 216/300, Training Loss: 0.14369882740008208\n",
      "Epoch 217/300, Training Loss: 0.14371190805084424\n",
      "Epoch 218/300, Training Loss: 0.14372887515630287\n",
      "Epoch 219/300, Training Loss: 0.14375036212803566\n",
      "Epoch 220/300, Training Loss: 0.14377547958773848\n",
      "Epoch 221/300, Training Loss: 0.14379788513830766\n",
      "Epoch 222/300, Training Loss: 0.14382569551844404\n",
      "Epoch 223/300, Training Loss: 0.14384930562810605\n",
      "Epoch 224/300, Training Loss: 0.14386396718275932\n",
      "Epoch 225/300, Training Loss: 0.14390378607523696\n",
      "Epoch 226/300, Training Loss: 0.14394666437569945\n",
      "Epoch 227/300, Training Loss: 0.14398564359807955\n",
      "Epoch 228/300, Training Loss: 0.14401844079117498\n",
      "Epoch 229/300, Training Loss: 0.1440441507979616\n",
      "Epoch 230/300, Training Loss: 0.14406026774027025\n",
      "Epoch 231/300, Training Loss: 0.14406709974683918\n",
      "Epoch 232/300, Training Loss: 0.1440680536405361\n",
      "Epoch 233/300, Training Loss: 0.14406572961261388\n",
      "Epoch 234/300, Training Loss: 0.14404963128698228\n",
      "Epoch 235/300, Training Loss: 0.14402393876715963\n",
      "Epoch 236/300, Training Loss: 0.14397959322025297\n",
      "Epoch 237/300, Training Loss: 0.1439237125141683\n",
      "Epoch 238/300, Training Loss: 0.14386202230691003\n",
      "Epoch 239/300, Training Loss: 0.14379580472152442\n",
      "Epoch 240/300, Training Loss: 0.14372773296775587\n",
      "Epoch 241/300, Training Loss: 0.14366113829570387\n",
      "Epoch 242/300, Training Loss: 0.1436007457829642\n",
      "Epoch 243/300, Training Loss: 0.1435449092988044\n",
      "Epoch 244/300, Training Loss: 0.14348633930597943\n",
      "Epoch 245/300, Training Loss: 0.14343493787270425\n",
      "Epoch 246/300, Training Loss: 0.1433864051135625\n",
      "Epoch 247/300, Training Loss: 0.14334697584842437\n",
      "Epoch 248/300, Training Loss: 0.14348082963609968\n",
      "Epoch 249/300, Training Loss: 0.1435049988851477\n",
      "Epoch 250/300, Training Loss: 0.1434996107813076\n",
      "Epoch 251/300, Training Loss: 0.1434825281803702\n",
      "Epoch 252/300, Training Loss: 0.14345661323846884\n",
      "Epoch 253/300, Training Loss: 0.14341201582755314\n",
      "Epoch 254/300, Training Loss: 0.14334996966223323\n",
      "Epoch 255/300, Training Loss: 0.14326783327772893\n",
      "Epoch 256/300, Training Loss: 0.14317333198950147\n",
      "Epoch 257/300, Training Loss: 0.14305863407711597\n",
      "Epoch 258/300, Training Loss: 0.14301980143827173\n",
      "Epoch 259/300, Training Loss: 0.142984206235507\n",
      "Epoch 260/300, Training Loss: 0.14296059540785858\n",
      "Epoch 261/300, Training Loss: 0.14296685939293\n",
      "Epoch 262/300, Training Loss: 0.14285891667104222\n",
      "Epoch 263/300, Training Loss: 0.14281011989193243\n",
      "Epoch 264/300, Training Loss: 0.14280638075137092\n",
      "Epoch 265/300, Training Loss: 0.14282906389077218\n",
      "Epoch 266/300, Training Loss: 0.1428189630145981\n",
      "Epoch 267/300, Training Loss: 0.1428531661444167\n",
      "Epoch 268/300, Training Loss: 0.14297734607874238\n",
      "Epoch 269/300, Training Loss: 0.14306698729459047\n",
      "Epoch 270/300, Training Loss: 0.14306817954495002\n",
      "Epoch 271/300, Training Loss: 0.14307723464789607\n",
      "Epoch 272/300, Training Loss: 0.1430934828054805\n",
      "Epoch 273/300, Training Loss: 0.14310263684634145\n",
      "Epoch 274/300, Training Loss: 0.1431004948152112\n",
      "Epoch 275/300, Training Loss: 0.143089575980025\n",
      "Epoch 276/300, Training Loss: 0.14307683392093204\n",
      "Epoch 277/300, Training Loss: 0.14308461915200887\n",
      "Epoch 278/300, Training Loss: 0.14307040458163586\n",
      "Epoch 279/300, Training Loss: 0.1430643568215815\n",
      "Epoch 280/300, Training Loss: 0.14306937365115566\n",
      "Epoch 281/300, Training Loss: 0.14305787262639127\n",
      "Epoch 282/300, Training Loss: 0.14305719527036795\n",
      "Epoch 283/300, Training Loss: 0.14305680085289654\n",
      "Epoch 284/300, Training Loss: 0.14305428336903617\n",
      "Epoch 285/300, Training Loss: 0.14305524800620362\n",
      "Epoch 286/300, Training Loss: 0.14306208075343427\n",
      "Epoch 287/300, Training Loss: 0.14307769516661417\n",
      "Epoch 288/300, Training Loss: 0.14311344286151634\n",
      "Epoch 289/300, Training Loss: 0.14315527813797027\n",
      "Epoch 290/300, Training Loss: 0.14319295638745907\n",
      "Epoch 291/300, Training Loss: 0.1432333315920582\n",
      "Epoch 292/300, Training Loss: 0.14326315289899735\n",
      "Epoch 293/300, Training Loss: 0.1432995418102809\n",
      "Epoch 294/300, Training Loss: 0.14334856631895462\n",
      "Epoch 295/300, Training Loss: 0.14340112197231838\n",
      "Epoch 296/300, Training Loss: 0.14346739646295567\n",
      "Epoch 297/300, Training Loss: 0.14354222080228765\n",
      "Epoch 298/300, Training Loss: 0.14361805441504225\n",
      "Epoch 299/300, Training Loss: 0.14369782863629577\n",
      "Epoch 300/300, Training Loss: 0.1437807681004197\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs+1):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    \n",
    "    for x_train, y_train in train_loader:\n",
    "        x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "\n",
    "        prediction, hidden = model(x_train)\n",
    "        cost = criterion(prediction, y_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += cost.item()\n",
    "        \n",
    "    total_train_loss /= len(train_loader)\n",
    "    train_losses.append(total_train_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch}/{num_epochs}, Training Loss: {total_train_loss}')\n",
    "    \n",
    "    save_path = f\"../데이터/Checkpoint/lstm_transaction_train_model_epoch_{epoch}.pth\"\n",
    "    torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_df = pd.DataFrame({\n",
    "    'train_losses': train_losses,\n",
    "    # 'val_losses': val_losses\n",
    "})\n",
    "\n",
    "losses_df.to_excel('lstm_transaction_train_losses.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_df = pd.read_excel('../데이터/Loss/lstm_transaction_train_losses.xlsx')\n",
    "\n",
    "# plt.figure(figsize=(12, 8)) \n",
    "# plt.plot(losses_df['train_losses'], label='Training Loss')\n",
    "# plt.plot(losses_df['val_losses'], label='Validation Loss')\n",
    "# plt.title('Training and Validation Losses', color='white')\n",
    "# plt.xlabel('Epoch', color='white')\n",
    "# plt.ylabel('Loss', color='white')\n",
    "# plt.xticks(color='white')\n",
    "# plt.yticks(color='white')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8)) \n",
    "plt.plot(losses_df['train_losses'], label='Training Loss')\n",
    "plt.title('Training Losses', color='white')\n",
    "plt.xlabel('Epoch', color='white')\n",
    "plt.ylabel('Loss', color='white')\n",
    "plt.xticks(color='white')\n",
    "plt.yticks(color='white')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(12, 8)) \n",
    "# plt.plot(losses_df['val_losses'], label='Validation Loss')\n",
    "# plt.title('Validation Losses', color='white')\n",
    "# plt.xlabel('Epoch', color='white')\n",
    "# plt.ylabel('Loss', color='white')\n",
    "# plt.xticks(color='white')\n",
    "# plt.yticks(color='white')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NODE 부동산 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transaction_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hkyoo\\OneDrive\\바탕 화면\\SCI\\코드\\train.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hkyoo/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hkyoo/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m LSTM_Transaction_Dataset(transaction_df[transaction_df[\u001b[39m'\u001b[39m\u001b[39m계약년월\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m2022\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hkyoo/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hkyoo/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m val_dataset \u001b[39m=\u001b[39m LSTM_Transaction_Dataset(transaction_df[transaction_df[\u001b[39m'\u001b[39m\u001b[39m계약년월\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m2022\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transaction_df' is not defined"
     ]
    }
   ],
   "source": [
    "# 데이터 다운로드\n",
    "batch_size = 1\n",
    "transaction_df = pd.read_excel('../데이터/Transaction/transaction_final.xlsx', index_col=0)\n",
    "\n",
    "train_dataset = ODE_Transaction_Dataset(transaction_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 크기 : torch.Size([2, 5])\n",
      "Y 크기 : torch.Size([2, 5])\n",
      "Z 크기 : torch.Size([2, 1])\n",
      "W 크기 : torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "for x,y,z,w in train_loader:\n",
    "      print(\"X 크기 : {}\".format(x.shape))\n",
    "      print(\"Y 크기 : {}\".format(y.shape))\n",
    "      print(\"Z 크기 : {}\".format(z.shape))\n",
    "      print(\"W 크기 : {}\".format(w.shape))\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 크기 : torch.float32\n",
      "Y 크기 : torch.float32\n",
      "Z 크기 : torch.float32\n",
      "W 크기 : torch.float32\n"
     ]
    }
   ],
   "source": [
    "for x,y,z,w in train_loader:\n",
    "      print(\"X 크기 : {}\".format(x.dtype))\n",
    "      print(\"Y 크기 : {}\".format(y.dtype))\n",
    "      print(\"Z 크기 : {}\".format(z.dtype))\n",
    "      print(\"W 크기 : {}\".format(w.dtype))\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu is available\n",
      "작동하는지 실험\n",
      "(tensor([[[0.3200],\n",
      "         [0.3020]],\n",
      "\n",
      "        [[0.3165],\n",
      "         [0.3053]],\n",
      "\n",
      "        [[0.3074],\n",
      "         [0.3091]],\n",
      "\n",
      "        [[0.3381],\n",
      "         [0.3102]],\n",
      "\n",
      "        [[0.3129],\n",
      "         [0.3069]]], grad_fn=<SliceBackward0>), tensor([[[-1.9246,  0.9916,  0.5816,  ..., -0.0953, -0.4212,  0.2555],\n",
      "         [-0.3911,  0.1918,  0.4691,  ..., -0.1469, -0.4416,  0.2056]],\n",
      "\n",
      "        [[-2.3361,  0.9280,  0.5030,  ..., -0.3589, -0.1182, -0.2516],\n",
      "         [-0.4929,  0.1927, -0.0292,  ...,  0.3396, -0.2586, -0.0658]],\n",
      "\n",
      "        [[-3.3343,  0.8663,  0.3372,  ..., -1.4191,  0.6670, -1.6950],\n",
      "         [-0.4960,  0.2321, -0.2474,  ...,  0.5994, -0.1838, -0.2525]],\n",
      "\n",
      "        [[ 0.8072,  1.3306,  1.7080,  ..., -0.8884, -1.7589,  2.7519],\n",
      "         [ 0.6183,  0.7583,  1.9381,  ..., -1.3055, -0.8485,  0.8296]],\n",
      "\n",
      "        [[-2.7293,  0.8898,  0.4417,  ..., -0.7159,  0.1923, -0.7784],\n",
      "         [-0.5078,  0.2105, -0.1448,  ...,  0.4671, -0.2156, -0.1514]],\n",
      "\n",
      "        [[ 1.2445,  1.3671,  1.8943,  ..., -1.1631, -1.9350,  3.0802],\n",
      "         [ 1.2932,  1.1343,  2.5548,  ..., -1.4962, -0.9968,  1.2692]]],\n",
      "       grad_fn=<ViewBackward0>), tensor([[ 3.9042e+00,  1.1460e+00, -8.7323e-01, -2.0277e-01, -1.4232e+00,\n",
      "         -9.7355e-01, -2.9226e-01,  1.7025e+00,  4.9917e-01,  3.9957e-02,\n",
      "         -4.1916e-01,  2.4280e-01, -1.5132e-02,  1.7395e-01, -2.0224e+00,\n",
      "          5.1864e-01,  1.5203e+00,  3.3595e+00, -8.0708e-01, -6.4386e-01,\n",
      "         -9.6330e-02,  8.3888e-01,  2.4769e-02, -2.2345e-01,  5.3007e-01,\n",
      "          1.2489e+00,  1.1800e+00,  3.9309e-01, -8.7714e-01,  1.6921e+00,\n",
      "          1.0720e+00,  5.2080e-01,  3.8115e-01, -1.2353e+00, -2.2096e-01,\n",
      "          4.0373e-01,  1.1288e+00, -3.7577e-01,  9.0160e-01,  1.1998e+00,\n",
      "          1.8166e+00, -1.0170e+00,  2.2276e-01, -6.2415e-01, -5.6234e-01,\n",
      "         -2.6533e-01,  5.2966e-01,  3.7777e-01, -7.3276e-02, -6.4162e-01,\n",
      "          4.7550e-01,  2.0280e+00, -3.7411e-01,  4.5963e-01,  4.7883e-01,\n",
      "         -5.3839e-01,  2.2202e+00, -6.2920e-02, -1.1106e-01, -8.5488e-01,\n",
      "         -3.6557e-01, -2.5014e+00,  5.9920e-01, -3.6857e-01],\n",
      "        [ 1.0762e-01, -3.4166e-01, -1.0522e+00,  1.2187e+00, -5.6238e-01,\n",
      "          1.9079e+00, -7.6549e-01,  5.2089e-01,  7.2618e-01, -4.3966e-01,\n",
      "         -1.1483e+00,  4.7966e-01,  5.7630e-01,  8.1643e-01, -1.3576e+00,\n",
      "         -5.7244e-01,  7.6411e-01,  2.0117e-02,  2.5561e-01,  1.0010e+00,\n",
      "         -4.1395e-01, -8.5190e-01,  1.2236e-01, -6.9191e-01,  6.0380e-01,\n",
      "         -6.9187e-01,  1.0257e+00,  1.0411e+00, -8.2717e-01, -1.0858e+00,\n",
      "         -1.0233e+00, -1.1872e+00, -1.3930e+00, -1.4010e-01, -8.0047e-01,\n",
      "          2.9662e-01,  1.9676e+00,  4.4584e-01,  1.9046e+00, -2.1059e-01,\n",
      "          1.5923e+00,  2.0074e-01, -1.1612e-01,  1.8093e-01,  1.2243e+00,\n",
      "          2.7559e-03,  1.3391e-01, -5.1903e-01, -1.5699e+00, -3.7198e-02,\n",
      "         -6.9918e-02,  3.8163e-01, -7.4425e-02, -7.5933e-01, -8.8816e-01,\n",
      "          1.7010e-01, -7.8576e-01,  1.8825e+00,  8.4531e-01, -8.4700e-01,\n",
      "         -1.5939e+00, -3.7945e-01, -5.2958e-01, -7.4874e-01]],\n",
      "       grad_fn=<AddBackward0>), tensor([[-2.7978e-02,  2.1564e-02, -1.9743e-02, -3.8898e-03,  3.6597e-02,\n",
      "         -5.5066e-02,  4.0261e-02,  4.9608e-02, -2.5658e-03,  9.2387e-02,\n",
      "         -6.3146e-02, -2.8552e-02,  3.3216e-02,  8.2328e-02, -6.9201e-03,\n",
      "         -3.6288e-02,  3.0480e-02, -9.8560e-03,  3.6546e-02,  2.4284e-02,\n",
      "         -4.0156e-02,  7.7440e-03, -2.4673e-02,  1.6008e-02,  3.8246e-02,\n",
      "          1.9721e-02,  7.4606e-02,  6.3284e-02, -3.7887e-03,  5.4471e-02,\n",
      "         -4.2920e-02, -3.5930e-02,  6.2402e-02,  7.3542e-02, -4.3362e-02,\n",
      "         -9.6650e-03, -9.0047e-02,  7.8424e-02,  1.5734e-03, -2.0391e-03,\n",
      "          6.8380e-02, -4.2523e-03, -5.9703e-02,  7.0611e-02,  6.4688e-02,\n",
      "         -6.2975e-02,  3.0866e-02, -2.6243e-02,  3.6632e-03, -4.1667e-02,\n",
      "          2.1046e-02, -3.1662e-02,  1.5487e-03, -1.8984e-02, -2.9312e-02,\n",
      "         -1.0388e-01,  2.0708e-02, -1.3626e-02, -4.2216e-02, -6.5238e-02,\n",
      "          5.3133e-02,  7.1187e-02,  5.7235e-02,  4.6491e-02],\n",
      "        [-1.9571e-02,  4.3879e-02, -5.0783e-03, -9.8894e-03,  3.7486e-02,\n",
      "         -6.2992e-02,  5.2318e-02,  3.7628e-02, -1.7702e-02,  8.1281e-02,\n",
      "         -4.3972e-02, -1.0214e-02,  5.3238e-02,  8.3248e-02, -2.6203e-04,\n",
      "         -3.3267e-02,  2.7521e-02, -8.1333e-04,  2.3006e-02,  3.6827e-02,\n",
      "         -4.1841e-02,  1.3795e-02, -4.0217e-02,  5.5267e-03,  1.2294e-02,\n",
      "          4.9167e-05,  6.8872e-02,  5.1969e-02,  5.1642e-03,  4.4822e-02,\n",
      "         -3.3580e-02, -4.7620e-02,  7.9498e-02,  4.5254e-02, -4.0060e-02,\n",
      "          6.4915e-04, -6.5277e-02,  6.5421e-02, -1.5464e-02, -3.0794e-02,\n",
      "          8.9416e-02,  3.7358e-03, -6.7441e-02,  7.1974e-02,  5.4571e-02,\n",
      "         -5.3111e-02,  3.0609e-02, -2.8819e-02, -1.4598e-02, -4.4193e-02,\n",
      "          3.2197e-02, -5.3195e-02, -7.4973e-03, -1.8159e-02, -1.2066e-02,\n",
      "         -1.0746e-01,  3.8273e-02, -3.0866e-03, -3.4713e-02, -1.0123e-01,\n",
      "          4.4599e-02,  7.0794e-02,  5.9186e-02,  6.3070e-02]],\n",
      "       grad_fn=<SliceBackward0>), tensor([[ 0.0155, -0.0075, -0.0300, -0.0190, -0.0339, -0.0477, -0.0162, -0.0334,\n",
      "         -0.0602, -0.0859,  0.0301,  0.0979, -0.0266,  0.0036,  0.0496,  0.0510,\n",
      "          0.0025, -0.0809, -0.0337,  0.0760,  0.0043,  0.0196,  0.0543, -0.0725,\n",
      "         -0.0324,  0.0053,  0.0032,  0.0442, -0.0300,  0.0521, -0.0945,  0.0549,\n",
      "          0.0717, -0.1073, -0.0669,  0.0128, -0.0119,  0.0157,  0.0189,  0.0650,\n",
      "          0.0086, -0.0841,  0.0187,  0.0170,  0.0094, -0.0108, -0.0065,  0.0556,\n",
      "          0.0479,  0.0164, -0.0572, -0.0344, -0.0694,  0.0377, -0.0301, -0.0064,\n",
      "          0.1002, -0.0484,  0.0019, -0.0736, -0.0714,  0.0944, -0.0039, -0.0209],\n",
      "        [ 0.0579, -0.0116, -0.0288, -0.0306, -0.0371, -0.0444, -0.0290, -0.0292,\n",
      "         -0.0747, -0.0663,  0.0496,  0.0840, -0.0257,  0.0075,  0.0487,  0.0475,\n",
      "          0.0025, -0.0878, -0.0380,  0.0758,  0.0044,  0.0135,  0.0488, -0.0813,\n",
      "         -0.0364,  0.0076, -0.0205,  0.0302, -0.0115,  0.0511, -0.0846,  0.0607,\n",
      "          0.0711, -0.0939, -0.0811, -0.0033, -0.0092,  0.0347,  0.0322,  0.0580,\n",
      "         -0.0061, -0.0910, -0.0055,  0.0147,  0.0043, -0.0052, -0.0028,  0.0524,\n",
      "          0.0341,  0.0373, -0.0631, -0.0340, -0.0806,  0.0498, -0.0214, -0.0112,\n",
      "          0.1168, -0.0336, -0.0097, -0.0747, -0.0705,  0.1314,  0.0226, -0.0298]],\n",
      "       grad_fn=<SliceBackward0>), tensor([[0.3404],\n",
      "        [0.3172]], grad_fn=<SelectBackward0>))\n",
      "torch.Size([5, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# 데이터 & 모델에 device 붙임!!!\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # 윈도우 gpu\n",
    "# device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu') # 맥 gpu\n",
    "print(f'{device} is available')\n",
    "\n",
    "model = NODE(output_dim=1,hidden_dim=256,latent_dim=64).to(device)\n",
    "\n",
    "print('작동하는지 실험')\n",
    "basic_data = torch.rand((5,2,1)).to(device)  # window_size, batch_size, 1\n",
    "time = torch.FloatTensor([[1,2,3,6,10,12],[1,3,5,8,10,12]]).reshape(6,2,1).to(device) # window_size, batch_size, 1\n",
    "data = model(basic_data,time)\n",
    "print(data)\n",
    "print(data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "noise_std = 0.02\n",
    "optim = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), lr=0.001)\n",
    "\n",
    "num_epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0\n",
      "loss : 11219012608.0, best loss : 11219012608.0\n",
      "loss : 13275686912.0, best loss : 11219012608.0\n",
      "loss : 13226204160.0, best loss : 11219012608.0\n",
      "loss : 11485132800.0, best loss : 11219012608.0\n",
      "loss : 8940823552.0, best loss : 8940823552.0\n",
      "loss : 7942053888.0, best loss : 7942053888.0\n",
      "loss : 8093862912.0, best loss : 7942053888.0\n",
      "loss : 9578081280.0, best loss : 7942053888.0\n",
      "loss : 11440328704.0, best loss : 7942053888.0\n",
      "loss : 13386883072.0, best loss : 7942053888.0\n",
      "loss : 15193017344.0, best loss : 7942053888.0\n",
      "loss : 1.5491499810317402e+19, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kimhakhyun/Desktop/Git/sci/SCI/코드/train.ipynb 셀 17\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimhakhyun/Desktop/Git/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m window_size\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimhakhyun/Desktop/Git/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()   \u001b[39m############### 여기서 second 오류가 나는지 확인!!!!\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kimhakhyun/Desktop/Git/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimhakhyun/Desktop/Git/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X22sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m optim\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimhakhyun/Desktop/Git/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X22sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/function.py:274\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    273\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 274\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/Desktop/Git/sci/SCI/코드/Model/ODEF.py:124\u001b[0m, in \u001b[0;36mODEAdjoint.backward\u001b[0;34m(ctx, dLdz)\u001b[0m\n\u001b[1;32m    120\u001b[0m adj_t[i_t] \u001b[39m=\u001b[39m adj_t[i_t] \u001b[39m-\u001b[39m dLdt_i\n\u001b[1;32m    122\u001b[0m aug_z \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((z_i\u001b[39m.\u001b[39mview(bs, n_dim), adj_z, torch\u001b[39m.\u001b[39mzeros(bs, n_params)\u001b[39m.\u001b[39mto(z), adj_t[i_t]), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 124\u001b[0m aug_ans \u001b[39m=\u001b[39m ode_solve(aug_z, t_i, t[i_t\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], augmented_dynamics)\n\u001b[1;32m    126\u001b[0m adj_z[:] \u001b[39m=\u001b[39m aug_ans[:, n_dim:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mn_dim]\n\u001b[1;32m    127\u001b[0m adj_p[:] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m aug_ans[:, \u001b[39m2\u001b[39m\u001b[39m*\u001b[39mn_dim:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mn_dim \u001b[39m+\u001b[39m n_params]\n",
      "File \u001b[0;32m~/Desktop/Git/sci/SCI/코드/Model/ODEF.py:28\u001b[0m, in \u001b[0;36mode_solve\u001b[0;34m(z0, t0, t1, f)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m i_step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_steps):\n\u001b[1;32m     27\u001b[0m     z \u001b[39m=\u001b[39m z \u001b[39m+\u001b[39m h \u001b[39m*\u001b[39m f(z, t)\n\u001b[0;32m---> 28\u001b[0m     t \u001b[39m=\u001b[39m t \u001b[39m+\u001b[39m h\n\u001b[1;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m z\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_train_loss = float('inf') \n",
    "for epoch in range(num_epochs + 1):\n",
    "    print('epoch : ',epoch)\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for batch_idx, samples in enumerate(train_loader):\n",
    "        tran_x, time_x, tran_y, time_y = samples\n",
    "        \n",
    "        t = torch.cat((time_x,time_y),dim=1)  \n",
    "        tran_x = tran_x.transpose(0,1).unsqueeze(2).to(device)\n",
    "        t = t.transpose(0,1).unsqueeze(2).to(device)\n",
    "        \n",
    "        x_p, _, z, z_mean, z_log_var, pred = model(tran_x, t)\n",
    "        kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean**2 - torch.exp(z_log_var), -1)\n",
    "        loss = 0.5 * ((tran_x-x_p)**2).sum(-1).sum(0) / noise_std**2 + kl_loss\n",
    "        loss = torch.mean(loss)\n",
    "        loss /= window_size\n",
    "        optim.zero_grad()   ############### 여기서 second 오류가 나는지 확인!!!!\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if loss < best_train_loss:\n",
    "            best_train_loss = loss\n",
    "            torch.save(model.state_dict(), \"../데이터/checkpoint/best_ODE_transaction_model.pth\")\n",
    "        \n",
    "        print('loss : {}, best loss : {}'.format(loss, best_train_loss))\n",
    "    print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ODERNN 부동산 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "num_epochs = 300\n",
    "\n",
    "model = ODE_RNN(256,64,1,200,200,device).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\ODE_Transaction_Dataset.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'].astype(str), format='%Y%m')\n",
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\ODE_Transaction_Dataset.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'].astype(str), format='%Y%m')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "train_dataset = ODE_Transaction_Dataset(transaction_df[transaction_df['계약년월']//100 != 2022])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "val_dataset = ODE_Transaction_Dataset(transaction_df[transaction_df['계약년월']//100 == 2022])\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 크기 : torch.Size([1, 5])\n",
      "Y 크기 : torch.Size([1, 5])\n",
      "Z 크기 : torch.Size([1, 1])\n",
      "W 크기 : torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "for x,y,z,w in train_loader:\n",
    "      print(\"X 크기 : {}\".format(x.shape))\n",
    "      print(\"Y 크기 : {}\".format(y.shape))\n",
    "      print(\"Z 크기 : {}\".format(z.shape))\n",
    "      print(\"W 크기 : {}\".format(w.shape))\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작동하는지 실험\n",
      "tensor([[0., 0., 0., 0., 0., 0.]], device='cuda:0', grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "print('작동하는지 실험')\n",
    "t = torch.FloatTensor([[1.,2.,3.,5.,9.,12.]]).to(device)\n",
    "data = torch.FloatTensor([[123,126,266,279,300]]).to(device)\n",
    "\n",
    "out = model(data,t)\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 학습 되는지 파악하기!!!!!!!\n",
    "\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# print(f'{device} is available')\n",
    "\n",
    "# model = ODE_RNN(256,64,1,200,200,device).to(device)\n",
    "\n",
    "# print('작동하는지 실험')\n",
    "# t = torch.FloatTensor([[1.,2.,3.,5.,9.,12.]]).to(device)\n",
    "# data = torch.FloatTensor([[123,126,266,279,300]]).to(device)\n",
    "\n",
    "# out, _ = model(data,t)\n",
    "\n",
    "# loss = criterion(out[0][:5], torch.FloatTensor([2,5,6,7,8]))\n",
    "\n",
    "# loss.backward()\n",
    "\n",
    "# for name,param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name,param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:530: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/300, LR: 0.0001, Training Loss: 42.8354185386184, Validation Loss: 29.2708740234375\n",
      "Epoch 1/300, LR: 0.0001, Training Loss: 42.345011187520115, Validation Loss: 29.2708740234375\n",
      "Epoch 2/300, LR: 0.0001, Training Loss: 42.345011187520115, Validation Loss: 29.2708740234375\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs+1):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    for batch_idx, (tran_x, time_x, tran_y, time_y) in enumerate(train_loader):\n",
    "        tran_x, time_x, tran_y, time_y = tran_x.to(device), time_x.to(device), tran_y.to(device), time_y.to(device)\n",
    "        time_f = torch.cat([time_x,time_y], axis=1)\n",
    "\n",
    "        prediction, hidden = model(tran_x, time_f)\n",
    "        cost = criterion(prediction[0][:5], tran_x[0])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += cost.item()\n",
    "    \n",
    "    total_train_loss /= len(train_loader)\n",
    "    train_losses.append(total_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (tran_x, time_x, tran_y, time_y) in enumerate(val_loader):\n",
    "            tran_x, time_x, tran_y, time_y = tran_x.to(device), time_x.to(device), tran_y.to(device), time_y.to(device)\n",
    "            time_f = torch.cat([time_x,time_y], axis=1)\n",
    "            \n",
    "            prediction, hidden = model(tran_x, time_f)\n",
    "            loss = criterion(prediction, y_val)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch}/{num_epochs}, LR: {lr}, Training Loss: {total_train_loss}, Validation Loss: {val_loss}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_path = f\"../데이터/Checkpoint/best_ode_transaction_model(300 epoch_0.0001 lr).pth\"\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "save_path = f\"../데이터/Checkpoint/recent_ode_transaction_model(300 epoch_0.0001 lr).pth\"\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\ODE_Transaction_Dataset.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'].astype(str), format='%Y%m')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning_rate: 1e-07, Epoch 0/5, Training Loss: 422.15655517578125\n",
      "Learning_rate: 1e-07, Epoch 1/5, Training Loss: 422.1231384277344\n",
      "Learning_rate: 1e-07, Epoch 2/5, Training Loss: 422.0897521972656\n",
      "Learning_rate: 1e-07, Epoch 3/5, Training Loss: 422.0563659667969\n",
      "Learning_rate: 1e-07, Epoch 4/5, Training Loss: 422.02294921875\n",
      "Learning_rate: 1e-07, Epoch 5/5, Training Loss: 421.989501953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\ODE_Transaction_Dataset.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'].astype(str), format='%Y%m')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning_rate: 1e-06, Epoch 0/5, Training Loss: 421.86181640625\n",
      "Learning_rate: 1e-06, Epoch 1/5, Training Loss: 421.5279235839844\n",
      "Learning_rate: 1e-06, Epoch 2/5, Training Loss: 421.1943359375\n",
      "Learning_rate: 1e-06, Epoch 3/5, Training Loss: 420.86114501953125\n",
      "Learning_rate: 1e-06, Epoch 4/5, Training Loss: 420.5282287597656\n",
      "Learning_rate: 1e-06, Epoch 5/5, Training Loss: 420.1957092285156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\ODE_Transaction_Dataset.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'].astype(str), format='%Y%m')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning_rate: 1e-05, Epoch 0/5, Training Loss: 418.931640625\n",
      "Learning_rate: 1e-05, Epoch 1/5, Training Loss: 415.6735534667969\n",
      "Learning_rate: 1e-05, Epoch 2/5, Training Loss: 412.51904296875\n",
      "Learning_rate: 1e-05, Epoch 3/5, Training Loss: 409.501953125\n",
      "Learning_rate: 1e-05, Epoch 4/5, Training Loss: 406.6498718261719\n",
      "Learning_rate: 1e-05, Epoch 5/5, Training Loss: 403.9827880859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\ODE_Transaction_Dataset.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'].astype(str), format='%Y%m')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning_rate: 0.0001, Epoch 0/5, Training Loss: 396.4898376464844\n",
      "Learning_rate: 0.0001, Epoch 1/5, Training Loss: 386.22821044921875\n",
      "Learning_rate: 0.0001, Epoch 2/5, Training Loss: 383.25341796875\n",
      "Learning_rate: 0.0001, Epoch 3/5, Training Loss: 382.4549865722656\n",
      "Learning_rate: 0.0001, Epoch 4/5, Training Loss: 382.2449645996094\n",
      "Learning_rate: 0.0001, Epoch 5/5, Training Loss: 382.1899108886719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\ODE_Transaction_Dataset.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'].astype(str), format='%Y%m')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning_rate: 0.001, Epoch 0/5, Training Loss: 382.3034362792969\n",
      "Learning_rate: 0.001, Epoch 1/5, Training Loss: 382.17236328125\n",
      "Learning_rate: 0.001, Epoch 2/5, Training Loss: 382.1704406738281\n",
      "Learning_rate: 0.001, Epoch 3/5, Training Loss: 382.1704406738281\n",
      "Learning_rate: 0.001, Epoch 4/5, Training Loss: 382.1704406738281\n",
      "Learning_rate: 0.001, Epoch 5/5, Training Loss: 382.1704406738281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\ODE_Transaction_Dataset.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'].astype(str), format='%Y%m')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning_rate: 0.01, Epoch 0/5, Training Loss: 382.17230224609375\n",
      "Learning_rate: 0.01, Epoch 1/5, Training Loss: 382.1704406738281\n",
      "Learning_rate: 0.01, Epoch 2/5, Training Loss: 382.1704406738281\n",
      "Learning_rate: 0.01, Epoch 3/5, Training Loss: 382.1704406738281\n",
      "Learning_rate: 0.01, Epoch 4/5, Training Loss: 382.1704406738281\n",
      "Learning_rate: 0.01, Epoch 5/5, Training Loss: 382.1704406738281\n"
     ]
    }
   ],
   "source": [
    "for lr in [1e-7,1e-6,1e-5,1e-4,1e-3,1e-2]:\n",
    "    model = ODE_RNN(256,64,1,200,200,device).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_train_loss = float('inf') \n",
    "    for epoch in range(num_epochs + 1):\n",
    "        losses = []\n",
    "        model.train()\n",
    "        \n",
    "        for batch_idx, samples in enumerate(train_loader):\n",
    "            tran_x, time_x, tran_y, time_y = samples\n",
    "            tran_x, time_x, tran_y, time_y = tran_x.to(device), time_x.to(device), tran_y.to(device), time_y.to(device)\n",
    "            time_f = torch.cat([time_x,time_y],axis=1)\n",
    "            prediction, hidden = model(tran_x,time_f)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            cost = criterion(prediction[0][:5], tran_x[0])\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "        # for name,param in model.named_parameters():\n",
    "            # if param.requires_grad:\n",
    "            #     print(name,param.grad)\n",
    "            #     break\n",
    "        print(f'Learning_rate: {lr}, Epoch {epoch}/{num_epochs}, Training Loss: {cost.item()}')\n",
    "        # model.eval()\n",
    "        # val_loss = 0.0\n",
    "        # with torch.no_grad():\n",
    "        #     for batch_idx, samples in enumerate(transaction_val_loader):\n",
    "        #         x_val, y_val = samples\n",
    "\n",
    "        #         prediction, hidden = model(x_val)\n",
    "        #         loss = criterion(prediction, y_val)\n",
    "        #         val_loss += loss.item()\n",
    "\n",
    "        # val_loss /= len(transaction_val_loader)\n",
    "        # print(f'Epoch {epoch}/{num_epochs}, Training Loss: {cost.item()}, Validation Loss: {val_loss}')\n",
    "        \n",
    "        # if val_loss < best_val_loss:\n",
    "        #     best_val_loss = val_loss\n",
    "        #     torch.save(model.state_dict(), '../데이터/Checkpoint/best_rnn_transaction_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
