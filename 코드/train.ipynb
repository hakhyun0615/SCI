{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from Dataset.Economy_Dataset import Economy_Dataset\n",
    "from Dataset.RNN_Transaction_Dataset import RNN_Transaction_Dataset\n",
    "from Dataset.ODE_Transaction_Dataset import ODE_Transaction_Dataset\n",
    "\n",
    "from Model.LSTM import LSTM\n",
    "from Model.NODE import NODE\n",
    "from Model.ODERNN import *\n",
    "from Model.ODEF import *\n",
    "\n",
    "from utils import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 다운 & 전처리\n",
    "\n",
    "transaction_df = pd.read_csv('../데이터/transaction/transaction_final.csv')\n",
    "economy_df = pd.read_excel('../데이터/economy/economy_all.xlsx')\n",
    "\n",
    "transaction_df, economy_df = preprocess(transaction_df, economy_df, window_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5\n",
    "hidden_size = 256\n",
    "output_size = 1\n",
    "\n",
    "lr = 1e-7\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 부동산 & 경제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df = pd.read_excel('../데이터/Transaction/transaction_final.xlsx', index_col=0)\n",
    "economy_df = pd.read_excel('../데이터/Economy/economy_all.xlsx')\n",
    "economy_df = economy_df['국고채금리']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\RNN_Transaction_Dataset.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'])\n",
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\RNN_Transaction_Dataset.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['계약년월'] = pd.to_datetime(data['계약년월'])\n",
      "c:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\Economy_Dataset.py:11: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xe (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  self.economy_x = torch.FloatTensor(economy_x)\n"
     ]
    }
   ],
   "source": [
    "trainsaction_train_size = int(len(transaction_df)*0.7)\n",
    "trainsaction_val_size = int(len(transaction_df)*0.3)\n",
    "\n",
    "transaction_train_dataset = RNN_Transaction_Dataset(transaction_df[:trainsaction_train_size])\n",
    "transaction_train_loader = DataLoader(transaction_train_dataset, batch_size=2)\n",
    "transaction_val_dataset = RNN_Transaction_Dataset(transaction_df[trainsaction_train_size:])\n",
    "transaction_val_loader = DataLoader(transaction_val_dataset, batch_size=2)\n",
    "\n",
    "economy_train_size = int(len(economy_df)*0.7)\n",
    "economy_val_size = int(len(economy_df)*0.3)\n",
    "\n",
    "economy_train_dataset = Economy_Dataset(economy_df[:economy_train_size])\n",
    "economy_train_loader = DataLoader(economy_train_dataset, batch_size=2)\n",
    "economy_val_dataset = Economy_Dataset(economy_df[economy_train_size:])\n",
    "economy_val_loader = DataLoader(economy_val_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경제 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1, Training Loss: 0.002730900188907981, Validation Loss: 0.3143093644015106\n",
      "Epoch 1/1, Training Loss: 0.0027428490575402975, Validation Loss: 0.31638226120186774\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf') \n",
    "for epoch in range(num_epochs + 1):\n",
    "    model.train()\n",
    "    for batch_idx, samples in enumerate(economy_train_loader):\n",
    "        economy_x_train, economy_y_train = samples\n",
    "\n",
    "        prediction, hidden = model(economy_x_train)\n",
    "        cost = criterion(prediction, economy_y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, samples in enumerate(economy_val_loader):\n",
    "            economy_x_val, economy_y_val = samples\n",
    "\n",
    "            prediction, hidden = model(economy_x_val)\n",
    "            loss = criterion(prediction, economy_y_val)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(economy_val_loader)\n",
    "    print(f'Epoch {epoch}/{num_epochs}, Training Loss: {cost.item()}, Validation Loss: {val_loss}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), '../데이터/Checkpoint/best_rnn_economy_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 부동산 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1, Training Loss: 28219.5234375, Validation Loss: 5213844.961625429\n",
      "Epoch 1/1, Training Loss: 27884.662109375, Validation Loss: 5179137.586078938\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf') \n",
    "for epoch in range(num_epochs + 1):\n",
    "    model.train()\n",
    "    for batch_idx, samples in enumerate(transaction_train_loader):\n",
    "        dong_x_train, dong_y_train = samples\n",
    "\n",
    "        prediction, hidden = model(dong_x_train)\n",
    "        cost = criterion(prediction, dong_y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, samples in enumerate(transaction_val_loader):\n",
    "            x_val, y_val = samples\n",
    "\n",
    "            prediction, hidden = model(x_val)\n",
    "            loss = criterion(prediction, y_val)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(transaction_val_loader)\n",
    "    print(f'Epoch {epoch}/{num_epochs}, Training Loss: {cost.item()}, Validation Loss: {val_loss}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), '../데이터/Checkpoint/best_rnn_transaction_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NODE 부동산 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 다운로드\n",
    "batch_size = 2\n",
    "transaction_df = pd.read_excel('../데이터/Transaction/transaction_final.xlsx', index_col=0)\n",
    "\n",
    "train_dataset = ODE_Transaction_Dataset(transaction_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 크기 : torch.Size([2, 5])\n",
      "Y 크기 : torch.Size([2, 5])\n",
      "Z 크기 : torch.Size([2, 1])\n",
      "W 크기 : torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "for x,y,z,w in train_loader:\n",
    "      print(\"X 크기 : {}\".format(x.shape))\n",
    "      print(\"Y 크기 : {}\".format(y.shape))\n",
    "      print(\"Z 크기 : {}\".format(z.shape))\n",
    "      print(\"W 크기 : {}\".format(w.shape))\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 크기 : torch.float32\n",
      "Y 크기 : torch.float32\n",
      "Z 크기 : torch.float32\n",
      "W 크기 : torch.float32\n"
     ]
    }
   ],
   "source": [
    "for x,y,z,w in train_loader:\n",
    "      print(\"X 크기 : {}\".format(x.dtype))\n",
    "      print(\"Y 크기 : {}\".format(y.dtype))\n",
    "      print(\"Z 크기 : {}\".format(z.dtype))\n",
    "      print(\"W 크기 : {}\".format(w.dtype))\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu is available\n",
      "작동하는지 실험\n",
      "(tensor([[[0.3200],\n",
      "         [0.3020]],\n",
      "\n",
      "        [[0.3165],\n",
      "         [0.3053]],\n",
      "\n",
      "        [[0.3074],\n",
      "         [0.3091]],\n",
      "\n",
      "        [[0.3381],\n",
      "         [0.3102]],\n",
      "\n",
      "        [[0.3129],\n",
      "         [0.3069]]], grad_fn=<SliceBackward0>), tensor([[[-1.9246,  0.9916,  0.5816,  ..., -0.0953, -0.4212,  0.2555],\n",
      "         [-0.3911,  0.1918,  0.4691,  ..., -0.1469, -0.4416,  0.2056]],\n",
      "\n",
      "        [[-2.3361,  0.9280,  0.5030,  ..., -0.3589, -0.1182, -0.2516],\n",
      "         [-0.4929,  0.1927, -0.0292,  ...,  0.3396, -0.2586, -0.0658]],\n",
      "\n",
      "        [[-3.3343,  0.8663,  0.3372,  ..., -1.4191,  0.6670, -1.6950],\n",
      "         [-0.4960,  0.2321, -0.2474,  ...,  0.5994, -0.1838, -0.2525]],\n",
      "\n",
      "        [[ 0.8072,  1.3306,  1.7080,  ..., -0.8884, -1.7589,  2.7519],\n",
      "         [ 0.6183,  0.7583,  1.9381,  ..., -1.3055, -0.8485,  0.8296]],\n",
      "\n",
      "        [[-2.7293,  0.8898,  0.4417,  ..., -0.7159,  0.1923, -0.7784],\n",
      "         [-0.5078,  0.2105, -0.1448,  ...,  0.4671, -0.2156, -0.1514]],\n",
      "\n",
      "        [[ 1.2445,  1.3671,  1.8943,  ..., -1.1631, -1.9350,  3.0802],\n",
      "         [ 1.2932,  1.1343,  2.5548,  ..., -1.4962, -0.9968,  1.2692]]],\n",
      "       grad_fn=<ViewBackward0>), tensor([[ 3.9042e+00,  1.1460e+00, -8.7323e-01, -2.0277e-01, -1.4232e+00,\n",
      "         -9.7355e-01, -2.9226e-01,  1.7025e+00,  4.9917e-01,  3.9957e-02,\n",
      "         -4.1916e-01,  2.4280e-01, -1.5132e-02,  1.7395e-01, -2.0224e+00,\n",
      "          5.1864e-01,  1.5203e+00,  3.3595e+00, -8.0708e-01, -6.4386e-01,\n",
      "         -9.6330e-02,  8.3888e-01,  2.4769e-02, -2.2345e-01,  5.3007e-01,\n",
      "          1.2489e+00,  1.1800e+00,  3.9309e-01, -8.7714e-01,  1.6921e+00,\n",
      "          1.0720e+00,  5.2080e-01,  3.8115e-01, -1.2353e+00, -2.2096e-01,\n",
      "          4.0373e-01,  1.1288e+00, -3.7577e-01,  9.0160e-01,  1.1998e+00,\n",
      "          1.8166e+00, -1.0170e+00,  2.2276e-01, -6.2415e-01, -5.6234e-01,\n",
      "         -2.6533e-01,  5.2966e-01,  3.7777e-01, -7.3276e-02, -6.4162e-01,\n",
      "          4.7550e-01,  2.0280e+00, -3.7411e-01,  4.5963e-01,  4.7883e-01,\n",
      "         -5.3839e-01,  2.2202e+00, -6.2920e-02, -1.1106e-01, -8.5488e-01,\n",
      "         -3.6557e-01, -2.5014e+00,  5.9920e-01, -3.6857e-01],\n",
      "        [ 1.0762e-01, -3.4166e-01, -1.0522e+00,  1.2187e+00, -5.6238e-01,\n",
      "          1.9079e+00, -7.6549e-01,  5.2089e-01,  7.2618e-01, -4.3966e-01,\n",
      "         -1.1483e+00,  4.7966e-01,  5.7630e-01,  8.1643e-01, -1.3576e+00,\n",
      "         -5.7244e-01,  7.6411e-01,  2.0117e-02,  2.5561e-01,  1.0010e+00,\n",
      "         -4.1395e-01, -8.5190e-01,  1.2236e-01, -6.9191e-01,  6.0380e-01,\n",
      "         -6.9187e-01,  1.0257e+00,  1.0411e+00, -8.2717e-01, -1.0858e+00,\n",
      "         -1.0233e+00, -1.1872e+00, -1.3930e+00, -1.4010e-01, -8.0047e-01,\n",
      "          2.9662e-01,  1.9676e+00,  4.4584e-01,  1.9046e+00, -2.1059e-01,\n",
      "          1.5923e+00,  2.0074e-01, -1.1612e-01,  1.8093e-01,  1.2243e+00,\n",
      "          2.7559e-03,  1.3391e-01, -5.1903e-01, -1.5699e+00, -3.7198e-02,\n",
      "         -6.9918e-02,  3.8163e-01, -7.4425e-02, -7.5933e-01, -8.8816e-01,\n",
      "          1.7010e-01, -7.8576e-01,  1.8825e+00,  8.4531e-01, -8.4700e-01,\n",
      "         -1.5939e+00, -3.7945e-01, -5.2958e-01, -7.4874e-01]],\n",
      "       grad_fn=<AddBackward0>), tensor([[-2.7978e-02,  2.1564e-02, -1.9743e-02, -3.8898e-03,  3.6597e-02,\n",
      "         -5.5066e-02,  4.0261e-02,  4.9608e-02, -2.5658e-03,  9.2387e-02,\n",
      "         -6.3146e-02, -2.8552e-02,  3.3216e-02,  8.2328e-02, -6.9201e-03,\n",
      "         -3.6288e-02,  3.0480e-02, -9.8560e-03,  3.6546e-02,  2.4284e-02,\n",
      "         -4.0156e-02,  7.7440e-03, -2.4673e-02,  1.6008e-02,  3.8246e-02,\n",
      "          1.9721e-02,  7.4606e-02,  6.3284e-02, -3.7887e-03,  5.4471e-02,\n",
      "         -4.2920e-02, -3.5930e-02,  6.2402e-02,  7.3542e-02, -4.3362e-02,\n",
      "         -9.6650e-03, -9.0047e-02,  7.8424e-02,  1.5734e-03, -2.0391e-03,\n",
      "          6.8380e-02, -4.2523e-03, -5.9703e-02,  7.0611e-02,  6.4688e-02,\n",
      "         -6.2975e-02,  3.0866e-02, -2.6243e-02,  3.6632e-03, -4.1667e-02,\n",
      "          2.1046e-02, -3.1662e-02,  1.5487e-03, -1.8984e-02, -2.9312e-02,\n",
      "         -1.0388e-01,  2.0708e-02, -1.3626e-02, -4.2216e-02, -6.5238e-02,\n",
      "          5.3133e-02,  7.1187e-02,  5.7235e-02,  4.6491e-02],\n",
      "        [-1.9571e-02,  4.3879e-02, -5.0783e-03, -9.8894e-03,  3.7486e-02,\n",
      "         -6.2992e-02,  5.2318e-02,  3.7628e-02, -1.7702e-02,  8.1281e-02,\n",
      "         -4.3972e-02, -1.0214e-02,  5.3238e-02,  8.3248e-02, -2.6203e-04,\n",
      "         -3.3267e-02,  2.7521e-02, -8.1333e-04,  2.3006e-02,  3.6827e-02,\n",
      "         -4.1841e-02,  1.3795e-02, -4.0217e-02,  5.5267e-03,  1.2294e-02,\n",
      "          4.9167e-05,  6.8872e-02,  5.1969e-02,  5.1642e-03,  4.4822e-02,\n",
      "         -3.3580e-02, -4.7620e-02,  7.9498e-02,  4.5254e-02, -4.0060e-02,\n",
      "          6.4915e-04, -6.5277e-02,  6.5421e-02, -1.5464e-02, -3.0794e-02,\n",
      "          8.9416e-02,  3.7358e-03, -6.7441e-02,  7.1974e-02,  5.4571e-02,\n",
      "         -5.3111e-02,  3.0609e-02, -2.8819e-02, -1.4598e-02, -4.4193e-02,\n",
      "          3.2197e-02, -5.3195e-02, -7.4973e-03, -1.8159e-02, -1.2066e-02,\n",
      "         -1.0746e-01,  3.8273e-02, -3.0866e-03, -3.4713e-02, -1.0123e-01,\n",
      "          4.4599e-02,  7.0794e-02,  5.9186e-02,  6.3070e-02]],\n",
      "       grad_fn=<SliceBackward0>), tensor([[ 0.0155, -0.0075, -0.0300, -0.0190, -0.0339, -0.0477, -0.0162, -0.0334,\n",
      "         -0.0602, -0.0859,  0.0301,  0.0979, -0.0266,  0.0036,  0.0496,  0.0510,\n",
      "          0.0025, -0.0809, -0.0337,  0.0760,  0.0043,  0.0196,  0.0543, -0.0725,\n",
      "         -0.0324,  0.0053,  0.0032,  0.0442, -0.0300,  0.0521, -0.0945,  0.0549,\n",
      "          0.0717, -0.1073, -0.0669,  0.0128, -0.0119,  0.0157,  0.0189,  0.0650,\n",
      "          0.0086, -0.0841,  0.0187,  0.0170,  0.0094, -0.0108, -0.0065,  0.0556,\n",
      "          0.0479,  0.0164, -0.0572, -0.0344, -0.0694,  0.0377, -0.0301, -0.0064,\n",
      "          0.1002, -0.0484,  0.0019, -0.0736, -0.0714,  0.0944, -0.0039, -0.0209],\n",
      "        [ 0.0579, -0.0116, -0.0288, -0.0306, -0.0371, -0.0444, -0.0290, -0.0292,\n",
      "         -0.0747, -0.0663,  0.0496,  0.0840, -0.0257,  0.0075,  0.0487,  0.0475,\n",
      "          0.0025, -0.0878, -0.0380,  0.0758,  0.0044,  0.0135,  0.0488, -0.0813,\n",
      "         -0.0364,  0.0076, -0.0205,  0.0302, -0.0115,  0.0511, -0.0846,  0.0607,\n",
      "          0.0711, -0.0939, -0.0811, -0.0033, -0.0092,  0.0347,  0.0322,  0.0580,\n",
      "         -0.0061, -0.0910, -0.0055,  0.0147,  0.0043, -0.0052, -0.0028,  0.0524,\n",
      "          0.0341,  0.0373, -0.0631, -0.0340, -0.0806,  0.0498, -0.0214, -0.0112,\n",
      "          0.1168, -0.0336, -0.0097, -0.0747, -0.0705,  0.1314,  0.0226, -0.0298]],\n",
      "       grad_fn=<SliceBackward0>), tensor([[0.3404],\n",
      "        [0.3172]], grad_fn=<SelectBackward0>))\n",
      "torch.Size([5, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# 데이터 & 모델에 device 붙임!!!\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # 윈도우 gpu\n",
    "# device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu') # 맥 gpu\n",
    "print(f'{device} is available')\n",
    "\n",
    "model = NODE(output_dim=1,hidden_dim=256,latent_dim=64).to(device)\n",
    "\n",
    "print('작동하는지 실험')\n",
    "basic_data = torch.rand((5,2,1)).to(device)  # window_size, batch_size, 1\n",
    "time = torch.FloatTensor([[1,2,3,6,10,12],[1,3,5,8,10,12]]).reshape(6,2,1).to(device) # window_size, batch_size, 1\n",
    "data = model(basic_data,time)\n",
    "print(data)\n",
    "print(data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "noise_std = 0.02\n",
    "optim = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), lr=0.001)\n",
    "\n",
    "num_epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0\n",
      "loss : 11219012608.0, best loss : 11219012608.0\n",
      "loss : 13275686912.0, best loss : 11219012608.0\n",
      "loss : 13226204160.0, best loss : 11219012608.0\n",
      "loss : 11485132800.0, best loss : 11219012608.0\n",
      "loss : 8940823552.0, best loss : 8940823552.0\n",
      "loss : 7942053888.0, best loss : 7942053888.0\n",
      "loss : 8093862912.0, best loss : 7942053888.0\n",
      "loss : 9578081280.0, best loss : 7942053888.0\n",
      "loss : 11440328704.0, best loss : 7942053888.0\n",
      "loss : 13386883072.0, best loss : 7942053888.0\n",
      "loss : 15193017344.0, best loss : 7942053888.0\n",
      "loss : 1.5491499810317402e+19, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n",
      "loss : nan, best loss : 7942053888.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kimhakhyun/Desktop/Git/sci/SCI/코드/train.ipynb 셀 17\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimhakhyun/Desktop/Git/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m window_size\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimhakhyun/Desktop/Git/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()   \u001b[39m############### 여기서 second 오류가 나는지 확인!!!!\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kimhakhyun/Desktop/Git/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimhakhyun/Desktop/Git/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X22sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m optim\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimhakhyun/Desktop/Git/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X22sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/function.py:274\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    273\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 274\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/Desktop/Git/sci/SCI/코드/Model/ODEF.py:124\u001b[0m, in \u001b[0;36mODEAdjoint.backward\u001b[0;34m(ctx, dLdz)\u001b[0m\n\u001b[1;32m    120\u001b[0m adj_t[i_t] \u001b[39m=\u001b[39m adj_t[i_t] \u001b[39m-\u001b[39m dLdt_i\n\u001b[1;32m    122\u001b[0m aug_z \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((z_i\u001b[39m.\u001b[39mview(bs, n_dim), adj_z, torch\u001b[39m.\u001b[39mzeros(bs, n_params)\u001b[39m.\u001b[39mto(z), adj_t[i_t]), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 124\u001b[0m aug_ans \u001b[39m=\u001b[39m ode_solve(aug_z, t_i, t[i_t\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], augmented_dynamics)\n\u001b[1;32m    126\u001b[0m adj_z[:] \u001b[39m=\u001b[39m aug_ans[:, n_dim:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mn_dim]\n\u001b[1;32m    127\u001b[0m adj_p[:] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m aug_ans[:, \u001b[39m2\u001b[39m\u001b[39m*\u001b[39mn_dim:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mn_dim \u001b[39m+\u001b[39m n_params]\n",
      "File \u001b[0;32m~/Desktop/Git/sci/SCI/코드/Model/ODEF.py:28\u001b[0m, in \u001b[0;36mode_solve\u001b[0;34m(z0, t0, t1, f)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m i_step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_steps):\n\u001b[1;32m     27\u001b[0m     z \u001b[39m=\u001b[39m z \u001b[39m+\u001b[39m h \u001b[39m*\u001b[39m f(z, t)\n\u001b[0;32m---> 28\u001b[0m     t \u001b[39m=\u001b[39m t \u001b[39m+\u001b[39m h\n\u001b[1;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m z\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_train_loss = float('inf') \n",
    "for epoch in range(num_epochs + 1):\n",
    "    print('epoch : ',epoch)\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for batch_idx, samples in enumerate(train_loader):\n",
    "        tran_x, time_x, tran_y, time_y = samples\n",
    "        \n",
    "        t = torch.cat((time_x,time_y),dim=1)  \n",
    "        tran_x = tran_x.transpose(0,1).unsqueeze(2).to(device)\n",
    "        t = t.transpose(0,1).unsqueeze(2).to(device)\n",
    "        \n",
    "        x_p, _, z, z_mean, z_log_var, pred = model(tran_x, t)\n",
    "        kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean**2 - torch.exp(z_log_var), -1)\n",
    "        loss = 0.5 * ((tran_x-x_p)**2).sum(-1).sum(0) / noise_std**2 + kl_loss\n",
    "        loss = torch.mean(loss)\n",
    "        loss /= window_size\n",
    "        optim.zero_grad()   ############### 여기서 second 오류가 나는지 확인!!!!\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if loss < best_train_loss:\n",
    "            best_train_loss = loss\n",
    "            torch.save(model.state_dict(), \"../데이터/checkpoint/best_ODE_transaction_model.pth\")\n",
    "        \n",
    "        print('loss : {}, best loss : {}'.format(loss, best_train_loss))\n",
    "    print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ODERNN 부동산 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "train_dataset = ODE_Transaction_Dataset(transaction_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1264\n"
     ]
    }
   ],
   "source": [
    "# 인덱스 번호가 안나와야 됨!!!\n",
    "\n",
    "for i, (x,y,z,w) in enumerate(train_loader):\n",
    "    time_f = torch.cat([y,w],axis=1)\n",
    "    if (time_f[0][1:]>time_f[0][:-1]).all() == False:\n",
    "        print(i)\n",
    "        break\n",
    "    \n",
    "print(y)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 크기 : torch.Size([1, 5])\n",
      "Y 크기 : torch.Size([1, 5])\n",
      "Z 크기 : torch.Size([1, 1])\n",
      "W 크기 : torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "for x,y,z,w in train_loader:\n",
    "      print(\"X 크기 : {}\".format(x.shape))\n",
    "      print(\"Y 크기 : {}\".format(y.shape))\n",
    "      print(\"Z 크기 : {}\".format(z.shape))\n",
    "      print(\"W 크기 : {}\".format(w.shape))\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu is available\n",
      "작동하는지 실험\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "t must be strictly increasing or decreasing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hkyoo\\OneDrive\\바탕 화면\\SCI\\코드\\train.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hkyoo/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/SCI/%EC%BD%94%EB%93%9C/train.ipynb#Y156sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor([[\u001b[39m1.\u001b[39m,\u001b[39m2.\u001b[39m,\u001b[39m3.\u001b[39m,\u001b[39m5.\u001b[39m,\u001b[39m9.\u001b[39m,\u001b[39m9.\u001b[39m]])\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hkyoo/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/SCI/%EC%BD%94%EB%93%9C/train.ipynb#Y156sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor([[\u001b[39m123\u001b[39m,\u001b[39m126\u001b[39m,\u001b[39m266\u001b[39m,\u001b[39m279\u001b[39m,\u001b[39m300\u001b[39m]])\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hkyoo/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/SCI/%EC%BD%94%EB%93%9C/train.ipynb#Y156sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m out \u001b[39m=\u001b[39m model(data,t)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hkyoo/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/SCI/%EC%BD%94%EB%93%9C/train.ipynb#Y156sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(out[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\hkyoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hkyoo\\OneDrive\\바탕 화면\\SCI\\코드\\Model\\ODERNN.py:59\u001b[0m, in \u001b[0;36mODE_RNN.forward\u001b[1;34m(self, data, t)\u001b[0m\n\u001b[0;32m     56\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn_cell(data[\u001b[39m0\u001b[39m,i]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m),hp)\n\u001b[0;32m     58\u001b[0m \u001b[39m# 마지막 \u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m hp \u001b[39m=\u001b[39m odeint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mode_func, h, t[\u001b[39m0\u001b[39;49m, t\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m]\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m:t\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m]], rtol \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrtol, atol \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49matol)[\u001b[39m1\u001b[39m]  \u001b[39m####### h->hp\u001b[39;00m\n\u001b[0;32m     60\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_hidden(hp)\n\u001b[0;32m     61\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_output(out)\n",
      "File \u001b[1;32mc:\\Users\\hkyoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchdiffeq\\_impl\\adjoint.py:192\u001b[0m, in \u001b[0;36modeint_adjoint\u001b[1;34m(func, y0, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol, adjoint_method, adjoint_options, adjoint_params)\u001b[0m\n\u001b[0;32m    188\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mAn adjoint parameter was passed without requiring gradient. For efficiency this will be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mexcluded from the adjoint pass, and will not appear as a tensor in the adjoint norm.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    191\u001b[0m \u001b[39m# Convert to flattened state.\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m shapes, func, y0, t, rtol, atol, method, options, event_fn, decreasing_time \u001b[39m=\u001b[39m _check_inputs(func, y0, t, rtol, atol, method, options, event_fn, SOLVERS)\n\u001b[0;32m    194\u001b[0m \u001b[39m# Handle the adjoint norm function.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m state_norm \u001b[39m=\u001b[39m options[\u001b[39m\"\u001b[39m\u001b[39mnorm\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\hkyoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchdiffeq\\_impl\\misc.py:286\u001b[0m, in \u001b[0;36m_check_inputs\u001b[1;34m(func, y0, t, rtol, atol, method, options, event_fn, SOLVERS)\u001b[0m\n\u001b[0;32m    283\u001b[0m     _flip_option(options, \u001b[39m'\u001b[39m\u001b[39mjump_t\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    285\u001b[0m \u001b[39m# Can only do after having normalised time\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m _assert_increasing(\u001b[39m'\u001b[39;49m\u001b[39mt\u001b[39;49m\u001b[39m'\u001b[39;49m, t)\n\u001b[0;32m    288\u001b[0m \u001b[39m# Tol checking\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_tensor(rtol):\n",
      "File \u001b[1;32mc:\\Users\\hkyoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchdiffeq\\_impl\\misc.py:101\u001b[0m, in \u001b[0;36m_assert_increasing\u001b[1;34m(name, t)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assert_increasing\u001b[39m(name, t):\n\u001b[1;32m--> 101\u001b[0m     \u001b[39massert\u001b[39;00m (t[\u001b[39m1\u001b[39m:] \u001b[39m>\u001b[39m t[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mall(), \u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m must be strictly increasing or decreasing\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(name)\n",
      "\u001b[1;31mAssertionError\u001b[0m: t must be strictly increasing or decreasing"
     ]
    }
   ],
   "source": [
    "# 데이터 & 모델에 device 붙임!!!\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device} is available')\n",
    "\n",
    "model = ODE_RNN(256,64,1,200,200,device).to(device)\n",
    "\n",
    "print('작동하는지 실험')\n",
    "t = torch.FloatTensor([[1.,2.,3.,5.,9.,9.]]).to(device)\n",
    "data = torch.FloatTensor([[123,126,266,279,300]]).to(device)\n",
    "\n",
    "out = model(data,t)\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "t must be strictly increasing or decreasing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\train.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m tran_x, time_x, tran_y, time_y \u001b[39m=\u001b[39m tran_x\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m), time_x\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m), tran_y\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m), time_y\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m time_f \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([time_x,time_y],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m prediction, hidden \u001b[39m=\u001b[39m model(tran_x,time_f)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X33sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m cost \u001b[39m=\u001b[39m criterion(prediction[:,:\u001b[39m5\u001b[39m], tran_x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/sci/SCI/%EC%BD%94%EB%93%9C/train.ipynb#X33sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Model\\ODERNN.py:62\u001b[0m, in \u001b[0;36mODE_RNN.forward\u001b[1;34m(self, data, t)\u001b[0m\n\u001b[0;32m     60\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_hidden(hp)\n\u001b[0;32m     61\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_output(out)\n\u001b[1;32m---> 62\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_tanh(out)\n\u001b[0;32m     64\u001b[0m \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m data\u001b[39m.\u001b[39mshape:\n\u001b[0;32m     65\u001b[0m     output[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchdiffeq\\_impl\\adjoint.py:192\u001b[0m, in \u001b[0;36modeint_adjoint\u001b[1;34m(func, y0, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol, adjoint_method, adjoint_options, adjoint_params)\u001b[0m\n\u001b[0;32m    188\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mAn adjoint parameter was passed without requiring gradient. For efficiency this will be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mexcluded from the adjoint pass, and will not appear as a tensor in the adjoint norm.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    191\u001b[0m \u001b[39m# Convert to flattened state.\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m shapes, func, y0, t, rtol, atol, method, options, event_fn, decreasing_time \u001b[39m=\u001b[39m _check_inputs(func, y0, t, rtol, atol, method, options, event_fn, SOLVERS)\n\u001b[0;32m    194\u001b[0m \u001b[39m# Handle the adjoint norm function.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m state_norm \u001b[39m=\u001b[39m options[\u001b[39m\"\u001b[39m\u001b[39mnorm\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchdiffeq\\_impl\\misc.py:286\u001b[0m, in \u001b[0;36m_check_inputs\u001b[1;34m(func, y0, t, rtol, atol, method, options, event_fn, SOLVERS)\u001b[0m\n\u001b[0;32m    283\u001b[0m     _flip_option(options, \u001b[39m'\u001b[39m\u001b[39mjump_t\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    285\u001b[0m \u001b[39m# Can only do after having normalised time\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m _assert_increasing(\u001b[39m'\u001b[39;49m\u001b[39mt\u001b[39;49m\u001b[39m'\u001b[39;49m, t)\n\u001b[0;32m    288\u001b[0m \u001b[39m# Tol checking\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_tensor(rtol):\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchdiffeq\\_impl\\misc.py:101\u001b[0m, in \u001b[0;36m_assert_increasing\u001b[1;34m(name, t)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assert_increasing\u001b[39m(name, t):\n\u001b[1;32m--> 101\u001b[0m     \u001b[39massert\u001b[39;00m (t[\u001b[39m1\u001b[39m:] \u001b[39m>\u001b[39m t[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mall(), \u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m must be strictly increasing or decreasing\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(name)\n",
      "\u001b[1;31mAssertionError\u001b[0m: t must be strictly increasing or decreasing"
     ]
    }
   ],
   "source": [
    "best_train_loss = float('inf') \n",
    "for epoch in range(num_epochs + 1):\n",
    "    print('epoch : ',epoch)\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for batch_idx, samples in enumerate(train_loader):\n",
    "        tran_x, time_x, tran_y, time_y = samples\n",
    "        tran_x, time_x, tran_y, time_y = tran_x.to('cuda:0'), time_x.to('cuda:0'), tran_y.to('cuda:0'), time_y.to('cuda:0')\n",
    "        time_f = torch.cat([time_x,time_y],axis=1)\n",
    "        prediction, hidden = model(tran_x,time_f)\n",
    "        cost = criterion(prediction[:,:5], tran_x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch}/{num_epochs}, Training Loss: {cost.item()}')\n",
    "    \n",
    "    # model.eval()\n",
    "    # val_loss = 0.0\n",
    "    # with torch.no_grad():\n",
    "    #     for batch_idx, samples in enumerate(transaction_val_loader):\n",
    "    #         x_val, y_val = samples\n",
    "\n",
    "    #         prediction, hidden = model(x_val)\n",
    "    #         loss = criterion(prediction, y_val)\n",
    "    #         val_loss += loss.item()\n",
    "\n",
    "    # val_loss /= len(transaction_val_loader)\n",
    "    # print(f'Epoch {epoch}/{num_epochs}, Training Loss: {cost.item()}, Validation Loss: {val_loss}')\n",
    "    \n",
    "    # if val_loss < best_val_loss:\n",
    "    #     best_val_loss = val_loss\n",
    "    #     torch.save(model.state_dict(), '../데이터/Checkpoint/best_rnn_transaction_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ODERNN 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
