{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "from Dataset.Embedding_Dataset import Embedding_Dataset\n",
    "from Model.Embedding import Embedding\n",
    "\n",
    "from Dataset.LSTM_Dataset import LSTM_Dataset\n",
    "from Model.LSTM import LSTM\n",
    "from Model.NLinear import NLinear\n",
    "from Model.Transformer import Transformer\n",
    "\n",
    "from Dataset.Attention_Dataset import Attention_Dataset\n",
    "from Model.Attention import LSTMSeq2Seq\n",
    "\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = torch.device('cpu') # CPU\n",
    "# DEVICE = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu') # MAC\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # WINDOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "connection_info = \"host=localhost dbname=postgres user=postgres password=hd219833 port=5432\"\n",
    "conn = psycopg2.connect(connection_info)\n",
    "table_1_query = '''\n",
    "    SELECT * FROM building\n",
    "    '''\n",
    "table_2_query = '''\n",
    "    SELECT * FROM economy\n",
    "    '''\n",
    "table_3_query = '''\n",
    "    SELECT * FROM building_price\n",
    "    '''\n",
    "table_1 = pd.read_sql(table_1_query,conn) \n",
    "table_2 = pd.read_sql(table_2_query,conn)\n",
    "table_3 = pd.read_sql(table_3_query,conn) \n",
    "\n",
    "# table_1 = pd.read_csv('../데이터/Table/table_1.csv') \n",
    "# table_2 = pd.read_csv('../데이터/Table/table_2.csv') \n",
    "# table_3 = pd.read_csv('../데이터/Table/table_3.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSE,self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = 1e-7\n",
    "\n",
    "    def forward(self, y, y_hat):\n",
    "        return torch.sqrt(self.mse(y, y_hat) + self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val_losses(train_losses, val_losses):\n",
    "    print(f'Min Validation Loss: {min(val_losses)}')\n",
    "    plt.plot(train_losses[1:], label='Training Loss')\n",
    "    plt.plot(val_losses[1:], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val loss가 연속적으로 오를 때\n",
    "def early_stop_1(val_losses, consecutive_val_loss_increases, max_consecutive_val_loss_increases):\n",
    "    if len(val_losses) > 1 and val_losses[-1] > val_losses[-2]:\n",
    "        consecutive_val_loss_increases += 1\n",
    "        if consecutive_val_loss_increases >= max_consecutive_val_loss_increases:\n",
    "            return True, consecutive_val_loss_increases\n",
    "        else:\n",
    "            return False, consecutive_val_loss_increases\n",
    "    else:\n",
    "        consecutive_val_loss_increases = 0\n",
    "        return False, consecutive_val_loss_increases\n",
    "\n",
    "# val loss가 최저 loss보다 연속적으로 클 때\n",
    "def early_stop_2(avg_val_loss, best_val_loss, consecutive_val_loss_increases, max_consecutive_val_loss_increases):\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        consecutive_val_loss_increases = 0\n",
    "    else:\n",
    "        consecutive_val_loss_increases += 1\n",
    "    if consecutive_val_loss_increases >= max_consecutive_val_loss_increases:\n",
    "        return True, best_val_loss, consecutive_val_loss_increases\n",
    "    else:\n",
    "        return False, best_val_loss, consecutive_val_loss_increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "\n",
    "embedding_lr = 0.00001\n",
    "embedding_weight_decay = 0\n",
    "embedding_batch = 128\n",
    "embedding_epochs = 150\n",
    "encoder_dim_1 = 128\n",
    "encoder_dim_2 = 256\n",
    "encoder_dim_3 = 512\n",
    "embedding_dim = 1024\n",
    "decoder_dim_1 = 512\n",
    "decoder_dim_2 = 256\n",
    "decoder_dim_3 = 128\n",
    "\n",
    "lstm_lr = 0.0001\n",
    "lstm_weight_decay = 0\n",
    "lstm_batch = 128\n",
    "lstm_epochs = 150\n",
    "lstm_hidden_dim = 256\n",
    "lstm_window_size = 10\n",
    "\n",
    "nlinear_lr = 0.0001\n",
    "nlinear_weight_decay = 0\n",
    "nlinear_batch = 128\n",
    "nlinear_epochs = 150\n",
    "nlinear_window_size = 10\n",
    "\n",
    "attention_lr = 0.0001\n",
    "attention_weight_decay = 0\n",
    "attention_batch = 1\n",
    "attention_epochs = 150\n",
    "attention_hidden_dim = 256\n",
    "attention_window_size = 10\n",
    "\n",
    "transformer_lr = 0.0001\n",
    "transformer_weight_decay = 0\n",
    "transformer_batch = 1\n",
    "transformer_epochs = 150\n",
    "transformer_window_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Embedding_Dataset(table_1, table_2, table_3)\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Train Loss: 6.5733, Val Loss: 6.5547\n",
      "Epoch [2/150], Train Loss: 6.1573, Val Loss: 6.1832\n",
      "Epoch [3/150], Train Loss: 5.8074, Val Loss: 5.8475\n",
      "Epoch [4/150], Train Loss: 5.5186, Val Loss: 5.6872\n",
      "Epoch [5/150], Train Loss: 5.2767, Val Loss: 5.6049\n",
      "Epoch [6/150], Train Loss: 5.0581, Val Loss: 5.5336\n",
      "Epoch [7/150], Train Loss: 4.8496, Val Loss: 5.4536\n",
      "Epoch [8/150], Train Loss: 4.6445, Val Loss: 5.3707\n",
      "Epoch [9/150], Train Loss: 4.4425, Val Loss: 5.2748\n",
      "Epoch [10/150], Train Loss: 4.2416, Val Loss: 5.1627\n",
      "Epoch [11/150], Train Loss: 4.0387, Val Loss: 5.0454\n",
      "Epoch [12/150], Train Loss: 3.8370, Val Loss: 4.9116\n",
      "Epoch [13/150], Train Loss: 3.6349, Val Loss: 4.7740\n",
      "Epoch [14/150], Train Loss: 3.4363, Val Loss: 4.6406\n",
      "Epoch [15/150], Train Loss: 3.2418, Val Loss: 4.5186\n",
      "Epoch [16/150], Train Loss: 3.0546, Val Loss: 4.4094\n",
      "Epoch [17/150], Train Loss: 2.8873, Val Loss: 4.3045\n",
      "Epoch [18/150], Train Loss: 2.7439, Val Loss: 4.2002\n",
      "Epoch [19/150], Train Loss: 2.6217, Val Loss: 4.1211\n",
      "Epoch [20/150], Train Loss: 2.5157, Val Loss: 4.0248\n",
      "Epoch [21/150], Train Loss: 2.4257, Val Loss: 3.9718\n",
      "Epoch [22/150], Train Loss: 2.3513, Val Loss: 3.9114\n",
      "Epoch [23/150], Train Loss: 2.2938, Val Loss: 3.8864\n",
      "Epoch [24/150], Train Loss: 2.2485, Val Loss: 3.7833\n",
      "Epoch [25/150], Train Loss: 2.2109, Val Loss: 3.8395\n",
      "Epoch [26/150], Train Loss: 2.1828, Val Loss: 3.6997\n",
      "Epoch [27/150], Train Loss: 2.1580, Val Loss: 3.7280\n",
      "Epoch [28/150], Train Loss: 2.1346, Val Loss: 3.6867\n",
      "Epoch [29/150], Train Loss: 2.1136, Val Loss: 3.6561\n",
      "Epoch [30/150], Train Loss: 2.0936, Val Loss: 3.6863\n",
      "Epoch [31/150], Train Loss: 2.0739, Val Loss: 3.5877\n",
      "Epoch [32/150], Train Loss: 2.0562, Val Loss: 3.6789\n",
      "Epoch [33/150], Train Loss: 2.0401, Val Loss: 3.5694\n",
      "Epoch [34/150], Train Loss: 2.0246, Val Loss: 3.6942\n",
      "Epoch [35/150], Train Loss: 2.0097, Val Loss: 3.5431\n",
      "Epoch [36/150], Train Loss: 1.9965, Val Loss: 3.6627\n",
      "Epoch [37/150], Train Loss: 1.9827, Val Loss: 3.5629\n",
      "Epoch [38/150], Train Loss: 1.9717, Val Loss: 3.5894\n",
      "Epoch [39/150], Train Loss: 1.9579, Val Loss: 3.6078\n",
      "Epoch [40/150], Train Loss: 1.9445, Val Loss: 3.5714\n",
      "Epoch [41/150], Train Loss: 1.9322, Val Loss: 3.5980\n",
      "Epoch [42/150], Train Loss: 1.9186, Val Loss: 3.5624\n",
      "Epoch [43/150], Train Loss: 1.9063, Val Loss: 3.6057\n",
      "Epoch [44/150], Train Loss: 1.8932, Val Loss: 3.5478\n",
      "Epoch [45/150], Train Loss: 1.8813, Val Loss: 3.5810\n",
      "Epoch [46/150], Train Loss: 1.8675, Val Loss: 3.5805\n",
      "Epoch [47/150], Train Loss: 1.8561, Val Loss: 3.5435\n",
      "Epoch [48/150], Train Loss: 1.8414, Val Loss: 3.6040\n",
      "Epoch [49/150], Train Loss: 1.8300, Val Loss: 3.5908\n",
      "Epoch [50/150], Train Loss: 1.8161, Val Loss: 3.5871\n",
      "Epoch [51/150], Train Loss: 1.8040, Val Loss: 3.5720\n",
      "Epoch [52/150], Train Loss: 1.7895, Val Loss: 3.5916\n",
      "Epoch [53/150], Train Loss: 1.7769, Val Loss: 3.5593\n",
      "Epoch [54/150], Train Loss: 1.7643, Val Loss: 3.5777\n",
      "Epoch [55/150], Train Loss: 1.7511, Val Loss: 3.5269\n",
      "Epoch [56/150], Train Loss: 1.7369, Val Loss: 3.5526\n",
      "Epoch [57/150], Train Loss: 1.7229, Val Loss: 3.5425\n",
      "Epoch [58/150], Train Loss: 1.7102, Val Loss: 3.4873\n",
      "Epoch [59/150], Train Loss: 1.6982, Val Loss: 3.5583\n",
      "Epoch [60/150], Train Loss: 1.6881, Val Loss: 3.4128\n",
      "Epoch [61/150], Train Loss: 1.6749, Val Loss: 3.5258\n",
      "Epoch [62/150], Train Loss: 1.6633, Val Loss: 3.4029\n",
      "Epoch [63/150], Train Loss: 1.6476, Val Loss: 3.4853\n",
      "Epoch [64/150], Train Loss: 1.6353, Val Loss: 3.4031\n",
      "Epoch [65/150], Train Loss: 1.6210, Val Loss: 3.4350\n",
      "Epoch [66/150], Train Loss: 1.6094, Val Loss: 3.4376\n",
      "Epoch [67/150], Train Loss: 1.5964, Val Loss: 3.3519\n",
      "Epoch [68/150], Train Loss: 1.5837, Val Loss: 3.4265\n",
      "Epoch [69/150], Train Loss: 1.5701, Val Loss: 3.3259\n",
      "Epoch [70/150], Train Loss: 1.5575, Val Loss: 3.3832\n",
      "Epoch [71/150], Train Loss: 1.5437, Val Loss: 3.3012\n",
      "Epoch [72/150], Train Loss: 1.5305, Val Loss: 3.3614\n",
      "Epoch [73/150], Train Loss: 1.5176, Val Loss: 3.2931\n",
      "Epoch [74/150], Train Loss: 1.5049, Val Loss: 3.3191\n",
      "Epoch [75/150], Train Loss: 1.4913, Val Loss: 3.2880\n",
      "Epoch [76/150], Train Loss: 1.4807, Val Loss: 3.3012\n",
      "Epoch [77/150], Train Loss: 1.4677, Val Loss: 3.2498\n",
      "Epoch [78/150], Train Loss: 1.4577, Val Loss: 3.3068\n",
      "Epoch [79/150], Train Loss: 1.4405, Val Loss: 3.2328\n",
      "Epoch [80/150], Train Loss: 1.4300, Val Loss: 3.2560\n",
      "Epoch [81/150], Train Loss: 1.4181, Val Loss: 3.2795\n",
      "Epoch [82/150], Train Loss: 1.4065, Val Loss: 3.2061\n",
      "Epoch [83/150], Train Loss: 1.3941, Val Loss: 3.2405\n",
      "Epoch [84/150], Train Loss: 1.3819, Val Loss: 3.1835\n",
      "Epoch [85/150], Train Loss: 1.3715, Val Loss: 3.2602\n",
      "Epoch [86/150], Train Loss: 1.3617, Val Loss: 3.2389\n",
      "Epoch [87/150], Train Loss: 1.3544, Val Loss: 3.2199\n",
      "Epoch [88/150], Train Loss: 1.3380, Val Loss: 3.2545\n",
      "Epoch [89/150], Train Loss: 1.3296, Val Loss: 3.2561\n",
      "Epoch [90/150], Train Loss: 1.3130, Val Loss: 3.2447\n",
      "Epoch [91/150], Train Loss: 1.3094, Val Loss: 3.2503\n",
      "Epoch [92/150], Train Loss: 1.2953, Val Loss: 3.2624\n",
      "Epoch [93/150], Train Loss: 1.2867, Val Loss: 3.2316\n",
      "Epoch [94/150], Train Loss: 1.2756, Val Loss: 3.1460\n",
      "Epoch [95/150], Train Loss: 1.2672, Val Loss: 3.1541\n",
      "Epoch [96/150], Train Loss: 1.2618, Val Loss: 3.2368\n",
      "Epoch [97/150], Train Loss: 1.2555, Val Loss: 3.3074 \n",
      "Early Stop Triggered!\n"
     ]
    }
   ],
   "source": [
    "model = Embedding(encoder_dim_1, encoder_dim_2, encoder_dim_3, embedding_dim, decoder_dim_1, decoder_dim_2, decoder_dim_3).to(DEVICE)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=embedding_lr, weight_decay=embedding_weight_decay)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "consecutive_val_loss_increases = 0\n",
    "max_consecutive_val_loss_increases = 3\n",
    "\n",
    "for epoch in range(embedding_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        input = data[0].to(DEVICE)\n",
    "        target = data[1].to(DEVICE)\n",
    "        output = model(input).to(DEVICE)\n",
    "\n",
    "        train_loss = criterion(output, target)\n",
    "        total_train_loss += train_loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            input = data[0].to(DEVICE)\n",
    "            target = data[1].to(DEVICE)\n",
    "            output = model(input).to(DEVICE)\n",
    "\n",
    "            val_loss = criterion(output, target)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    early_stop, consecutive_val_loss_increases = early_stop_1(val_losses, consecutive_val_loss_increases, max_consecutive_val_loss_increases)\n",
    "    # early_stop, best_val_loss, consecutive_val_loss_increases = early_stop_2(avg_val_loss, best_val_loss, consecutive_val_loss_increases, max_consecutive_val_loss_increases)\n",
    "    if early_stop:\n",
    "        print(f'Epoch [{epoch+1}/{embedding_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f} \\nEarly Stop Triggered!')\n",
    "        torch.save(model, f'../데이터/Checkpoint/embedding_tr_{train_ratio}_lr_{embedding_lr}_wd_{embedding_weight_decay}_batch_{embedding_batch}_epochs_{epoch+1}_e1_{encoder_dim_1}_e2_{encoder_dim_1}_e3_{encoder_dim_3}_emb_{embedding_dim}_d1{decoder_dim_1}_d2_{decoder_dim_2}_d3_{decoder_dim_3}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{embedding_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "plot_train_val_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../데이터/Checkpoint/embedding_tr_0.8_lr_1e-05_wd_0_batch_128_epochs_97_e1_128_e2_128_e3_512_emb_1024_d1512_d2_256_d3_128.pth')\n",
    "dataset = LSTM_Dataset(model, table_1, table_2, table_3, embedding_dim, lstm_window_size)\n",
    "\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Train Loss: 0.0579, Val Loss: 0.0221\n",
      "Epoch [2/150], Train Loss: 0.0162, Val Loss: 0.0095\n",
      "Epoch [3/150], Train Loss: 0.0064, Val Loss: 0.0048\n",
      "Epoch [4/150], Train Loss: 0.0037, Val Loss: 0.0074\n",
      "Epoch [5/150], Train Loss: 0.0027, Val Loss: 0.0030\n",
      "Epoch [6/150], Train Loss: 0.0019, Val Loss: 0.0020\n",
      "Epoch [7/150], Train Loss: 0.0015, Val Loss: 0.0021\n",
      "Epoch [8/150], Train Loss: 0.0011, Val Loss: 0.0015\n",
      "Epoch [9/150], Train Loss: 0.0009, Val Loss: 0.0011\n",
      "Epoch [10/150], Train Loss: 0.0008, Val Loss: 0.0025\n",
      "Epoch [11/150], Train Loss: 0.0007, Val Loss: 0.0023\n",
      "Epoch [12/150], Train Loss: 0.0007, Val Loss: 0.0016 \n",
      "Early Stop Triggered!\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(embedding_dim, lstm_hidden_dim)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lstm_lr, weight_decay=lstm_weight_decay)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "consecutive_val_loss_increases = 0\n",
    "max_consecutive_val_loss_increases = 3\n",
    "\n",
    "for epoch in range(lstm_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        src = data[0]\n",
    "        trg = data[1]\n",
    "\n",
    "        if trg.sum() != 0: \n",
    "            output, _, _ = model(src)\n",
    "            \n",
    "            train_loss = criterion(output, trg)\n",
    "            total_train_loss += train_loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0]\n",
    "            trg = data[1]\n",
    "\n",
    "            if trg.sum() != 0:\n",
    "                output, _, _ = model(src)\n",
    "\n",
    "                val_loss = criterion(output, trg)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # early_stop, consecutive_val_loss_increases = early_stop_1(val_losses, consecutive_val_loss_increases, max_consecutive_val_loss_increases)\n",
    "    early_stop, best_val_loss, consecutive_val_loss_increases = early_stop_2(avg_val_loss, best_val_loss, consecutive_val_loss_increases, max_consecutive_val_loss_increases)\n",
    "    if early_stop:\n",
    "        print(f'Epoch [{epoch+1}/{embedding_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f} \\nEarly Stop Triggered!')\n",
    "        torch.save(model, f'../데이터/Checkpoint/lstm_tr_{train_ratio}_lr_{lstm_lr}_wd_{lstm_weight_decay}_batch_{lstm_batch}_epochs_{epoch+1}_hdim_{lstm_hidden_dim}_ws_{lstm_window_size}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{lstm_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "plot_train_val_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../데이터/Checkpoint/embedding_tr_0.8_lr_1e-05_wd_0_batch_128_epochs_97_e1_128_e2_128_e3_512_emb_1024_d1512_d2_256_d3_128.pth')\n",
    "dataset = LSTM_Dataset(model, table_1, table_2, table_3, embedding_dim, nlinear_window_size)\n",
    "\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=nlinear_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=nlinear_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Train Loss: 0.3216, Val Loss: 0.1944\n",
      "Epoch [2/150], Train Loss: 0.1736, Val Loss: 0.1169\n",
      "Epoch [3/150], Train Loss: 0.1104, Val Loss: 0.0789\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m trg \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trg\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 19\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(src)\n\u001b[1;32m     21\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m criterion(output, trg)\n\u001b[1;32m     22\u001b[0m     total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Git/sci/SCI/코드/Model/NLinear.py:24\u001b[0m, in \u001b[0;36mNLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb_dim):\n\u001b[1;32m     23\u001b[0m     linear_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLinear[i](x[:,:,i])\n\u001b[0;32m---> 24\u001b[0m     bn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBatchNorm[i](linear_output)\n\u001b[1;32m     25\u001b[0m     output[:,i] \u001b[38;5;241m=\u001b[39m bn_output\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     27\u001b[0m output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m seq_last\u001b[38;5;241m.\u001b[39msqueeze()  \u001b[38;5;66;03m# num * emb_dim  \u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m   2479\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[1;32m   2480\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = NLinear(embedding_dim, nlinear_window_size).to(DEVICE)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=nlinear_lr, weight_decay=nlinear_weight_decay)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "consecutive_val_loss_increases = 0\n",
    "max_consecutive_val_loss_increases = 3\n",
    "\n",
    "for epoch in range(nlinear_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        src = data[0].to(DEVICE)\n",
    "        trg = data[1].to(DEVICE)\n",
    "\n",
    "        if trg.sum() != 0:\n",
    "            output = model(src)\n",
    "\n",
    "            train_loss = criterion(output, trg)\n",
    "            total_train_loss += train_loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0].to(DEVICE)\n",
    "            trg = data[1].to(DEVICE)\n",
    "\n",
    "            if trg.sum() != 0:\n",
    "                output = model(src)\n",
    "\n",
    "                val_loss = criterion(output, trg)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # early_stop, consecutive_val_loss_increases = early_stop_1(val_losses, consecutive_val_loss_increases, max_consecutive_val_loss_increases)\n",
    "    early_stop, best_val_loss, consecutive_val_loss_increases = early_stop_2(avg_val_loss, best_val_loss, consecutive_val_loss_increases, max_consecutive_val_loss_increases)\n",
    "    if early_stop:\n",
    "        print(f'Epoch [{epoch+1}/{embedding_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f} \\nEarly Stop Triggered!')\n",
    "        torch.save(model, f'../데이터/Checkpoint/nlinear_tr_{train_ratio}_lr_{nlinear_lr}_wd_{nlinear_weight_decay}_batch_{nlinear_batch}_epochs_{epoch+1}_emb_{embedding_dim}_ws_{nlinear_window_size}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{lstm_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "plot_train_val_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../데이터/Checkpoint/embedding_tr_0.8_lr_1e-05_wd_0_batch_128_epochs_97_e1_128_e2_128_e3_512_emb_1024_d1512_d2_256_d3_128.pth')\n",
    "dataset = Attention_Dataset(model, table_1, table_2, table_3, embedding_dim, attention_window_size)\n",
    "\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=attention_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=attention_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Train Loss: 0.0021, Val Loss: 0.0006\n",
      "Epoch [2/150], Train Loss: 0.0008, Val Loss: 0.0006\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# dong_loss = 0\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m anw:\n\u001b[0;32m---> 26\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(src, index, max_len)\n\u001b[1;32m     28\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m criterion(output, trg[index])\n\u001b[1;32m     29\u001b[0m     total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Git/sci/SCI/코드/Model/Attention.py:71\u001b[0m, in \u001b[0;36mLSTMSeq2Seq.forward\u001b[0;34m(self, src, index, mx_len)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, index, mx_len):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Encoder \u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     hiddens, hidden, cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src)  \u001b[38;5;66;03m# hidden : 1, num, hid_dim\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Decoder\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(hidden[\u001b[38;5;241m0\u001b[39m][index], hidden[:,:mx_len,:])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Git/sci/SCI/코드/Model/Attention.py:17\u001b[0m, in \u001b[0;36mLSTMEncoder.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     15\u001b[0m hidden \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhid_dim))\n\u001b[1;32m     16\u001b[0m cell \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhid_dim))\n\u001b[0;32m---> 17\u001b[0m hiddens, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, (hidden, cell))  \u001b[38;5;66;03m# hiddens : num * window_size * emb_dim\u001b[39;00m\n\u001b[1;32m     18\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(hidden[\u001b[38;5;241m0\u001b[39m])   \u001b[38;5;66;03m# num * out_dim  \u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_hat, hidden, cell\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    876\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    880\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    883\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LSTMSeq2Seq(embedding_dim, attention_hidden_dim, 1).to(DEVICE)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=attention_lr, weight_decay=attention_weight_decay)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "consecutive_val_loss_increases = 0\n",
    "max_consecutive_val_loss_increases = 3\n",
    "\n",
    "for epoch in range(attention_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        src = data[0][0].to(DEVICE)\n",
    "        max_len = data[1][0].to(DEVICE)\n",
    "        anw = data[2][0].to(DEVICE)\n",
    "        trg = data[3][0].to(DEVICE)\n",
    "        \n",
    "        if len(anw)==0:\n",
    "            continue\n",
    "    \n",
    "        # dong_loss = 0\n",
    "\n",
    "        for index in anw:\n",
    "            output = model(src, index, max_len)\n",
    "\n",
    "            train_loss = criterion(output, trg[index])\n",
    "            total_train_loss += train_loss.item()\n",
    "            # dong_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "        # # dong_loss /= len(anw)\n",
    "        # dong_loss = torch.tensor(dong_loss, requires_grad=True).to(DEVICE)\n",
    "        # dong_loss.backward()\n",
    "        # optimizer.step()\n",
    "            \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0][0].to(DEVICE)\n",
    "            max_len = data[1][0].to(DEVICE)\n",
    "            anw = data[2][0].to(DEVICE)\n",
    "            trg = data[3][0].to(DEVICE)\n",
    "\n",
    "            if len(anw)==0:\n",
    "                continue\n",
    "\n",
    "            for index in anw:\n",
    "                output = model(src, index, max_len)\n",
    "\n",
    "                val_loss = criterion(output, trg[index])\n",
    "                total_val_loss += val_loss.item()\n",
    "                \n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "            \n",
    "    # early_stop, consecutive_val_loss_increases = early_stop_1(val_losses, consecutive_val_loss_increases, max_consecutive_val_loss_increases)\n",
    "    early_stop, best_val_loss, consecutive_val_loss_increases = early_stop_2(avg_val_loss, best_val_loss, consecutive_val_loss_increases, max_consecutive_val_loss_increases)\n",
    "    if early_stop:\n",
    "        print(f'Epoch [{epoch+1}/{embedding_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f} \\nEarly Stop Triggered!')\n",
    "        torch.save(model, f'../데이터/Checkpoint/attention_tr_{train_ratio}_lr_{attention_lr}_wd_{attention_weight_decay}_batch_{attention_batch}_epochs_{epoch+1}_hdim_{attention_hidden_dim}_ws_{attention_window_size}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{lstm_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "plot_train_val_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../데이터/Checkpoint/embedding_tr_0.8_lr_1e-05_wd_0_batch_128_epochs_97_e1_128_e2_128_e3_512_emb_1024_d1512_d2_256_d3_128.pth')\n",
    "dataset = LSTM_Dataset(model, table_1, table_2, table_3, embedding_dim, transformer_window_size)\n",
    "\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=transformer_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=transformer_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Train Loss: 1.6242, Val Loss: 1.2924\n",
      "Epoch [2/150], Train Loss: 1.0068, Val Loss: 1.1342\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     25\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 26\u001b[0m         train_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     27\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Transformer(embedding_dim, transformer_window_size, 1, 2, 2).to(DEVICE)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=transformer_lr, weight_decay=transformer_weight_decay)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "consecutive_val_loss_increases = 0\n",
    "max_consecutive_val_loss_increases = 3\n",
    "\n",
    "for epoch in range(transformer_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        src = data[0].to(DEVICE)\n",
    "        trg = data[1].to(DEVICE)\n",
    "\n",
    "        if (trg[0] != 0):\n",
    "            src_mask = model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "            output = model(src, src_mask)\n",
    "\n",
    "            train_loss = criterion(output[0], trg[0])\n",
    "            total_train_loss += train_loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0].to(DEVICE)\n",
    "            trg = data[1].to(DEVICE)\n",
    "\n",
    "            if (trg[0] != 0):\n",
    "                src_mask = model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "                output = model(src, src_mask)\n",
    "\n",
    "                val_loss = criterion(output[0], trg[0])\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # early_stop, consecutive_val_loss_increases = early_stop_1(val_losses, consecutive_val_loss_increases, max_consecutive_val_loss_increases)\n",
    "    early_stop, best_val_loss, consecutive_val_loss_increases = early_stop_2(avg_val_loss, best_val_loss, consecutive_val_loss_increases, max_consecutive_val_loss_increases)\n",
    "    if early_stop:\n",
    "        print(f'Epoch [{epoch+1}/{embedding_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f} \\nEarly Stop Triggered!')\n",
    "        torch.save(model, f'../데이터/Checkpoint/transformer_tr_{train_ratio}_lr_{transformer_lr}_wd_{transformer_weight_decay}_batch_{transformer_batch}_epochs_{epoch+1}_ws_{transformer_window_size}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{lstm_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "plot_train_val_losses(train_losses, val_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
