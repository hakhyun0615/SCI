{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "from Dataset.Embedding_Dataset import Embedding_Dataset\n",
    "from Model.Embedding import Embedding\n",
    "\n",
    "from Dataset.LSTM_Dataset import LSTM_Dataset\n",
    "from Model.LSTM import LSTM\n",
    "from Model.NLinear import NLinear\n",
    "\n",
    "from Dataset.Attention_Dataset import Attention_Dataset\n",
    "from Model.Attention import LSTMSeq2Seq\n",
    "\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = torch.device('cpu') # CPU\n",
    "# DEVICE = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu') # MAC\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # WINDOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "\n",
    "embedding_lr = 0.001\n",
    "embedding_batch = 32\n",
    "embedding_epochs = 150\n",
    "encoder_dim_1 = 128\n",
    "encoder_dim_2 = 256\n",
    "encoder_dim_3 = 512\n",
    "embedding_dim = 1024\n",
    "decoder_dim_1 = 512\n",
    "decoder_dim_2 = 256\n",
    "decoder_dim_3 = 128\n",
    "\n",
    "lstm_lr = 0.01\n",
    "lstm_batch = 1\n",
    "lstm_epochs = 50\n",
    "window_size = 5\n",
    "hidden_dim = 128\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "connection_info = \"host=localhost dbname=postgres user=postgres password=hd219833 port=5432\"\n",
    "conn = psycopg2.connect(connection_info)\n",
    "table_1_query = '''\n",
    "    SELECT * FROM building\n",
    "    '''\n",
    "table_2_query = '''\n",
    "    SELECT * FROM economy\n",
    "    '''\n",
    "table_3_query = '''\n",
    "    SELECT * FROM building_price\n",
    "    '''\n",
    "table_1 = pd.read_sql(table_1_query,conn) \n",
    "table_2 = pd.read_sql(table_2_query,conn)\n",
    "table_3 = pd.read_sql(table_3_query,conn) \n",
    "\n",
    "# table_1 = pd.read_csv('../데이터/Table/table_1.csv') \n",
    "# table_2 = pd.read_csv('../데이터/Table/table_2.csv') \n",
    "# table_3 = pd.read_csv('../데이터/Table/table_3.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val_losses(train_losses, val_losses):\n",
    "    print(f'Min Validation Loss: {min(val_losses)}')\n",
    "    plt.plot(train_losses[1:], label='Training Loss')\n",
    "    plt.plot(val_losses[1:], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Embedding_Dataset(table_1, table_2, table_3)\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Train Loss: 5485744021.1436, Val Loss: 5862229347.5945\n",
      "Epoch [2/150], Train Loss: 5330702124.5198, Val Loss: 5677250688.0000\n",
      "Epoch [3/150], Train Loss: 5082821323.7319, Val Loss: 5464689039.4301\n",
      "Epoch [4/150], Train Loss: 4774895943.3324, Val Loss: 5431757052.3178\n",
      "Epoch [5/150], Train Loss: 4423141044.2462, Val Loss: 5120573020.0548\n",
      "Epoch [6/150], Train Loss: 4048584302.6758, Val Loss: 5154541196.2740\n",
      "Epoch [7/150], Train Loss: 3666082441.2695, Val Loss: 5004660386.3671\n",
      "Epoch [8/150], Train Loss: 3290655976.6731, Val Loss: 4531224757.4795\n",
      "Epoch [9/150], Train Loss: 2932164632.1094, Val Loss: 4829075747.5945\n",
      "Epoch [10/150], Train Loss: 2605983041.6060, Val Loss: 4698504058.0384\n",
      "Epoch [11/150], Train Loss: 2324649433.8386, Val Loss: 4336684090.3890\n",
      "Epoch [12/150], Train Loss: 2081418097.3352, Val Loss: 4335683187.6822\n",
      "Epoch [13/150], Train Loss: 1858267360.4087, Val Loss: 4217284531.9890\n",
      "Epoch [14/150], Train Loss: 1687583435.3673, Val Loss: 4251321506.1918\n",
      "Epoch [15/150], Train Loss: 1492186946.1758, Val Loss: 3646984101.5233\n",
      "Epoch [16/150], Train Loss: 1368494657.7750, Val Loss: 3423670576.3616\n",
      "Epoch [17/150], Train Loss: 1287829191.7794, Val Loss: 3430017521.8959\n",
      "Epoch [18/150], Train Loss: 1220573144.3314, Val Loss: 3021764153.8411\n",
      "Epoch [19/150], Train Loss: 1159782180.1984, Val Loss: 3348530209.0521\n",
      "Epoch [20/150], Train Loss: 1107798787.7866, Val Loss: 1997718719.1945\n",
      "Epoch [21/150], Train Loss: 1068949811.7454, Val Loss: 1911169889.5945\n",
      "Epoch [22/150], Train Loss: 1045969602.4425, Val Loss: 2019203645.8329\n",
      "Epoch [23/150], Train Loss: 1024349621.9525, Val Loss: 1931190182.8822\n",
      "Epoch [24/150], Train Loss: 1007710827.4083, Val Loss: 2128136597.2986\n",
      "Epoch [25/150], Train Loss: 999196847.8526, Val Loss: 2164844545.7479\n",
      "Early Stopping Triggered\n"
     ]
    }
   ],
   "source": [
    "model = Embedding(encoder_dim_1, encoder_dim_2, encoder_dim_3, embedding_dim, decoder_dim_1, decoder_dim_2, decoder_dim_3).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=embedding_lr)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "consecutive_val_loss_increases = 0\n",
    "max_consecutive_val_loss_increases = 5\n",
    "for epoch in range(embedding_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        input = data[0].to(DEVICE)\n",
    "        target = data[1].to(DEVICE)\n",
    "        output = model(input).to(DEVICE)\n",
    "\n",
    "        train_loss = criterion(output, target)\n",
    "        total_train_loss += train_loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            input = data[0].to(DEVICE)\n",
    "            target = data[1].to(DEVICE)\n",
    "            output = model(input).to(DEVICE)\n",
    "\n",
    "            val_loss = criterion(output, target)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # if len(val_losses) > 1 and val_losses[-1] > val_losses[-2]:\n",
    "    #     consecutive_val_loss_increases += 1\n",
    "    #     if consecutive_val_loss_increases >= max_consecutive_val_loss_increases:\n",
    "    #         torch.save(model, f'../데이터/Checkpoint/embedding_tr_{train_ratio}_lr_{embedding_lr}_batch_{embedding_batch}_epochs_{embedding_epochs}_e1_{encoder_dim_1}_e2_{encoder_dim_1}_e3_{encoder_dim_3}_emb_{embedding_dim}_d1{decoder_dim_1}_d2_{decoder_dim_2}_d3_{decoder_dim_3}.pth')\n",
    "    #         print(f\"Early Stopping Triggered!\")\n",
    "    #         break\n",
    "    # else:\n",
    "    #     consecutive_val_loss_increases = 0\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        consecutive_val_loss_increases = 0  \n",
    "    else:\n",
    "        consecutive_val_loss_increases += 1 \n",
    "    if consecutive_val_loss_increases == max_consecutive_val_loss_increases:\n",
    "        torch.save(model, f'../데이터/Checkpoint/embedding_tr_{train_ratio}_lr_{embedding_lr}_batch_{embedding_batch}_epochs_{embedding_epochs}_e1_{encoder_dim_1}_e2_{encoder_dim_1}_e3_{encoder_dim_3}_emb_{embedding_dim}_d1{decoder_dim_1}_d2_{decoder_dim_2}_d3_{decoder_dim_3}.pth')\n",
    "        print(\"Early Stopping Triggered\")\n",
    "        break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{embedding_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Validation Loss: 1911169889.5945206\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_train_val_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../데이터/Checkpoint/embedding_train_0.8_lr_0.01_batch_32_epochs_50_dim_6.pth')\n",
    "dataset = LSTM_Dataset(model, table_1, table_2, table_3, embedding_dim, window_size)\n",
    "\n",
    "train_ratio = 0.8\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 4457084913.2495\n",
      "Epoch [2/50], Train Loss: 2639799669.2864\n",
      "Epoch [3/50], Train Loss: 1889211181.3742\n",
      "Epoch [4/50], Train Loss: 1621113765.7466\n",
      "Epoch [5/50], Train Loss: 1536347630.7305\n",
      "Epoch [6/50], Train Loss: 1508256809.3546\n",
      "Epoch [7/50], Train Loss: 1501316050.4178\n",
      "Epoch [8/50], Train Loss: 1496720547.7443\n",
      "Epoch [9/50], Train Loss: 1493436904.9662\n",
      "Epoch [10/50], Train Loss: 1491849440.4544\n",
      "Epoch [11/50], Train Loss: 1490709939.9167\n",
      "Epoch [12/50], Train Loss: 1490298252.6531\n",
      "Epoch [13/50], Train Loss: 1490066771.7568\n",
      "Epoch [14/50], Train Loss: 1489904553.6412\n",
      "Epoch [15/50], Train Loss: 1489809264.2112\n",
      "Epoch [16/50], Train Loss: 1489719638.4178\n",
      "Epoch [17/50], Train Loss: 1489666388.2125\n",
      "Epoch [18/50], Train Loss: 1489614520.1380\n",
      "Epoch [19/50], Train Loss: 1489531656.4119\n",
      "Epoch [20/50], Train Loss: 1489476864.0525\n",
      "Epoch [21/50], Train Loss: 1489432332.4914\n",
      "Epoch [22/50], Train Loss: 1489389663.6376\n",
      "Epoch [23/50], Train Loss: 1489300576.0239\n",
      "Epoch [24/50], Train Loss: 1489243003.7281\n",
      "Epoch [25/50], Train Loss: 1489210289.8569\n",
      "Epoch [26/50], Train Loss: 1489147177.3672\n",
      "Epoch [27/50], Train Loss: 1489087240.8590\n",
      "Epoch [28/50], Train Loss: 1489049705.3208\n",
      "Epoch [29/50], Train Loss: 1488954457.3110\n",
      "Epoch [30/50], Train Loss: 1488918861.9109\n",
      "Epoch [31/50], Train Loss: 1488849718.9130\n",
      "Epoch [32/50], Train Loss: 1488789314.3850\n",
      "Epoch [33/50], Train Loss: 1488730489.2280\n",
      "Epoch [34/50], Train Loss: 1488664809.6480\n",
      "Epoch [35/50], Train Loss: 1488615758.3526\n",
      "Epoch [36/50], Train Loss: 1488567962.9942\n",
      "Epoch [37/50], Train Loss: 1488495349.4339\n",
      "Epoch [38/50], Train Loss: 1488466819.5393\n",
      "Epoch [39/50], Train Loss: 1488416725.0039\n",
      "Epoch [40/50], Train Loss: 1488351656.2833\n",
      "Epoch [41/50], Train Loss: 1488283501.6679\n",
      "Epoch [42/50], Train Loss: 1488228943.3389\n",
      "Epoch [43/50], Train Loss: 1488196455.7372\n",
      "Epoch [44/50], Train Loss: 1488117979.1185\n",
      "Epoch [45/50], Train Loss: 1488069540.7553\n",
      "Epoch [46/50], Train Loss: 1488010932.9404\n",
      "Epoch [47/50], Train Loss: 1487973938.8470\n",
      "Epoch [48/50], Train Loss: 1487893113.0089\n",
      "Epoch [49/50], Train Loss: 1487875749.7484\n",
      "Epoch [50/50], Train Loss: 1487800924.4878\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lstm_lr)\n",
    "\n",
    "model.train()\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(lstm_epochs):\n",
    "    total_train_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        src = data[0]\n",
    "        trg = data[1]\n",
    "\n",
    "        if trg.sum() != 0: \n",
    "            output = model(src)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0]\n",
    "            trg = data[1]\n",
    "\n",
    "            if trg.sum() != 0:\n",
    "                output = model(src)\n",
    "\n",
    "                val_loss = criterion(output, trg)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{lstm_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "torch.save(model, f'../데이터/Checkpoint/lstm_lr_{lstm_lr}_batch_{lstm_batch}_epochs_{lstm_epochs}_hdim_{hidden_dim}_odim_{output_dim}_ws_{window_size}.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../데이터/Checkpoint/embedding_tr_0.8_lr_0.001_batch_32_epochs_150_e1_128_e2_128_e3_512_emb_1024_d1512_d2_256_d3_128.pth')\n",
    "dataset = LSTM_Dataset(model, table_1, table_2, table_3, embedding_dim, window_size)\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=lstm_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=lstm_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     23\u001b[0m         train_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 24\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[1;32m     27\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     adam(\n\u001b[1;32m    164\u001b[0m         params_with_grad,\n\u001b[1;32m    165\u001b[0m         grads,\n\u001b[1;32m    166\u001b[0m         exp_avgs,\n\u001b[1;32m    167\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    168\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    169\u001b[0m         state_steps,\n\u001b[1;32m    170\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    171\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    172\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    173\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    174\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    175\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    176\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    177\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    178\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    179\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    180\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    181\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    182\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m func(params,\n\u001b[1;32m    312\u001b[0m      grads,\n\u001b[1;32m    313\u001b[0m      exp_avgs,\n\u001b[1;32m    314\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    315\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    316\u001b[0m      state_steps,\n\u001b[1;32m    317\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    318\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    319\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    320\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    321\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    322\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    323\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    324\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    325\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    326\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    327\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:385\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    384\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 385\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    388\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = NLinear(embedding_dim, window_size).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lstm_lr)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "consecutive_val_loss_increases = 0\n",
    "max_consecutive_val_loss_increases = 3\n",
    "for epoch in range(lstm_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        src = data[0].to(DEVICE)\n",
    "        trg = data[1].to(DEVICE)\n",
    "\n",
    "        if trg.sum() != 0:\n",
    "            output = model(src)\n",
    "\n",
    "            train_loss = criterion(output, trg)\n",
    "            total_train_loss += train_loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0].to(DEVICE)\n",
    "            trg = data[1].to(DEVICE)\n",
    "\n",
    "            if trg.sum() != 0:\n",
    "                output = model(src)\n",
    "\n",
    "                val_loss = criterion(output, trg)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    if len(val_losses) > 1 and val_losses[-1] > val_losses[-2]:\n",
    "        consecutive_val_loss_increases += 1\n",
    "        if consecutive_val_loss_increases >= max_consecutive_val_loss_increases:\n",
    "            torch.save(model, f'../데이터/Checkpoint/nlinear_tr_{train_ratio}_lr_{lstm_lr}_batch_{lstm_batch}_epochs_{lstm_epochs}_emb_{embedding_dim}_ws_{window_size}.pth')\n",
    "            print(f\"Early Stopping Triggered!\")\n",
    "            break\n",
    "    else:\n",
    "        consecutive_val_loss_increases = 0\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{lstm_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../데이터/Checkpoint/embedding_train_0.8_lr_0.01_batch_32_epochs_50_dim_6.pth')\n",
    "dataset = Attention_Dataset(model, table_1, table_2, table_3, embedding_dim, window_size)\n",
    "\n",
    "train_ratio = 0.8\n",
    "dataset_length = len(dataset)\n",
    "split_point = int(train_ratio * len(dataset))\n",
    "train_indices = range(0, split_point)\n",
    "val_indices = range(split_point, dataset_length)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=embedding_batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('C:/Users/hkyoo/OneDrive/바탕 화면/SCI/데이터/Checkpoint/embedding_lr_0.01_batch_32_epochs_50_dim_6.pth')\n",
    "dataset = Attention_Dataset(model, table_1, table_2, table_3, embedding_dim, window_size)\n",
    "dataloader = DataLoader(dataset, batch_size=lstm_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0254], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = 38\n",
    "window_size = 5\n",
    "mx_len = 1\n",
    "input = torch.randn(num, window_size, embedding_dim)\n",
    "\n",
    "model = LSTMSeq2Seq(embedding_dim, hidden_dim, output_dim, DEVICE)\n",
    "model(input,1, mx_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# 학습\n",
    "history = {'train_loss':[], 'val_loss':[], 'lr':[]}\n",
    "\n",
    "for epoch in range(att_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    num = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        src = batch[0][0].to(DEVICE)\n",
    "        max_len = batch[1][0].to(DEVICE)\n",
    "        anw = batch[2][0]\n",
    "        trg = batch[3][0].to(DEVICE)\n",
    "        \n",
    "        if len(anw)==0:\n",
    "            continue\n",
    "        \n",
    "        num += len(anw)\n",
    "        \n",
    "        dong_loss = 0\n",
    "        for index in anw:\n",
    "            index.to(DEVICE)\n",
    "            output = model(src, index, max_len)\n",
    "            loss = criterion(output, trg)\n",
    "            # dong_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # optimizer.zero_grad()\n",
    "        # # dong_loss /= len(anw)\n",
    "        # dong_loss = torch.tensor(dong_loss, requires_grad=True).to(DEVICE)\n",
    "        # dong_loss.backward()\n",
    "        # optimizer.step()\n",
    "    train_loss = epoch_loss / num\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    \n",
    "    \n",
    "\n",
    "#     if epoch%valid_every==0:\n",
    "#         print(\"==========================\")\n",
    "#         model.eval()\n",
    "#         epoch_loss = 0\n",
    "#         valid_num = 1e-9\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for i, batch in enumerate(val_loader):\n",
    "#                 src = batch[0].to(DEVICE)\n",
    "#                 trg = batch[1].to(DEVICE)\n",
    "#                 if(trg != 0):\n",
    "#                     output = model(src, trg)\n",
    "#                     loss = criterion(output, trg)\n",
    "#                     epoch_loss += loss.item()\n",
    "#                     valid_num += 1\n",
    "#         valid_loss = epoch_loss / valid_num\n",
    "            \n",
    "#         if valid_loss < best_valid_loss:\n",
    "#             best_valid_loss = valid_loss\n",
    "#             model.decoder.t=0\n",
    "#             torch.save(model.state_dict(), 'lstm-model.pt')\n",
    "#         print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
    "\n",
    "#         history['train_loss'].append(train_loss)\n",
    "#         history['val_loss'].append(valid_loss)\n",
    "#         history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('C:/Users/hkyoo/OneDrive/바탕 화면/SCI/데이터/Checkpoint/embedding_lr_0.01_batch_32_epochs_50_dim_6.pth')\n",
    "\n",
    "dataset = LSTM_Dataset(model, table_1, table_2, table_3, embedding_dim, window_size)\n",
    "dataloader = DataLoader(dataset, batch_size=lstm_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_dim, out_dim, nhead, nlayers\n",
    "model = TFModel(embedding_dim, window_size, output_dim, 2, 2)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lstm_lr)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(lstm_epochs):\n",
    "    epoch_loss = 0\n",
    "    train_num = 1e-9\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        src = batch[0].to(DEVICE)\n",
    "        trg = batch[1].to(DEVICE)\n",
    "        if(trg != 0):\n",
    "            train_num += 1\n",
    "            optimizer.zero_grad()\n",
    "            src_mask = model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "            output = model(src, src_mask)\n",
    "            loss = criterion(output[0], trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "    train_loss = epoch_loss / train_num\n",
    "    print(f'Epoch [{epoch+1}/{embedding_epochs}], Train Loss: {train_loss:.4f}')\n",
    "\n",
    "torch.save(model, f'../데이터/Checkpoint/lstm_lsr_{lstm_lr}_batch_{lstm_batch}_epochs_{lstm_epochs}_hdim_{hidden_dim}_odim_{output_dim}_ws{window_size}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
