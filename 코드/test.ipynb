{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "from Dataset.Embedding_Dataset import Embedding_Dataset\n",
    "from Model.Embedding import Embedding\n",
    "\n",
    "from Dataset.Apartment_Complex_Dataset import Apartment_Complex_Dataset\n",
    "from Model.LSTM import LSTM\n",
    "from Model.GRU import GRU\n",
    "from Model.Transformer import Transformer\n",
    "\n",
    "from Dataset.District_Dataset import District_Dataset\n",
    "from Model.LSTM_Attention import LSTMAttention\n",
    "from Model.GRU_Attention import GRUAttention\n",
    "from Model.Transformer_Attention import TransformerAttention\n",
    "\n",
    "from utils import RMSE, rmse, mse, mae, mape, save_train_val_losses\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# connection_info = \"host=localhost dbname=postgres user=postgres password=hd219833 port=5432\"\n",
    "# conn = psycopg2.connect(connection_info)\n",
    "# table_1_query = '''\n",
    "#     SELECT * FROM building\n",
    "#     '''\n",
    "# table_2_query = '''\n",
    "#     SELECT * FROM economy\n",
    "#     '''\n",
    "# table_3_query = '''\n",
    "#     SELECT * FROM building_price\n",
    "#     '''\n",
    "# table_1 = pd.read_sql(table_1_query,conn) \n",
    "# table_2 = pd.read_sql(table_2_query,conn)\n",
    "# table_3 = pd.read_sql(table_3_query,conn) \n",
    "\n",
    "table_1 = pd.read_csv('../데이터/Table/table_1.csv') \n",
    "table_2 = pd.read_csv('../데이터/Table/table_2.csv') \n",
    "table_3 = pd.read_csv('../데이터/Table/table_3.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "lr = 1e-4\n",
    "batch = 64\n",
    "hidden_dim = 1024\n",
    "sub = True # True\n",
    "embedding_dim = 1024 # 1024\n",
    "window_size = 12 # 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = torch.load(\"../데이터/Checkpoint/embedding/sub_False/embedding_lr_0.0001_batch_64_sub_False_emb_1024_ws_12_epochs_11.pth\", map_location=DEVICE)\n",
    "dataset = District_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, sub, DEVICE)\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "# train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "# val_indices = range(train_size, train_size + val_size)\n",
    "test_size = int(test_ratio * dataset_length)\n",
    "test_indices = range(train_size + val_size, dataset_length)\n",
    "# train_dataset = Subset(dataset, train_indices)\n",
    "# val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "model = torch.load(\"../데이터/Checkpoint/lstm/default/lstm_lr_0.0001_batch_64_hid_1024_sub_True_emb_1024_ws_12_epochs_10.pth\", map_location=DEVICE)\n",
    "\n",
    "# GRU\n",
    "# model = torch.load(\"../데이터/Checkpoint/gru/default/gru_lr_0.0001_batch_64_hid_1024_sub_True_emb_1024_ws_12_epochs_9.pth\", map_location=DEVICE)\n",
    "\n",
    "# transformer\n",
    "# model = torch.load(\"../데이터/Checkpoint/transformer/default/transformer_lr_0.0001_batch_64_sub_True_emb_1024_ws_12_epochs_15.pth\", map_location=DEVICE)\n",
    "\n",
    "# LSTM attention\n",
    "# model = torch.load(\"../데이터/Checkpoint/lstm/attention/lstm_attention_lr_0.0001_batch_64_sub_True_emb_1024_ws_12_epochs_8.pth\", map_location=DEVICE)\n",
    "\n",
    "# GRU attention\n",
    "# model = torch.load(\"../데이터/Checkpoint/gru/attention/gru_attention_lr_0.0001_batch_64_sub_True_emb_1024_ws_12_epochs_4.pth\", map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 60>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m             test_mapes\u001b[38;5;241m.\u001b[39mappend(test_mape\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# save_path = f'../데이터/Checkpoint/transformer/default/transformer_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{15}'\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# with open(f'{save_path}_test_rmses.txt', 'w') as f:\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m#     for item in test_rmses:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m#     for item in test_mapes:\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m#         f.write(\"%s\\n\" % item)\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m avg_test_rmse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_rmses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_rmses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m avg_test_mse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(test_mses) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_mses)\n\u001b[0;32m     62\u001b[0m avg_test_mape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(test_mapes) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_mapes)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_rmses = []\n",
    "test_mses = []\n",
    "test_mapes = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        src = data[0][0].to(DEVICE)\n",
    "        max_len = data[1][0].to(DEVICE)\n",
    "        try:\n",
    "            anw = torch.nonzero(data[2][0]).to(DEVICE)[0]\n",
    "        except:\n",
    "            continue\n",
    "        trg = data[3][0].to(DEVICE)\n",
    "\n",
    "        for index in anw:\n",
    "            \n",
    "            # LSTM\n",
    "            output, _, _ = model(src)\n",
    "            \n",
    "            # GRU\n",
    "            # output, _ = model(src)\n",
    "            \n",
    "            # nlinear\n",
    "            # output, _ = model(src)\n",
    "            \n",
    "            # transformer\n",
    "            # src_mask = model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "            # output, _ = model(src, src_mask)\n",
    "            \n",
    "            test_rmse = rmse(output[index], trg[index])\n",
    "            test_mse = mse(output[index], trg[index])\n",
    "            test_mape = mape(output[index], trg[index])\n",
    "            print(output[index])\n",
    "            print(trg[index])\n",
    "            print(test_mape)\n",
    "            \n",
    "            # attention\n",
    "            # output = model(src, index, max_len)\n",
    "\n",
    "            # test_rmse = rmse(output, trg[index])\n",
    "            # test_mse = mse(output, trg[index])\n",
    "            # test_mape = mape(output, trg[index])\n",
    "            \n",
    "            test_rmses.append(test_rmse.item())\n",
    "            test_mses.append(test_mse.item())\n",
    "            test_mapes.append(test_mape.item())\n",
    "\n",
    "# save_path = f'../데이터/Checkpoint/transformer/default/transformer_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{15}'\n",
    "# with open(f'{save_path}_test_rmses.txt', 'w') as f:\n",
    "#     for item in test_rmses:\n",
    "#         f.write(\"%s\\n\" % item)\n",
    "# with open(f'{save_path}_test_mses.txt', 'w') as f:\n",
    "#     for item in test_mses:\n",
    "#         f.write(\"%s\\n\" % item)\n",
    "# with open(f'{save_path}_test_mapes.txt', 'w') as f:\n",
    "#     for item in test_mapes:\n",
    "#         f.write(\"%s\\n\" % item)\n",
    "\n",
    "avg_test_rmse = sum(test_rmses) / len(test_rmses)\n",
    "avg_test_mse = sum(test_mses) / len(test_mses)\n",
    "avg_test_mape = sum(test_mapes) / len(test_mapes)\n",
    "\n",
    "print(f'Test RMSE: {avg_test_rmse:.4f}')\n",
    "print(f'Test MSE: {avg_test_mse:.4f}')\n",
    "print(f'Test MAPE: {avg_test_mape:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = torch.load('../데이터/Checkpoint/embedding/default/embedding_lr_0.0001_batch_64_sub_True_emb_1024_ws_12_epochs_13.pth', map_location=DEVICE)\n",
    "dataset = Apartment_Complex_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, 'ML', DEVICE)\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "# train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "# val_indices = range(train_size, train_size + val_size)\n",
    "test_size = int(test_ratio * dataset_length)\n",
    "test_indices = range(train_size + val_size, dataset_length)\n",
    "# train_dataset = Subset(dataset, train_indices)\n",
    "# val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm\n",
    "model = joblib.load(f'../데이터/Checkpoint/lightgbm/lightgbm_batch_64_ws_12.pkl')\n",
    "\n",
    "# catboost\n",
    "model = joblib.load(f'../데이터/Checkpoint/catboost/catboost_batch_64_ws_12.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "      mse = np.mean((y_true - y_pred) ** 2)\n",
    "      return np.sqrt(mse)\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "      return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "      return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 18.5903\n",
      "Test MSE: 367.7499\n",
      "Test MAPE: 298.4526\n"
     ]
    }
   ],
   "source": [
    "test_rmses = []\n",
    "test_mses = []\n",
    "test_mapes = []\n",
    "\n",
    "for data in test_dataloader:\n",
    "    X, y = data[0].squeeze().cpu().numpy(), data[1].squeeze().cpu().numpy()\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    test_rmse = rmse(y, y_pred)\n",
    "    test_mse = mse(y, y_pred)\n",
    "    test_mape = mape(y, y_pred)\n",
    "\n",
    "    test_rmses.append(test_rmse)\n",
    "    test_mses.append(test_mse)\n",
    "    test_mapes.append(test_mape)\n",
    "\n",
    "save_path = f'../데이터/Checkpoint/lightgbm/lightgbm_batch_{batch}_ws_{window_size}'\n",
    "with open(f'{save_path}_test_rmses.txt', 'w') as f:\n",
    "    for item in test_rmses:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open(f'{save_path}_test_mses.txt', 'w') as f:\n",
    "    for item in test_mses:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open(f'{save_path}_test_mapes.txt', 'w') as f:\n",
    "    for item in test_mapes:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "avg_test_rmse = sum(test_rmses) / len(test_rmses)\n",
    "avg_test_mse = sum(test_mses) / len(test_mses)\n",
    "avg_test_mape = sum(test_mapes) / len(test_mapes)\n",
    "\n",
    "print(f'Test RMSE: {avg_test_rmse:.4f}')\n",
    "print(f'Test MSE: {avg_test_mse:.4f}')\n",
    "print(f'Test MAPE: {avg_test_mape:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
