{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "from Dataset.Embedding_Dataset import Embedding_Dataset\n",
    "from Model.Embedding import Embedding\n",
    "\n",
    "from Dataset.Apartment_Complex_Dataset import Apartment_Complex_Dataset\n",
    "from Model.LSTM import LSTM\n",
    "from Model.GRU import GRU\n",
    "from Model.Transformer import Transformer\n",
    "\n",
    "from Dataset.District_Dataset import District_Dataset\n",
    "from Model.LSTM_Attention import LSTMAttention\n",
    "from Model.GRU_Attention import GRUAttention\n",
    "from Model.Transformer_Attention import TransformerAttention\n",
    "\n",
    "from utils import RMSE, rmse, mse, mae, mape, save_train_val_losses\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "table_1 = pd.read_csv('../데이터/Table/table_1.csv') \n",
    "table_2 = pd.read_csv('../데이터/Table/table_2.csv') \n",
    "table_3 = pd.read_csv('../데이터/Table/table_3.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "epochs = 10000\n",
    "lr = 1e-4\n",
    "batch = 256\n",
    "subs = [True, False]\n",
    "# sub = True\n",
    "# embedding_dims = [512, 1024, 2048]\n",
    "embedding_dim = 1024\n",
    "# window_sizes = [1, 6, 12]\n",
    "window_size = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\t Epoch [1/10000], Train Loss: 4.522762, Val Loss: 7.855382\n",
      "Embedding\t Epoch [2/10000], Train Loss: 3.422679, Val Loss: 7.002362\n",
      "Embedding\t Epoch [3/10000], Train Loss: 2.443402, Val Loss: 6.018097\n",
      "Embedding\t Epoch [4/10000], Train Loss: 1.460522, Val Loss: 5.026543\n",
      "Embedding\t Epoch [5/10000], Train Loss: 0.767148, Val Loss: 4.310271\n",
      "Embedding\t Epoch [6/10000], Train Loss: 0.630525, Val Loss: 4.018781\n",
      "Embedding\t Epoch [7/10000], Train Loss: 0.603969, Val Loss: 3.856025\n",
      "Embedding\t Epoch [8/10000], Train Loss: 0.590682, Val Loss: 3.747646\n",
      "Embedding\t Epoch [9/10000], Train Loss: 0.583022, Val Loss: 3.709238\n",
      "Embedding\t Epoch [10/10000], Train Loss: 0.569611, Val Loss: 3.659194\n",
      "Embedding\t Epoch [11/10000], Train Loss: 0.557321, Val Loss: 3.595130\n",
      "Embedding\t Epoch [12/10000], Train Loss: 0.549572, Val Loss: 3.603923\n",
      "Embedding\t Epoch [13/10000], Train Loss: 0.538739, Val Loss: 3.570874\n",
      "Embedding\t Epoch [14/10000], Train Loss: 0.529397, Val Loss: 3.536874\n",
      "Embedding\t Epoch [15/10000], Train Loss: 0.522401, Val Loss: 3.513501\n",
      "Embedding\t Epoch [16/10000], Train Loss: 0.512719, Val Loss: 3.499871\n",
      "Embedding\t Epoch [17/10000], Train Loss: 0.507036, Val Loss: 3.495504\n",
      "Embedding\t Epoch [18/10000], Train Loss: 0.499391, Val Loss: 3.450203\n",
      "Embedding\t Epoch [19/10000], Train Loss: 0.493784, Val Loss: 3.428766\n",
      "Embedding\t Epoch [20/10000], Train Loss: 0.487058, Val Loss: 3.411013\n",
      "Embedding\t Epoch [21/10000], Train Loss: 0.484023, Val Loss: 3.398193\n",
      "Embedding\t Epoch [22/10000], Train Loss: 0.478562, Val Loss: 3.384567\n",
      "Embedding\t Epoch [23/10000], Train Loss: 0.473440, Val Loss: 3.359202\n",
      "Embedding\t Epoch [24/10000], Train Loss: 0.469109, Val Loss: 3.352561\n",
      "Embedding\t Epoch [25/10000], Train Loss: 0.464886, Val Loss: 3.343167\n",
      "Embedding\t Epoch [26/10000], Train Loss: 0.461541, Val Loss: 3.319257\n",
      "Embedding\t Epoch [27/10000], Train Loss: 0.457584, Val Loss: 3.354413\n",
      "Embedding\t Epoch [28/10000], Train Loss: 0.452881, Val Loss: 3.342028\n",
      "Embedding\t Epoch [29/10000], Train Loss: 0.448311, Val Loss: 3.329246 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 0.4483105374105049\n",
      "Min Val Loss: 3.319256653176977\n",
      "Transformer\t Epoch [1/10000], Train Loss: 3.885806, Val Loss: 4.022878\n",
      "Transformer\t Epoch [2/10000], Train Loss: 3.785732, Val Loss: 3.821091\n",
      "Transformer\t Epoch [3/10000], Train Loss: 3.731820, Val Loss: 3.985175\n",
      "Transformer\t Epoch [4/10000], Train Loss: 3.691489, Val Loss: 4.071572\n",
      "Transformer\t Epoch [5/10000], Train Loss: 3.657876, Val Loss: 4.071753 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 3.657876079572391\n",
      "Min Val Loss: 3.821091367912176\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [2] at entry 0 and [4] at entry 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    199\u001b[0m transformer_att_total_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    200\u001b[0m transformer_att_total_train_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-9\u001b[39m\n\u001b[1;32m--> 201\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m    202\u001b[0m     src \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m    203\u001b[0m     max_len \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    139\u001b[0m         storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    140\u001b[0m         out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr_\u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring_\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndarray\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemmap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [2] at entry 0 and [4] at entry 2"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(columns=['lr','batch','sub','embedding_dim','window_size','val_loss'])\n",
    "for sub in subs:\n",
    "    ### embedding\n",
    "    dataset = Embedding_Dataset(table_1, table_2, table_3)\n",
    "    dataset_length = len(dataset)\n",
    "    train_size = int(train_ratio * dataset_length)\n",
    "    train_indices = range(0, train_size)\n",
    "    val_size = int(val_ratio * dataset_length)\n",
    "    val_indices = range(train_size, train_size + val_size)\n",
    "    # test_size = int(test_ratio * dataset_length)\n",
    "    # test_indices = range(train_size + val_size, dataset_length)\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    # test_dataset = Subset(dataset, test_indices)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "    # test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "    embedding_model = Embedding(128, 256, 512, embedding_dim, 512, 256, 128).to(DEVICE)\n",
    "    criterion = RMSE()\n",
    "    optimizer = torch.optim.Adam(embedding_model.parameters(), lr=lr)\n",
    "\n",
    "    embedding_train_losses = []\n",
    "    embedding_val_losses = []\n",
    "\n",
    "    max_early_stop_count = 3\n",
    "    early_stop_count = 0\n",
    "    embedding_best_val_loss = float('inf')\n",
    "    embedding_best_model_weights = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        embedding_model.train()\n",
    "        embedding_total_train_loss = 0\n",
    "        for data in train_dataloader:\n",
    "            input = data[0].to(DEVICE)\n",
    "            target = data[1].to(DEVICE)\n",
    "            output = embedding_model(input).to(DEVICE)\n",
    "\n",
    "            embedding_train_loss = criterion(output, target)\n",
    "            embedding_total_train_loss += embedding_train_loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            embedding_train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        embedding_avg_train_loss = embedding_total_train_loss / len(train_dataloader)\n",
    "        embedding_train_losses.append(embedding_avg_train_loss)\n",
    "\n",
    "        embedding_model.eval()\n",
    "        embedding_total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_dataloader:\n",
    "                input = data[0].to(DEVICE)\n",
    "                target = data[1].to(DEVICE)\n",
    "                output = embedding_model(input).to(DEVICE)\n",
    "\n",
    "                embedding_val_loss = criterion(output, target)\n",
    "                embedding_total_val_loss += embedding_val_loss.item()\n",
    "\n",
    "        embedding_avg_val_loss = embedding_total_val_loss / len(val_dataloader)\n",
    "        embedding_val_losses.append(embedding_avg_val_loss)\n",
    "\n",
    "        if  embedding_best_val_loss > embedding_avg_val_loss:\n",
    "            embedding_best_val_loss = embedding_avg_val_loss\n",
    "            embedding_best_model_weights = copy.deepcopy(embedding_model.state_dict())\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= max_early_stop_count:\n",
    "            print(f'Embedding\\t Epoch [{epoch+1}/{epochs}], Train Loss: {embedding_avg_train_loss:.6f}, Val Loss: {embedding_avg_val_loss:.6f} \\nEarly Stop Triggered!')\n",
    "            embedding_model.load_state_dict(embedding_best_model_weights)\n",
    "            torch.save(embedding_model, f'../데이터/Checkpoint/emb/embedding_lr_{lr}_batch_{batch}_sub_{sub}_ed_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "            break\n",
    "\n",
    "        print(f'Embedding\\t Epoch [{epoch+1}/{epochs}], Train Loss: {embedding_avg_train_loss:.6f}, Val Loss: {embedding_avg_val_loss:.6f}')\n",
    "        \n",
    "    save_train_val_losses(embedding_train_losses, embedding_val_losses, f'../데이터/Checkpoint/emb/embedding_lr_{lr}_batch_{batch}_sub_{sub}_ed_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "\n",
    "    ### transformer\n",
    "    dataset = Apartment_Complex_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, 'DL', DEVICE)\n",
    "    dataset_length = len(dataset)\n",
    "    train_size = int(train_ratio * dataset_length)\n",
    "    train_indices = range(0, train_size)\n",
    "    val_size = int(val_ratio * dataset_length)\n",
    "    val_indices = range(train_size, train_size + val_size)\n",
    "    # test_size = int(test_ratio * dataset_length)\n",
    "    # test_indices = range(train_size + val_size, dataset_length)\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    # test_dataset = Subset(dataset, test_indices)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "    # test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "    transformer_model = Transformer(embedding_dim, 1, 2, 2).to(DEVICE)\n",
    "    criterion = RMSE()\n",
    "    optimizer = torch.optim.Adam(transformer_model.parameters(), lr=lr)\n",
    "\n",
    "    transformer_train_losses = []\n",
    "    transformer_val_losses = []\n",
    "\n",
    "    max_early_stop_count = 3\n",
    "    early_stop_count = 0\n",
    "    transformer_best_val_loss = float('inf')\n",
    "    transformer_best_model_weights = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        transformer_model.train()\n",
    "        transformer_total_train_loss = 0\n",
    "        transformer_total_train_num = 1e-9\n",
    "        for data in train_dataloader:\n",
    "            src = data[0].to(DEVICE)\n",
    "            trg = data[1].to(DEVICE)\n",
    "\n",
    "            if (trg[0] != 0):\n",
    "                transformer_total_train_num += 1\n",
    "\n",
    "                src_mask = transformer_model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "                output = transformer_model(src, src_mask)\n",
    "\n",
    "                transformer_train_loss = criterion(output[0], trg)\n",
    "                transformer_total_train_loss += transformer_train_loss.item()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                transformer_train_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "        transformer_avg_train_loss = transformer_total_train_loss / transformer_total_train_num\n",
    "        transformer_train_losses.append(transformer_avg_train_loss)\n",
    "\n",
    "        transformer_model.eval()\n",
    "        transformer_total_val_loss = 0\n",
    "        transformer_total_val_num = 1e-9\n",
    "        with torch.no_grad():\n",
    "            for data in val_dataloader:\n",
    "                src = data[0].to(DEVICE)\n",
    "                trg = data[1].to(DEVICE)\n",
    "\n",
    "                if (trg[0] != 0):\n",
    "                    transformer_total_val_num += 1\n",
    "\n",
    "                    src_mask = transformer_model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "                    output = transformer_model(src, src_mask)\n",
    "\n",
    "                    transformer_val_loss = criterion(output[0], trg)\n",
    "                    transformer_total_val_loss += transformer_val_loss.item()\n",
    "\n",
    "        transformer_avg_val_loss = transformer_total_val_loss / transformer_total_val_num\n",
    "        transformer_val_losses.append(transformer_avg_val_loss)\n",
    "\n",
    "        if  transformer_best_val_loss > transformer_avg_val_loss:\n",
    "            transformer_best_val_loss = transformer_avg_val_loss\n",
    "            transformer_best_model_weights = copy.deepcopy(transformer_model.state_dict())\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            \n",
    "        if early_stop_count >= max_early_stop_count:\n",
    "            print(f'Transformer\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_avg_train_loss:.6f}, Val Loss: {transformer_avg_val_loss:.6f} \\nEarly Stop Triggered!')\n",
    "            transformer_model.load_state_dict(transformer_best_model_weights)\n",
    "            torch.save(transformer_model, f'../데이터/Checkpoint/transformer/transformer_lr_{lr}_batch_{batch}_sub_{sub}_ed_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "            break\n",
    "\n",
    "        print(f'Transformer\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_avg_train_loss:.6f}, Val Loss: {transformer_avg_val_loss:.6f}')\n",
    "    \n",
    "    save_train_val_losses(transformer_train_losses, transformer_val_losses, f'../데이터/Checkpoint/transformer/transformer_lr_{lr}_batch_{batch}_sub_{sub}_ed_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "\n",
    "    ### transformer attention\n",
    "    dataset = District_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, sub, DEVICE)\n",
    "    dataset_length = len(dataset)\n",
    "    train_size = int(train_ratio * dataset_length)\n",
    "    train_indices = range(0, train_size)\n",
    "    val_size = int(val_ratio * dataset_length)\n",
    "    val_indices = range(train_size, train_size + val_size)\n",
    "    # test_size = int(test_ratio * dataset_length)\n",
    "    # test_indices = range(train_size + val_size, dataset_length)\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    # test_dataset = Subset(dataset, test_indices)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "    # test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "    transformer_att_model = TransformerAttention(transformer_model, embedding_dim, 1, DEVICE).to(DEVICE)\n",
    "    criterion = RMSE()\n",
    "    optimizer = torch.optim.Adam(transformer_att_model.parameters(), lr=lr)\n",
    "\n",
    "    transformer_att_train_losses = []\n",
    "    transformer_att_val_losses = []\n",
    "\n",
    "    max_early_stop_count = 3\n",
    "    early_stop_count = 0\n",
    "    transformer_att_best_val_loss = float('inf')\n",
    "    transformer_att_best_model_weights = None\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "        transformer_att_model.train()\n",
    "        transformer_att_total_train_loss = 0\n",
    "        transformer_att_total_train_num = 1e-9\n",
    "        for data in train_dataloader:\n",
    "            src = data[0][0].to(DEVICE)\n",
    "            max_len = data[1][0].to(DEVICE)\n",
    "            anw = data[2][0].to(DEVICE)\n",
    "            trg = data[3][0].to(DEVICE)\n",
    "            \n",
    "            if len(anw) == 0:\n",
    "                continue\n",
    "            \n",
    "            transformer_att_total_train_num += len(anw)\n",
    "\n",
    "            for index in anw:\n",
    "                output = transformer_att_model(src, index, max_len)\n",
    "                \n",
    "                transformer_att_train_loss = criterion(output, trg[index])\n",
    "                transformer_att_total_train_loss += transformer_att_train_loss.item()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                transformer_att_train_loss.backward()\n",
    "                optimizer.step() \n",
    "                \n",
    "        transformer_att_avg_train_loss = transformer_att_total_train_loss / transformer_att_total_train_num\n",
    "        transformer_att_train_losses.append(transformer_att_avg_train_loss)\n",
    "\n",
    "        transformer_att_model.eval()\n",
    "        transformer_att_total_val_loss = 0\n",
    "        transformer_att_total_val_num = 1e-9\n",
    "        with torch.no_grad():\n",
    "            for data in val_dataloader:\n",
    "                src = data[0][0].to(DEVICE)\n",
    "                max_len = data[1][0].to(DEVICE)\n",
    "                anw = data[2][0].to(DEVICE)\n",
    "                trg = data[3][0].to(DEVICE)\n",
    "\n",
    "                if len(anw) == 0:\n",
    "                    continue\n",
    "                \n",
    "                transformer_att_total_val_num += len(anw)\n",
    "\n",
    "                for index in anw:\n",
    "                    output = transformer_att_model(src, index, max_len)\n",
    "\n",
    "                    transformer_att_val_loss = criterion(output, trg[index])\n",
    "                    transformer_att_total_val_loss += transformer_att_val_loss.item()\n",
    "                    \n",
    "        transformer_att_avg_val_loss = transformer_att_total_val_loss / transformer_att_total_val_num\n",
    "        transformer_att_val_losses.append(transformer_att_avg_val_loss)\n",
    "                \n",
    "        if  transformer_att_best_val_loss > transformer_att_avg_val_loss:\n",
    "            transformer_att_best_val_loss = transformer_att_avg_val_loss\n",
    "            transformer_att_best_model_weights = copy.deepcopy(transformer_att_model.state_dict())\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= max_early_stop_count:\n",
    "            print(f'Attention\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_att_avg_train_loss:.6f}, Val Loss: {transformer_att_avg_val_loss:.6f} \\nEarly Stop Triggered!')\n",
    "            transformer_att_model.load_state_dict(transformer_att_best_model_weights)\n",
    "            torch.save(transformer_att_model, f'../데이터/Checkpoint/attention/attention_lr_{lr}_batch_{batch}_sub_{sub}_ed_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "            break\n",
    "\n",
    "        print(f'Attention\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_att_avg_train_loss:.6f}, Val Loss: {transformer_att_avg_val_loss:.6f}')\n",
    "\n",
    "    save_train_val_losses(transformer_att_train_losses, transformer_att_val_losses, f'../데이터/Checkpoint/attention/attention_lr_{lr}_batch_{batch}_sub_{sub}_ed_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "\n",
    "    results_df = results_df.append({\n",
    "        'lr': lr,\n",
    "        'batch': batch,\n",
    "        'sub': sub,\n",
    "        'embedding_dim': embedding_dim,\n",
    "        'window_size': window_size,\n",
    "        'val_loss': min(transformer_att_val_losses),\n",
    "    }, ignore_index=True)\n",
    "\n",
    "results_df.to_excel('../데이터/Checkpoint/experiment_results.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_att_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer attention\n",
    "dataset = District_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, sub, DEVICE)\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "# train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "# val_indices = range(train_size, train_size + val_size)\n",
    "test_size = int(test_ratio * dataset_length)\n",
    "test_indices = range(train_size + val_size, dataset_length)\n",
    "# train_dataset = Subset(dataset, train_indices)\n",
    "# val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "transformer_att_model.eval()\n",
    "transformer_att_total_test_rmse = 0\n",
    "transformer_att_total_test_mse = 0\n",
    "transformer_att_total_test_mae = 0\n",
    "transformer_att_total_test_mape = 0\n",
    "transformer_att_total_test_num = 1e-9\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        src = data[0][0].to(DEVICE)\n",
    "        max_len = data[1][0].to(DEVICE)\n",
    "        anw = data[2][0].to(DEVICE)\n",
    "        trg = data[3][0].to(DEVICE)\n",
    "\n",
    "        if len(anw) == 0:\n",
    "            continue\n",
    "\n",
    "        transformer_att_total_test_num += len(anw)\n",
    "\n",
    "        for index in anw:\n",
    "            output = transformer_att_model(src, index, max_len)\n",
    "\n",
    "            transformer_att_test_rmse = rmse(output, trg[index])\n",
    "            transformer_att_test_mse = mse(output, trg[index])\n",
    "            transformer_att_test_mae = mae(output, trg[index])\n",
    "            transformer_att_test_mape = mape(output, trg[index])\n",
    "            \n",
    "            transformer_att_total_test_rmse += transformer_att_test_rmse.item()\n",
    "            transformer_att_total_test_mse += transformer_att_test_mse.item()\n",
    "            transformer_att_total_test_mae += transformer_att_test_mae.item()\n",
    "            transformer_att_total_test_mape += transformer_att_test_mape.item()\n",
    "            \n",
    "transformer_att_avg_test_rmse = transformer_att_total_test_rmse / transformer_att_total_test_num\n",
    "transformer_att_avg_test_mse = transformer_att_total_test_mse / transformer_att_total_test_num\n",
    "transformer_att_avg_test_mae = transformer_att_total_test_mae / transformer_att_total_test_num\n",
    "transformer_att_avg_test_mape = transformer_att_total_test_mape / transformer_att_total_test_num\n",
    "        \n",
    "print(f'Test RMSE: {transformer_att_avg_test_rmse}')\n",
    "print(f'Test MSE: {transformer_att_avg_test_mse}')\n",
    "print(f'Test MAE: {transformer_att_avg_test_mae}')\n",
    "print(f'Test MAPE: {transformer_att_avg_test_mape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
