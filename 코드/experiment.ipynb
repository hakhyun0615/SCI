{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "from Dataset.Embedding_Dataset import Embedding_Dataset\n",
    "from Model.Embedding import Embedding\n",
    "\n",
    "from Dataset.Apartment_Complex_Dataset import Apartment_Complex_Dataset\n",
    "from Model.LSTM import LSTM\n",
    "from Model.GRU import GRU\n",
    "from Model.Transformer import Transformer\n",
    "from Model.N_BEATS import NBeats\n",
    "from Model.DLinear import DLinear\n",
    "\n",
    "from Dataset.District_Dataset import District_Dataset\n",
    "from Model.LSTM_Attention import LSTMAttention\n",
    "from Model.GRU_Attention import GRUAttention\n",
    "from Model.Transformer_Attention import TransformerAttention\n",
    "\n",
    "from utils import RMSE, rmse, mse, mae, save_train_val_losses\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "table_1 = pd.read_csv('../데이터/Table/table_1.csv') \n",
    "table_2 = pd.read_csv('../데이터/Table/table_2.csv') \n",
    "table_3 = pd.read_csv('../데이터/Table/table_3.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "epochs = 10000\n",
    "lr = 1e-4\n",
    "batch = 64\n",
    "sub = True # True\n",
    "embedding_dim = 1024 # 1024\n",
    "window_size = 3 # 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\t Epoch [1/10000], Train Loss: 3.096520, Val Loss: 4.578956\n",
      "Embedding\t Epoch [2/10000], Train Loss: 0.711954, Val Loss: 3.245828\n",
      "Embedding\t Epoch [3/10000], Train Loss: 0.631292, Val Loss: 3.207866\n",
      "Embedding\t Epoch [4/10000], Train Loss: 0.579081, Val Loss: 3.143797\n",
      "Embedding\t Epoch [5/10000], Train Loss: 0.543079, Val Loss: 3.102309\n",
      "Embedding\t Epoch [6/10000], Train Loss: 0.516315, Val Loss: 3.082897\n",
      "Embedding\t Epoch [7/10000], Train Loss: 0.493515, Val Loss: 3.070941\n",
      "Embedding\t Epoch [8/10000], Train Loss: 0.474083, Val Loss: 3.071344\n",
      "Embedding\t Epoch [9/10000], Train Loss: 0.457296, Val Loss: 3.061797\n",
      "Embedding\t Epoch [10/10000], Train Loss: 0.439914, Val Loss: 3.065285\n",
      "Embedding\t Epoch [11/10000], Train Loss: 0.425424, Val Loss: 3.069661\n",
      "Embedding\t Epoch [12/10000], Train Loss: 0.412229, Val Loss: 3.051016\n",
      "Embedding\t Epoch [13/10000], Train Loss: 0.400112, Val Loss: 3.074282\n",
      "Embedding\t Epoch [14/10000], Train Loss: 0.389233, Val Loss: 3.086326\n",
      "Embedding\t Epoch [15/10000], Train Loss: 0.378998, Val Loss: 3.081447 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 0.3789982353196\n",
      "Min Val Loss: 3.051015912693866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimhakhyun/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer\t Epoch [1/10000], Train Loss: 3.937904, Val Loss: 3.910343\n",
      "Transformer\t Epoch [2/10000], Train Loss: 3.821057, Val Loss: 3.904831\n",
      "Transformer\t Epoch [3/10000], Train Loss: 3.777618, Val Loss: 3.777583\n",
      "Transformer\t Epoch [4/10000], Train Loss: 3.723065, Val Loss: 3.747514\n",
      "Transformer\t Epoch [5/10000], Train Loss: 3.707320, Val Loss: 3.760592\n",
      "Transformer\t Epoch [6/10000], Train Loss: 3.709654, Val Loss: 3.814709\n",
      "Transformer\t Epoch [7/10000], Train Loss: 3.729027, Val Loss: 3.638350\n",
      "Transformer\t Epoch [8/10000], Train Loss: 3.685845, Val Loss: 3.585972\n",
      "Transformer\t Epoch [9/10000], Train Loss: 3.669526, Val Loss: 3.582432\n",
      "Transformer\t Epoch [10/10000], Train Loss: 3.632802, Val Loss: 3.541595\n",
      "Transformer\t Epoch [11/10000], Train Loss: 3.621147, Val Loss: 3.553754\n",
      "Transformer\t Epoch [12/10000], Train Loss: 3.610913, Val Loss: 3.536692\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 129\u001b[0m\n\u001b[1;32m    126\u001b[0m         transformer_total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m transformer_train_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    128\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 129\u001b[0m         \u001b[43mtransformer_train_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    132\u001b[0m transformer_avg_train_loss \u001b[38;5;241m=\u001b[39m transformer_total_train_loss \u001b[38;5;241m/\u001b[39m transformer_total_train_num\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# result_df = pd.DataFrame(columns=['lr','batch','sub','embedding_dim','window_size','val_loss'])\n",
    "\n",
    "### embedding\n",
    "dataset = Embedding_Dataset(table_1, table_2, table_3, DEVICE)\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "val_indices = range(train_size, train_size + val_size)\n",
    "# test_size = int(test_ratio * dataset_length)\n",
    "# test_indices = range(train_size + val_size, dataset_length)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "# test_dataset = Subset(dataset, test_indices)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "embedding_model = Embedding(128, 256, 512, embedding_dim, 512, 256, 128).to(DEVICE)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(embedding_model.parameters(), lr=lr)\n",
    "\n",
    "embedding_train_losses = []\n",
    "embedding_val_losses = []\n",
    "\n",
    "max_early_stop_count = 3\n",
    "early_stop_count = 0\n",
    "embedding_best_val_loss = float('inf')\n",
    "embedding_best_model_weights = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    embedding_model.train()\n",
    "    embedding_total_train_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        input = data[0].to(DEVICE)\n",
    "        target = data[1].to(DEVICE)\n",
    "        output = embedding_model(input).to(DEVICE)\n",
    "\n",
    "        embedding_train_loss = criterion(output, target)\n",
    "        embedding_total_train_loss += embedding_train_loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embedding_train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    embedding_avg_train_loss = embedding_total_train_loss / len(train_dataloader)\n",
    "    embedding_train_losses.append(embedding_avg_train_loss)\n",
    "\n",
    "    embedding_model.eval()\n",
    "    embedding_total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            input = data[0].to(DEVICE)\n",
    "            target = data[1].to(DEVICE)\n",
    "            output = embedding_model(input).to(DEVICE)\n",
    "\n",
    "            embedding_val_loss = criterion(output, target)\n",
    "            embedding_total_val_loss += embedding_val_loss.item()\n",
    "\n",
    "    embedding_avg_val_loss = embedding_total_val_loss / len(val_dataloader)\n",
    "    embedding_val_losses.append(embedding_avg_val_loss)\n",
    "\n",
    "    if  embedding_best_val_loss > embedding_avg_val_loss:\n",
    "        embedding_best_val_loss = embedding_avg_val_loss\n",
    "        embedding_best_model_weights = copy.deepcopy(embedding_model.state_dict())\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "\n",
    "    if early_stop_count >= max_early_stop_count:\n",
    "        print(f'Embedding\\t Epoch [{epoch+1}/{epochs}], Train Loss: {embedding_avg_train_loss:.6f}, Val Loss: {embedding_avg_val_loss:.6f} \\nEarly Stop Triggered!')\n",
    "        embedding_model.load_state_dict(embedding_best_model_weights)\n",
    "        torch.save(embedding_model, f'../데이터/Checkpoint/embedding/embedding_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Embedding\\t Epoch [{epoch+1}/{epochs}], Train Loss: {embedding_avg_train_loss:.6f}, Val Loss: {embedding_avg_val_loss:.6f}')\n",
    "    \n",
    "save_train_val_losses(embedding_train_losses, embedding_val_losses, f'../데이터/Checkpoint/embedding/embedding_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "\n",
    "### transformer\n",
    "# embedding_model = 'None'\n",
    "# embedding_dim = 'None'\n",
    "dataset = Apartment_Complex_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, 'DL', DEVICE)\n",
    "# embedding_dim = 12\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "val_indices = range(train_size, train_size + val_size)\n",
    "# test_size = int(test_ratio * dataset_length)\n",
    "# test_indices = range(train_size + val_size, dataset_length)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "# test_dataset = Subset(dataset, test_indices)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "transformer_model = Transformer(embedding_dim, 1, 2, 2).to(DEVICE)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=lr)\n",
    "\n",
    "transformer_train_losses = []\n",
    "transformer_val_losses = []\n",
    "\n",
    "max_early_stop_count = 3\n",
    "early_stop_count = 0\n",
    "transformer_best_val_loss = float('inf')\n",
    "transformer_best_model_weights = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    transformer_model.train()\n",
    "    transformer_total_train_loss = 0\n",
    "    transformer_total_train_num = 1e-9\n",
    "    for data in train_dataloader:\n",
    "        src = data[0].to(DEVICE)\n",
    "        trg = data[1].to(DEVICE)\n",
    "\n",
    "        if (trg[0] != 0):\n",
    "            transformer_total_train_num += 1\n",
    "\n",
    "            src_mask = transformer_model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "            output = transformer_model(src, src_mask)\n",
    "\n",
    "            transformer_train_loss = criterion(output[0], trg)\n",
    "            transformer_total_train_loss += transformer_train_loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            transformer_train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    transformer_avg_train_loss = transformer_total_train_loss / transformer_total_train_num\n",
    "    transformer_train_losses.append(transformer_avg_train_loss)\n",
    "\n",
    "    transformer_model.eval()\n",
    "    transformer_total_val_loss = 0\n",
    "    transformer_total_val_num = 1e-9\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0].to(DEVICE)\n",
    "            trg = data[1].to(DEVICE)\n",
    "\n",
    "            if (trg[0] != 0):\n",
    "                transformer_total_val_num += 1\n",
    "\n",
    "                src_mask = transformer_model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "                output = transformer_model(src, src_mask)\n",
    "\n",
    "                transformer_val_loss = criterion(output[0], trg)\n",
    "                transformer_total_val_loss += transformer_val_loss.item()\n",
    "\n",
    "    transformer_avg_val_loss = transformer_total_val_loss / transformer_total_val_num\n",
    "    transformer_val_losses.append(transformer_avg_val_loss)\n",
    "\n",
    "    if  transformer_best_val_loss > transformer_avg_val_loss:\n",
    "        transformer_best_val_loss = transformer_avg_val_loss\n",
    "        transformer_best_model_weights = copy.deepcopy(transformer_model.state_dict())\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "        \n",
    "    if early_stop_count >= max_early_stop_count:\n",
    "        print(f'Transformer\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_avg_train_loss:.6f}, Val Loss: {transformer_avg_val_loss:.6f} \\nEarly Stop Triggered!')\n",
    "        transformer_model.load_state_dict(transformer_best_model_weights)\n",
    "        torch.save(transformer_model, f'../데이터/Checkpoint/transformer/transformer_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Transformer\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_avg_train_loss:.6f}, Val Loss: {transformer_avg_val_loss:.6f}')\n",
    "\n",
    "save_train_val_losses(transformer_train_losses, transformer_val_losses, f'../데이터/Checkpoint/transformer/transformer_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "\n",
    "# embedding_dim = 'None'\n",
    "### transformer attention\n",
    "dataset = District_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, sub, DEVICE)\n",
    "# embedding_dim = 12\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "val_indices = range(train_size, train_size + val_size)\n",
    "# test_size = int(test_ratio * dataset_length)\n",
    "# test_indices = range(train_size + val_size, dataset_length)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "# test_dataset = Subset(dataset, test_indices)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "transformer_att_model = TransformerAttention(transformer_model, embedding_dim, 1, DEVICE).to(DEVICE)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(transformer_att_model.parameters(), lr=lr)\n",
    "\n",
    "transformer_att_train_losses = []\n",
    "transformer_att_val_losses = []\n",
    "\n",
    "max_early_stop_count = 3\n",
    "early_stop_count = 0\n",
    "transformer_att_best_val_loss = float('inf')\n",
    "transformer_att_best_model_weights = None\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    transformer_att_model.train()\n",
    "    transformer_att_total_train_loss = 0\n",
    "    transformer_att_total_train_num = 1e-9\n",
    "    for data in train_dataloader:\n",
    "        src = data[0][0].to(DEVICE)\n",
    "        max_len = data[1][0].to(DEVICE)\n",
    "        try:\n",
    "            anw = torch.nonzero(data[2][0]).to(DEVICE)[0]\n",
    "        except:\n",
    "            continue\n",
    "        trg = data[3][0].to(DEVICE)\n",
    "        \n",
    "        transformer_att_total_train_num += len(anw)\n",
    "\n",
    "        for index in anw:\n",
    "            output = transformer_att_model(src, index, max_len)\n",
    "            \n",
    "            transformer_att_train_loss = criterion(output, trg[index])\n",
    "            transformer_att_total_train_loss += transformer_att_train_loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            transformer_att_train_loss.backward()\n",
    "            optimizer.step() \n",
    "            \n",
    "    transformer_att_avg_train_loss = transformer_att_total_train_loss / transformer_att_total_train_num\n",
    "    transformer_att_train_losses.append(transformer_att_avg_train_loss)\n",
    "\n",
    "    transformer_att_model.eval()\n",
    "    transformer_att_total_val_loss = 0\n",
    "    transformer_att_total_val_num = 1e-9\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0][0].to(DEVICE)\n",
    "            max_len = data[1][0].to(DEVICE)\n",
    "            try:\n",
    "                anw = torch.nonzero(data[2][0]).to(DEVICE)[0]\n",
    "            except:\n",
    "                continue\n",
    "            trg = data[3][0].to(DEVICE)\n",
    "            \n",
    "            transformer_att_total_val_num += len(anw)\n",
    "\n",
    "            for index in anw:\n",
    "                output = transformer_att_model(src, index, max_len)\n",
    "\n",
    "                transformer_att_val_loss = criterion(output, trg[index])\n",
    "                transformer_att_total_val_loss += transformer_att_val_loss.item()\n",
    "                \n",
    "    transformer_att_avg_val_loss = transformer_att_total_val_loss / transformer_att_total_val_num\n",
    "    transformer_att_val_losses.append(transformer_att_avg_val_loss)\n",
    "            \n",
    "    if  transformer_att_best_val_loss > transformer_att_avg_val_loss:\n",
    "        transformer_att_best_val_loss = transformer_att_avg_val_loss\n",
    "        transformer_att_best_model_weights = copy.deepcopy(transformer_att_model.state_dict())\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "\n",
    "    if early_stop_count >= max_early_stop_count:\n",
    "        print(f'Attention\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_att_avg_train_loss:.6f}, Val Loss: {transformer_att_avg_val_loss:.6f} \\nEarly Stop Triggered!')\n",
    "        transformer_att_model.load_state_dict(transformer_att_best_model_weights)\n",
    "        torch.save(transformer_att_model, f'../데이터/Checkpoint/transformer/attention/transformer_attention_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Attention\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_att_avg_train_loss:.6f}, Val Loss: {transformer_att_avg_val_loss:.6f}')\n",
    "\n",
    "save_train_val_losses(transformer_att_train_losses, transformer_att_val_losses, f'../데이터/Checkpoint/attention/transformer/attention_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "\n",
    "# result_df = result_df.append({\n",
    "#     'lr': lr,\n",
    "#     'batch': batch,\n",
    "#     'sub': sub,\n",
    "#     'embedding_dim': embedding_dim,\n",
    "#     'window_size': window_size,\n",
    "#     'val_loss': min(transformer_att_val_losses),\n",
    "# }, ignore_index=True)\n",
    "\n",
    "# result_df = result_df.sort_values('val_loss')\n",
    "# result_df.to_excel(f'../데이터/Checkpoint/result/result_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: torch.Size([1024, 1])\n",
      "att: torch.Size([1024])\n",
      "s: torch.Size([2048])\n",
      "query: torch.Size([1024, 1])\n",
      "att: torch.Size([1024])\n",
      "s: torch.Size([2048])\n",
      "query: torch.Size([1024, 1])\n",
      "att: torch.Size([1024])\n",
      "s: torch.Size([2048])\n",
      "query: torch.Size([1024, 1])\n",
      "att: torch.Size([1024])\n",
      "s: torch.Size([2048])\n",
      "query: torch.Size([1024, 1])\n",
      "att: torch.Size([1024])\n",
      "s: torch.Size([2048])\n",
      "query: torch.Size([1024, 1])\n",
      "att: torch.Size([1024])\n",
      "s: torch.Size([2048])\n",
      "query: torch.Size([1024, 1])\n",
      "att: torch.Size([1024])\n",
      "s: torch.Size([2048])\n",
      "query: torch.Size([1024, 1])\n",
      "att: torch.Size([1024])\n",
      "s: torch.Size([2048])\n",
      "query: torch.Size([1024, 1])\n",
      "att: torch.Size([1024])\n",
      "s: torch.Size([2048])\n",
      "query: torch.Size([1024, 1])\n",
      "att: torch.Size([1024])\n",
      "s: torch.Size([2048])\n",
      "query: torch.Size([1024, 1])\n",
      "att: torch.Size([1024])\n",
      "s: torch.Size([2048])\n",
      "query: torch.Size([1024, 1])\n",
      "att: torch.Size([1024])\n",
      "s: torch.Size([2048])\n",
      "query: torch.Size([1024, 1])\n",
      "att: torch.Size([1024])\n",
      "s: torch.Size([2048])\n",
      "query: torch.Size([1024, 1])\n",
      "att: torch.Size([1024])\n",
      "s: torch.Size([2048])\n",
      "query: torch.Size([1024, 1])\n",
      "att: torch.Size([1024])\n",
      "s: torch.Size([2048])\n",
      "query: torch.Size([1024, 1])\n",
      "att: torch.Size([1024])\n",
      "s: torch.Size([2048])\n",
      "query: torch.Size([1024, 1])\n",
      "att: torch.Size([1024])\n",
      "s: torch.Size([2048])\n",
      "query: torch.Size([1024, 1])\n",
      "att: torch.Size([1024])\n",
      "s: torch.Size([2048])\n",
      "query: torch.Size([1024, 1])\n",
      "att: torch.Size([1024])\n",
      "s: torch.Size([2048])\n",
      "query: torch.Size([1024, 1])\n",
      "att: torch.Size([1024])\n",
      "s: torch.Size([2048])\n",
      "query: torch.Size([1024, 1])\n",
      "att: torch.Size([1024])\n",
      "s: torch.Size([2048])\n",
      "Test RMSE: 3.1351\n",
      "Test MSE: 9.8290\n",
      "Test MAE: 2.0718\n"
     ]
    }
   ],
   "source": [
    "### transformer attention\n",
    "embedding_model = torch.load(\"../데이터/Checkpoint/embedding/ws_3/embedding_lr_0.0001_batch_64_sub_True_emb_1024_ws_3_epochs_13.pth\", map_location=DEVICE)\n",
    "transformer_att_model = torch.load(\"../데이터/Checkpoint/transformer/attention/ws_3/transformer_attention_lr_0.0001_batch_64_sub_True_emb_1024_ws_3_epochs_4.pth\", map_location=DEVICE)\n",
    "dataset = District_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, sub, DEVICE)\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "# train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "# val_indices = range(train_size, train_size + val_size)\n",
    "test_size = int(test_ratio * dataset_length)\n",
    "test_indices = range(train_size + val_size, dataset_length)\n",
    "# train_dataset = Subset(dataset, train_indices)\n",
    "# val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "transformer_att_model.eval()\n",
    "transformer_att_test_rmses = []\n",
    "transformer_att_test_mses = []\n",
    "transformer_att_test_maes = []\n",
    "\n",
    "transformer_att_test_outputs = []\n",
    "transformer_att_test_trgs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        src = data[0][0].to(DEVICE)\n",
    "        max_len = data[1][0].to(DEVICE)\n",
    "        try:\n",
    "            anw = torch.nonzero(data[2][0]).to(DEVICE)[0]\n",
    "        except:\n",
    "            continue\n",
    "        trg = data[3][0].to(DEVICE)\n",
    "\n",
    "        for index in anw:\n",
    "            output = transformer_att_model(src, index, max_len)\n",
    "            \n",
    "            transformer_att_test_outputs.append(output)\n",
    "            transformer_att_test_trgs.append(trg[index])\n",
    "\n",
    "save_path = f'../데이터/Checkpoint/transformer/attention/ws_3/transformer_attention_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{4}'\n",
    "with open(f'{save_path}_test_rmses.txt', 'w') as f:\n",
    "    for item in transformer_att_test_rmses:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open(f'{save_path}_test_mses.txt', 'w') as f:\n",
    "    for item in transformer_att_test_mses:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open(f'{save_path}_test_maes.txt', 'w') as f:\n",
    "    for item in transformer_att_test_maes:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "transformer_att_test_outputs = torch.FloatTensor(transformer_att_test_outputs)\n",
    "transformer_att_test_trgs = torch.FloatTensor(transformer_att_test_trgs) \n",
    "\n",
    "transformer_att_test_rmse = rmse(transformer_att_test_outputs, transformer_att_test_trgs)\n",
    "transformer_att_test_mse = mse(transformer_att_test_outputs, transformer_att_test_trgs)\n",
    "transformer_att_test_mae = mae(transformer_att_test_outputs, transformer_att_test_trgs)\n",
    "        \n",
    "print(f'Test RMSE: {transformer_att_test_rmse:.4f}')\n",
    "print(f'Test MSE: {transformer_att_test_mse:.4f}')\n",
    "print(f'Test MAE: {transformer_att_test_mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
