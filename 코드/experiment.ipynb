{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "from Dataset.Embedding_Dataset import Embedding_Dataset\n",
    "from Model.Embedding import Embedding\n",
    "\n",
    "from Dataset.Apartment_Complex_Dataset import Apartment_Complex_Dataset\n",
    "from Model.LSTM import LSTM\n",
    "from Model.GRU import GRU\n",
    "from Model.Transformer import Transformer\n",
    "\n",
    "from Dataset.District_Dataset import District_Dataset\n",
    "from Model.LSTM_Attention import LSTMAttention\n",
    "from Model.GRU_Attention import GRUAttention\n",
    "from Model.Transformer_Attention import TransformerAttention\n",
    "\n",
    "from utils import RMSE, rmse, mse, mae, save_train_val_losses\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "table_1 = pd.read_csv('../데이터/Table/table_1.csv') \n",
    "table_2 = pd.read_csv('../데이터/Table/table_2.csv') \n",
    "table_3 = pd.read_csv('../데이터/Table/table_3.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "epochs = 10000\n",
    "lr = 1e-4\n",
    "batch = 64\n",
    "sub = True # True\n",
    "embedding_dim = 1024 # 1024\n",
    "window_size = 3 # 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\t Epoch [1/10000], Train Loss: 3.101394, Val Loss: 4.661668\n",
      "Embedding\t Epoch [2/10000], Train Loss: 0.707610, Val Loss: 3.254987\n",
      "Embedding\t Epoch [3/10000], Train Loss: 0.627575, Val Loss: 3.205271\n",
      "Embedding\t Epoch [4/10000], Train Loss: 0.576125, Val Loss: 3.168624\n",
      "Embedding\t Epoch [5/10000], Train Loss: 0.540231, Val Loss: 3.122379\n",
      "Embedding\t Epoch [6/10000], Train Loss: 0.512214, Val Loss: 3.084050\n",
      "Embedding\t Epoch [7/10000], Train Loss: 0.492385, Val Loss: 3.054386\n",
      "Embedding\t Epoch [8/10000], Train Loss: 0.472176, Val Loss: 3.016980\n",
      "Embedding\t Epoch [9/10000], Train Loss: 0.455123, Val Loss: 2.990596\n",
      "Embedding\t Epoch [10/10000], Train Loss: 0.439883, Val Loss: 2.980507\n",
      "Embedding\t Epoch [11/10000], Train Loss: 0.426027, Val Loss: 3.003143\n",
      "Embedding\t Epoch [12/10000], Train Loss: 0.412224, Val Loss: 3.008355\n",
      "Embedding\t Epoch [13/10000], Train Loss: 0.399900, Val Loss: 3.005877 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 0.39989988018165934\n",
      "Min Val Loss: 2.980506809626488\n",
      "Transformer\t Epoch [1/10000], Train Loss: 3.926404, Val Loss: 3.818102\n",
      "Transformer\t Epoch [2/10000], Train Loss: 3.792444, Val Loss: 3.646528\n",
      "Transformer\t Epoch [3/10000], Train Loss: 3.745173, Val Loss: 3.738418\n",
      "Transformer\t Epoch [4/10000], Train Loss: 3.739836, Val Loss: 3.757974\n",
      "Transformer\t Epoch [5/10000], Train Loss: 3.702286, Val Loss: 3.741135 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 3.7022860439789995\n",
      "Min Val Loss: 3.6465284194573613\n",
      "Attention\t Epoch [1/10000], Train Loss: 2.906748, Val Loss: 3.804111\n",
      "Attention\t Epoch [2/10000], Train Loss: 2.580906, Val Loss: 3.607813\n",
      "Attention\t Epoch [3/10000], Train Loss: 2.533765, Val Loss: 3.395283\n",
      "Attention\t Epoch [4/10000], Train Loss: 2.529912, Val Loss: 3.276834\n",
      "Min Train Loss: 2.5299117542649534\n",
      "Min Val Loss: 3.276833853423619\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../데이터/Checkpoint/attention/transformer/attention_lr_0.0001_batch_64_sub_True_emb_1024_ws_3_epochs_4.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 269>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttention\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Epoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformer_att_avg_train_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformer_att_avg_val_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 269\u001b[0m \u001b[43msave_train_val_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_att_train_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer_att_val_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../데이터/Checkpoint/attention/transformer/attention_lr_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlr\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_batch_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbatch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_sub_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msub\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_emb_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43membedding_dim\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_ws_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mwindow_size\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_epochs_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\utils.py:32\u001b[0m, in \u001b[0;36msave_train_val_losses\u001b[1;34m(train_losses, val_losses, save_path)\u001b[0m\n\u001b[0;32m     30\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[1;32m---> 32\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_train_losses.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m train_losses:\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\pyplot.py:979\u001b[0m, in \u001b[0;36msavefig\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Figure\u001b[38;5;241m.\u001b[39msavefig)\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msavefig\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    978\u001b[0m     fig \u001b[38;5;241m=\u001b[39m gcf()\n\u001b[1;32m--> 979\u001b[0m     res \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    980\u001b[0m     fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mdraw_idle()   \u001b[38;5;66;03m# need this if 'transparent=True' to reset colors\u001b[39;00m\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\figure.py:3046\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[1;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[0;32m   3042\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes:\n\u001b[0;32m   3043\u001b[0m         stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[0;32m   3044\u001b[0m             ax\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39m_cm_set(facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m-> 3046\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mprint_figure(fname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\backend_bases.py:2319\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2316\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[0;32m   2317\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[0;32m   2318\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[1;32m-> 2319\u001b[0m         result \u001b[38;5;241m=\u001b[39m print_method(\n\u001b[0;32m   2320\u001b[0m             filename,\n\u001b[0;32m   2321\u001b[0m             facecolor\u001b[38;5;241m=\u001b[39mfacecolor,\n\u001b[0;32m   2322\u001b[0m             edgecolor\u001b[38;5;241m=\u001b[39medgecolor,\n\u001b[0;32m   2323\u001b[0m             orientation\u001b[38;5;241m=\u001b[39morientation,\n\u001b[0;32m   2324\u001b[0m             bbox_inches_restore\u001b[38;5;241m=\u001b[39m_bbox_inches_restore,\n\u001b[0;32m   2325\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2326\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\backend_bases.py:1648\u001b[0m, in \u001b[0;36m_check_savefig_extra_args.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1640\u001b[0m     _api\u001b[38;5;241m.\u001b[39mwarn_deprecated(\n\u001b[0;32m   1641\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.3\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39mname, removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.6\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1642\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m() got unexpected keyword argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1643\u001b[0m                 \u001b[38;5;241m+\u001b[39m arg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m which is no longer supported as of \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1644\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m and will become an error \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1645\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1646\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(arg)\n\u001b[1;32m-> 1648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:415\u001b[0m, in \u001b[0;36mdelete_parameter.<locals>.wrapper\u001b[1;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[0;32m    405\u001b[0m     deprecation_addendum \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf any parameter follows \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m, they should be passed as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword, not positionally.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    408\u001b[0m     warn_deprecated(\n\u001b[0;32m    409\u001b[0m         since,\n\u001b[0;32m    410\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrepr\u001b[39m(name),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    413\u001b[0m                  \u001b[38;5;28;01melse\u001b[39;00m deprecation_addendum,\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39minner_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minner_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:583\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_jpg\u001b[1;34m(self, filename_or_obj, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    581\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi))\n\u001b[0;32m    582\u001b[0m \u001b[38;5;66;03m# Drop alpha channel now.\u001b[39;00m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (Image\u001b[38;5;241m.\u001b[39mfromarray(np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba())[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m    584\u001b[0m         \u001b[38;5;241m.\u001b[39msave(filename_or_obj, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpeg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpil_kwargs))\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:2297\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2295\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2296\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2297\u001b[0m         fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw+b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2299\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2300\u001b[0m     save_handler(\u001b[38;5;28mself\u001b[39m, fp, filename)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../데이터/Checkpoint/attention/transformer/attention_lr_0.0001_batch_64_sub_True_emb_1024_ws_3_epochs_4.jpg'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# result_df = pd.DataFrame(columns=['lr','batch','sub','embedding_dim','window_size','val_loss'])\n",
    "\n",
    "### embedding\n",
    "dataset = Embedding_Dataset(table_1, table_2, table_3, DEVICE)\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "val_indices = range(train_size, train_size + val_size)\n",
    "# test_size = int(test_ratio * dataset_length)\n",
    "# test_indices = range(train_size + val_size, dataset_length)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "# test_dataset = Subset(dataset, test_indices)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "embedding_model = Embedding(128, 256, 512, embedding_dim, 512, 256, 128).to(DEVICE)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(embedding_model.parameters(), lr=lr)\n",
    "\n",
    "embedding_train_losses = []\n",
    "embedding_val_losses = []\n",
    "\n",
    "max_early_stop_count = 3\n",
    "early_stop_count = 0\n",
    "embedding_best_val_loss = float('inf')\n",
    "embedding_best_model_weights = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    embedding_model.train()\n",
    "    embedding_total_train_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        input = data[0].to(DEVICE)\n",
    "        target = data[1].to(DEVICE)\n",
    "        output = embedding_model(input).to(DEVICE)\n",
    "\n",
    "        embedding_train_loss = criterion(output, target)\n",
    "        embedding_total_train_loss += embedding_train_loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embedding_train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    embedding_avg_train_loss = embedding_total_train_loss / len(train_dataloader)\n",
    "    embedding_train_losses.append(embedding_avg_train_loss)\n",
    "\n",
    "    embedding_model.eval()\n",
    "    embedding_total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            input = data[0].to(DEVICE)\n",
    "            target = data[1].to(DEVICE)\n",
    "            output = embedding_model(input).to(DEVICE)\n",
    "\n",
    "            embedding_val_loss = criterion(output, target)\n",
    "            embedding_total_val_loss += embedding_val_loss.item()\n",
    "\n",
    "    embedding_avg_val_loss = embedding_total_val_loss / len(val_dataloader)\n",
    "    embedding_val_losses.append(embedding_avg_val_loss)\n",
    "\n",
    "    if  embedding_best_val_loss > embedding_avg_val_loss:\n",
    "        embedding_best_val_loss = embedding_avg_val_loss\n",
    "        embedding_best_model_weights = copy.deepcopy(embedding_model.state_dict())\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "\n",
    "    if early_stop_count >= max_early_stop_count:\n",
    "        print(f'Embedding\\t Epoch [{epoch+1}/{epochs}], Train Loss: {embedding_avg_train_loss:.6f}, Val Loss: {embedding_avg_val_loss:.6f} \\nEarly Stop Triggered!')\n",
    "        embedding_model.load_state_dict(embedding_best_model_weights)\n",
    "        torch.save(embedding_model, f'../데이터/Checkpoint/embedding/embedding_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Embedding\\t Epoch [{epoch+1}/{epochs}], Train Loss: {embedding_avg_train_loss:.6f}, Val Loss: {embedding_avg_val_loss:.6f}')\n",
    "    \n",
    "save_train_val_losses(embedding_train_losses, embedding_val_losses, f'../데이터/Checkpoint/embedding/embedding_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "\n",
    "### transformer\n",
    "# embedding_model = 'None'\n",
    "# embedding_dim = 'None'\n",
    "dataset = Apartment_Complex_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, 'DL', DEVICE)\n",
    "# embedding_dim = 12\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "val_indices = range(train_size, train_size + val_size)\n",
    "# test_size = int(test_ratio * dataset_length)\n",
    "# test_indices = range(train_size + val_size, dataset_length)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "# test_dataset = Subset(dataset, test_indices)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "transformer_model = Transformer(embedding_dim, 1, 2, 2).to(DEVICE)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=lr)\n",
    "\n",
    "transformer_train_losses = []\n",
    "transformer_val_losses = []\n",
    "\n",
    "max_early_stop_count = 3\n",
    "early_stop_count = 0\n",
    "transformer_best_val_loss = float('inf')\n",
    "transformer_best_model_weights = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    transformer_model.train()\n",
    "    transformer_total_train_loss = 0\n",
    "    transformer_total_train_num = 1e-9\n",
    "    for data in train_dataloader:\n",
    "        src = data[0].to(DEVICE)\n",
    "        trg = data[1].to(DEVICE)\n",
    "\n",
    "        if (trg[0] != 0):\n",
    "            transformer_total_train_num += 1\n",
    "\n",
    "            src_mask = transformer_model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "            output = transformer_model(src, src_mask)\n",
    "\n",
    "            transformer_train_loss = criterion(output[0], trg)\n",
    "            transformer_total_train_loss += transformer_train_loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            transformer_train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    transformer_avg_train_loss = transformer_total_train_loss / transformer_total_train_num\n",
    "    transformer_train_losses.append(transformer_avg_train_loss)\n",
    "\n",
    "    transformer_model.eval()\n",
    "    transformer_total_val_loss = 0\n",
    "    transformer_total_val_num = 1e-9\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0].to(DEVICE)\n",
    "            trg = data[1].to(DEVICE)\n",
    "\n",
    "            if (trg[0] != 0):\n",
    "                transformer_total_val_num += 1\n",
    "\n",
    "                src_mask = transformer_model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "                output = transformer_model(src, src_mask)\n",
    "\n",
    "                transformer_val_loss = criterion(output[0], trg)\n",
    "                transformer_total_val_loss += transformer_val_loss.item()\n",
    "\n",
    "    transformer_avg_val_loss = transformer_total_val_loss / transformer_total_val_num\n",
    "    transformer_val_losses.append(transformer_avg_val_loss)\n",
    "\n",
    "    if  transformer_best_val_loss > transformer_avg_val_loss:\n",
    "        transformer_best_val_loss = transformer_avg_val_loss\n",
    "        transformer_best_model_weights = copy.deepcopy(transformer_model.state_dict())\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "        \n",
    "    if early_stop_count >= max_early_stop_count:\n",
    "        print(f'Transformer\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_avg_train_loss:.6f}, Val Loss: {transformer_avg_val_loss:.6f} \\nEarly Stop Triggered!')\n",
    "        transformer_model.load_state_dict(transformer_best_model_weights)\n",
    "        torch.save(transformer_model, f'../데이터/Checkpoint/transformer/transformer_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Transformer\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_avg_train_loss:.6f}, Val Loss: {transformer_avg_val_loss:.6f}')\n",
    "\n",
    "save_train_val_losses(transformer_train_losses, transformer_val_losses, f'../데이터/Checkpoint/transformer/transformer_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "\n",
    "# embedding_dim = 'None'\n",
    "### transformer attention\n",
    "dataset = District_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, sub, DEVICE)\n",
    "# embedding_dim = 12\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "val_indices = range(train_size, train_size + val_size)\n",
    "# test_size = int(test_ratio * dataset_length)\n",
    "# test_indices = range(train_size + val_size, dataset_length)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "# test_dataset = Subset(dataset, test_indices)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "transformer_att_model = TransformerAttention(transformer_model, embedding_dim, 1, DEVICE).to(DEVICE)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(transformer_att_model.parameters(), lr=lr)\n",
    "\n",
    "transformer_att_train_losses = []\n",
    "transformer_att_val_losses = []\n",
    "\n",
    "max_early_stop_count = 3\n",
    "early_stop_count = 0\n",
    "transformer_att_best_val_loss = float('inf')\n",
    "transformer_att_best_model_weights = None\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    transformer_att_model.train()\n",
    "    transformer_att_total_train_loss = 0\n",
    "    transformer_att_total_train_num = 1e-9\n",
    "    for data in train_dataloader:\n",
    "        src = data[0][0].to(DEVICE)\n",
    "        max_len = data[1][0].to(DEVICE)\n",
    "        try:\n",
    "            anw = torch.nonzero(data[2][0]).to(DEVICE)[0]\n",
    "        except:\n",
    "            continue\n",
    "        trg = data[3][0].to(DEVICE)\n",
    "        \n",
    "        transformer_att_total_train_num += len(anw)\n",
    "\n",
    "        for index in anw:\n",
    "            output = transformer_att_model(src, index, max_len)\n",
    "            \n",
    "            transformer_att_train_loss = criterion(output, trg[index])\n",
    "            transformer_att_total_train_loss += transformer_att_train_loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            transformer_att_train_loss.backward()\n",
    "            optimizer.step() \n",
    "            \n",
    "    transformer_att_avg_train_loss = transformer_att_total_train_loss / transformer_att_total_train_num\n",
    "    transformer_att_train_losses.append(transformer_att_avg_train_loss)\n",
    "\n",
    "    transformer_att_model.eval()\n",
    "    transformer_att_total_val_loss = 0\n",
    "    transformer_att_total_val_num = 1e-9\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0][0].to(DEVICE)\n",
    "            max_len = data[1][0].to(DEVICE)\n",
    "            try:\n",
    "                anw = torch.nonzero(data[2][0]).to(DEVICE)[0]\n",
    "            except:\n",
    "                continue\n",
    "            trg = data[3][0].to(DEVICE)\n",
    "            \n",
    "            transformer_att_total_val_num += len(anw)\n",
    "\n",
    "            for index in anw:\n",
    "                output = transformer_att_model(src, index, max_len)\n",
    "\n",
    "                transformer_att_val_loss = criterion(output, trg[index])\n",
    "                transformer_att_total_val_loss += transformer_att_val_loss.item()\n",
    "                \n",
    "    transformer_att_avg_val_loss = transformer_att_total_val_loss / transformer_att_total_val_num\n",
    "    transformer_att_val_losses.append(transformer_att_avg_val_loss)\n",
    "            \n",
    "    if  transformer_att_best_val_loss > transformer_att_avg_val_loss:\n",
    "        transformer_att_best_val_loss = transformer_att_avg_val_loss\n",
    "        transformer_att_best_model_weights = copy.deepcopy(transformer_att_model.state_dict())\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "\n",
    "    if early_stop_count >= max_early_stop_count:\n",
    "        print(f'Attention\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_att_avg_train_loss:.6f}, Val Loss: {transformer_att_avg_val_loss:.6f} \\nEarly Stop Triggered!')\n",
    "        transformer_att_model.load_state_dict(transformer_att_best_model_weights)\n",
    "        torch.save(transformer_att_model, f'../데이터/Checkpoint/transformer/attention/transformer_attention_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Attention\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_att_avg_train_loss:.6f}, Val Loss: {transformer_att_avg_val_loss:.6f}')\n",
    "\n",
    "save_train_val_losses(transformer_att_train_losses, transformer_att_val_losses, f'../데이터/Checkpoint/attention/transformer/attention_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "\n",
    "# result_df = result_df.append({\n",
    "#     'lr': lr,\n",
    "#     'batch': batch,\n",
    "#     'sub': sub,\n",
    "#     'embedding_dim': embedding_dim,\n",
    "#     'window_size': window_size,\n",
    "#     'val_loss': min(transformer_att_val_losses),\n",
    "# }, ignore_index=True)\n",
    "\n",
    "# result_df = result_df.sort_values('val_loss')\n",
    "# result_df.to_excel(f'../데이터/Checkpoint/result/result_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 2.1724\n",
      "Test MSE: 4.7191\n",
      "Test MAE: 1.6243\n"
     ]
    }
   ],
   "source": [
    "### transformer attention\n",
    "embedding_model = torch.load(\"../데이터/Checkpoint/embedding/ws_3/embedding_lr_0.0001_batch_64_sub_True_emb_1024_ws_3_epochs_13.pth\", map_location=DEVICE)\n",
    "transformer_att_model = torch.load(\"../데이터/Checkpoint/transformer/attention/ws_3/transformer_attention_lr_0.0001_batch_64_sub_True_emb_1024_ws_3_epochs_4.pth\", map_location=DEVICE)\n",
    "dataset = District_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, sub, DEVICE)\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "# train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "# val_indices = range(train_size, train_size + val_size)\n",
    "test_size = int(test_ratio * dataset_length)\n",
    "test_indices = range(train_size + val_size, dataset_length)\n",
    "# train_dataset = Subset(dataset, train_indices)\n",
    "# val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "transformer_att_model.eval()\n",
    "transformer_att_test_rmses = []\n",
    "transformer_att_test_mses = []\n",
    "transformer_att_test_maes = []\n",
    "\n",
    "transformer_att_test_outputs = []\n",
    "transformer_att_test_trgs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        src = data[0][0].to(DEVICE)\n",
    "        max_len = data[1][0].to(DEVICE)\n",
    "        try:\n",
    "            anw = torch.nonzero(data[2][0]).to(DEVICE)[0]\n",
    "        except:\n",
    "            continue\n",
    "        trg = data[3][0].to(DEVICE)\n",
    "\n",
    "        for index in anw:\n",
    "            output = transformer_att_model(src, index, max_len)\n",
    "            \n",
    "            transformer_att_test_outputs.append(output)\n",
    "            transformer_att_test_trgs.append(trg[index])\n",
    "\n",
    "save_path = f'../데이터/Checkpoint/transformer/attention/ws_3/transformer_attention_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{4}'\n",
    "with open(f'{save_path}_test_rmses.txt', 'w') as f:\n",
    "    for item in transformer_att_test_rmses:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open(f'{save_path}_test_mses.txt', 'w') as f:\n",
    "    for item in transformer_att_test_mses:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open(f'{save_path}_test_maes.txt', 'w') as f:\n",
    "    for item in transformer_att_test_maes:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "transformer_att_test_outputs = torch.FloatTensor(transformer_att_test_outputs)\n",
    "transformer_att_test_trgs = torch.FloatTensor(transformer_att_test_trgs) \n",
    "\n",
    "transformer_att_test_rmse = rmse(transformer_att_test_outputs, transformer_att_test_trgs)\n",
    "transformer_att_test_mse = mse(transformer_att_test_outputs, transformer_att_test_trgs)\n",
    "transformer_att_test_mae = mae(transformer_att_test_outputs, transformer_att_test_trgs)\n",
    "        \n",
    "print(f'Test RMSE: {transformer_att_test_rmse:.4f}')\n",
    "print(f'Test MSE: {transformer_att_test_mse:.4f}')\n",
    "print(f'Test MAE: {transformer_att_test_mae:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
