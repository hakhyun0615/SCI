{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "from Dataset.Embedding_Dataset import Embedding_Dataset\n",
    "from Model.Embedding import Embedding\n",
    "\n",
    "from Dataset.Apartment_Complex_Dataset import Apartment_Complex_Dataset\n",
    "from Model.LSTM import LSTM\n",
    "from Model.GRU import GRU\n",
    "from Model.Transformer import Transformer\n",
    "\n",
    "from Dataset.District_Dataset import District_Dataset\n",
    "from Model.LSTM_Attention import LSTMAttention\n",
    "from Model.GRU_Attention import GRUAttention\n",
    "from Model.Transformer_Attention import TransformerAttention\n",
    "\n",
    "from utils import RMSE, rmse, mse, mae, mape, save_train_val_losses\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "table_1 = pd.read_csv('../데이터/Table/table_1.csv') \n",
    "table_2 = pd.read_csv('../데이터/Table/table_2.csv') \n",
    "table_3 = pd.read_csv('../데이터/Table/table_3.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "epochs = 10000\n",
    "lr = 1e-4\n",
    "batch = 64\n",
    "sub = True # True\n",
    "embedding_dim = 1024 # 1024\n",
    "window_size = 36 # 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\t Epoch [1/10000], Train Loss: 3.101394, Val Loss: 4.661668\n",
      "Embedding\t Epoch [2/10000], Train Loss: 0.707610, Val Loss: 3.254987\n",
      "Embedding\t Epoch [3/10000], Train Loss: 0.627575, Val Loss: 3.205271\n",
      "Embedding\t Epoch [4/10000], Train Loss: 0.576125, Val Loss: 3.168624\n",
      "Embedding\t Epoch [5/10000], Train Loss: 0.540231, Val Loss: 3.122379\n",
      "Embedding\t Epoch [6/10000], Train Loss: 0.512214, Val Loss: 3.084050\n",
      "Embedding\t Epoch [7/10000], Train Loss: 0.492385, Val Loss: 3.054386\n",
      "Embedding\t Epoch [8/10000], Train Loss: 0.472176, Val Loss: 3.016980\n",
      "Embedding\t Epoch [9/10000], Train Loss: 0.455123, Val Loss: 2.990596\n",
      "Embedding\t Epoch [10/10000], Train Loss: 0.439883, Val Loss: 2.980507\n",
      "Embedding\t Epoch [11/10000], Train Loss: 0.426027, Val Loss: 3.003143\n",
      "Embedding\t Epoch [12/10000], Train Loss: 0.412224, Val Loss: 3.008355\n",
      "Embedding\t Epoch [13/10000], Train Loss: 0.399900, Val Loss: 3.005877 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 0.39989988018165934\n",
      "Min Val Loss: 2.980506809626488\n",
      "Transformer\t Epoch [1/10000], Train Loss: 4.103902, Val Loss: 4.099857\n",
      "Transformer\t Epoch [2/10000], Train Loss: 4.053867, Val Loss: 4.043708\n",
      "Transformer\t Epoch [3/10000], Train Loss: 3.968680, Val Loss: 4.016782\n",
      "Transformer\t Epoch [4/10000], Train Loss: 3.944392, Val Loss: 4.041918\n",
      "Transformer\t Epoch [5/10000], Train Loss: 3.917449, Val Loss: 3.987929\n",
      "Transformer\t Epoch [6/10000], Train Loss: 3.875855, Val Loss: 4.022445\n",
      "Transformer\t Epoch [7/10000], Train Loss: 3.848301, Val Loss: 3.823436\n",
      "Transformer\t Epoch [8/10000], Train Loss: 3.857012, Val Loss: 3.684347\n",
      "Transformer\t Epoch [9/10000], Train Loss: 3.848409, Val Loss: 3.671438\n",
      "Transformer\t Epoch [10/10000], Train Loss: 3.805808, Val Loss: 3.676134\n",
      "Transformer\t Epoch [11/10000], Train Loss: 3.812582, Val Loss: 3.634733\n",
      "Transformer\t Epoch [12/10000], Train Loss: 3.776112, Val Loss: 3.641240\n",
      "Transformer\t Epoch [13/10000], Train Loss: 3.769883, Val Loss: 3.649311\n",
      "Transformer\t Epoch [14/10000], Train Loss: 3.765759, Val Loss: 3.598225\n",
      "Transformer\t Epoch [15/10000], Train Loss: 3.756588, Val Loss: 3.666875\n",
      "Transformer\t Epoch [16/10000], Train Loss: 3.766898, Val Loss: 3.596880\n",
      "Transformer\t Epoch [17/10000], Train Loss: 3.742509, Val Loss: 3.667286\n",
      "Transformer\t Epoch [18/10000], Train Loss: 3.759889, Val Loss: 3.660718\n",
      "Transformer\t Epoch [19/10000], Train Loss: 3.748725, Val Loss: 3.814170 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 3.7425091211602277\n",
      "Min Val Loss: 3.596879509287905\n"
     ]
    }
   ],
   "source": [
    "# result_df = pd.DataFrame(columns=['lr','batch','sub','embedding_dim','window_size','val_loss'])\n",
    "\n",
    "### embedding\n",
    "dataset = Embedding_Dataset(table_1, table_2, table_3, DEVICE)\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "val_indices = range(train_size, train_size + val_size)\n",
    "# test_size = int(test_ratio * dataset_length)\n",
    "# test_indices = range(train_size + val_size, dataset_length)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "# test_dataset = Subset(dataset, test_indices)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "embedding_model = Embedding(128, 256, 512, embedding_dim, 512, 256, 128).to(DEVICE)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(embedding_model.parameters(), lr=lr)\n",
    "\n",
    "embedding_train_losses = []\n",
    "embedding_val_losses = []\n",
    "\n",
    "max_early_stop_count = 3\n",
    "early_stop_count = 0\n",
    "embedding_best_val_loss = float('inf')\n",
    "embedding_best_model_weights = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    embedding_model.train()\n",
    "    embedding_total_train_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        input = data[0].to(DEVICE)\n",
    "        target = data[1].to(DEVICE)\n",
    "        output = embedding_model(input).to(DEVICE)\n",
    "\n",
    "        embedding_train_loss = criterion(output, target)\n",
    "        embedding_total_train_loss += embedding_train_loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embedding_train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    embedding_avg_train_loss = embedding_total_train_loss / len(train_dataloader)\n",
    "    embedding_train_losses.append(embedding_avg_train_loss)\n",
    "\n",
    "    embedding_model.eval()\n",
    "    embedding_total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            input = data[0].to(DEVICE)\n",
    "            target = data[1].to(DEVICE)\n",
    "            output = embedding_model(input).to(DEVICE)\n",
    "\n",
    "            embedding_val_loss = criterion(output, target)\n",
    "            embedding_total_val_loss += embedding_val_loss.item()\n",
    "\n",
    "    embedding_avg_val_loss = embedding_total_val_loss / len(val_dataloader)\n",
    "    embedding_val_losses.append(embedding_avg_val_loss)\n",
    "\n",
    "    if  embedding_best_val_loss > embedding_avg_val_loss:\n",
    "        embedding_best_val_loss = embedding_avg_val_loss\n",
    "        embedding_best_model_weights = copy.deepcopy(embedding_model.state_dict())\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "\n",
    "    if early_stop_count >= max_early_stop_count:\n",
    "        print(f'Embedding\\t Epoch [{epoch+1}/{epochs}], Train Loss: {embedding_avg_train_loss:.6f}, Val Loss: {embedding_avg_val_loss:.6f} \\nEarly Stop Triggered!')\n",
    "        embedding_model.load_state_dict(embedding_best_model_weights)\n",
    "        torch.save(embedding_model, f'../데이터/Checkpoint/embedding/embedding_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Embedding\\t Epoch [{epoch+1}/{epochs}], Train Loss: {embedding_avg_train_loss:.6f}, Val Loss: {embedding_avg_val_loss:.6f}')\n",
    "    \n",
    "save_train_val_losses(embedding_train_losses, embedding_val_losses, f'../데이터/Checkpoint/embedding/embedding_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "\n",
    "### transformer\n",
    "dataset = Apartment_Complex_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, 'DL', DEVICE)\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "val_indices = range(train_size, train_size + val_size)\n",
    "# test_size = int(test_ratio * dataset_length)\n",
    "# test_indices = range(train_size + val_size, dataset_length)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "# test_dataset = Subset(dataset, test_indices)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "transformer_model = Transformer(embedding_dim, 1, 2, 2).to(DEVICE)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=lr)\n",
    "\n",
    "transformer_train_losses = []\n",
    "transformer_val_losses = []\n",
    "\n",
    "max_early_stop_count = 3\n",
    "early_stop_count = 0\n",
    "transformer_best_val_loss = float('inf')\n",
    "transformer_best_model_weights = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    transformer_model.train()\n",
    "    transformer_total_train_loss = 0\n",
    "    transformer_total_train_num = 1e-9\n",
    "    for data in train_dataloader:\n",
    "        src = data[0].to(DEVICE)\n",
    "        trg = data[1].to(DEVICE)\n",
    "\n",
    "        if (trg[0] != 0):\n",
    "            transformer_total_train_num += 1\n",
    "\n",
    "            src_mask = transformer_model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "            output = transformer_model(src, src_mask)\n",
    "\n",
    "            transformer_train_loss = criterion(output[0], trg)\n",
    "            transformer_total_train_loss += transformer_train_loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            transformer_train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    transformer_avg_train_loss = transformer_total_train_loss / transformer_total_train_num\n",
    "    transformer_train_losses.append(transformer_avg_train_loss)\n",
    "\n",
    "    transformer_model.eval()\n",
    "    transformer_total_val_loss = 0\n",
    "    transformer_total_val_num = 1e-9\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0].to(DEVICE)\n",
    "            trg = data[1].to(DEVICE)\n",
    "\n",
    "            if (trg[0] != 0):\n",
    "                transformer_total_val_num += 1\n",
    "\n",
    "                src_mask = transformer_model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "                output = transformer_model(src, src_mask)\n",
    "\n",
    "                transformer_val_loss = criterion(output[0], trg)\n",
    "                transformer_total_val_loss += transformer_val_loss.item()\n",
    "\n",
    "    transformer_avg_val_loss = transformer_total_val_loss / transformer_total_val_num\n",
    "    transformer_val_losses.append(transformer_avg_val_loss)\n",
    "\n",
    "    if  transformer_best_val_loss > transformer_avg_val_loss:\n",
    "        transformer_best_val_loss = transformer_avg_val_loss\n",
    "        transformer_best_model_weights = copy.deepcopy(transformer_model.state_dict())\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "        \n",
    "    if early_stop_count >= max_early_stop_count:\n",
    "        print(f'Transformer\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_avg_train_loss:.6f}, Val Loss: {transformer_avg_val_loss:.6f} \\nEarly Stop Triggered!')\n",
    "        transformer_model.load_state_dict(transformer_best_model_weights)\n",
    "        torch.save(transformer_model, f'../데이터/Checkpoint/transformer/transformer_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Transformer\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_avg_train_loss:.6f}, Val Loss: {transformer_avg_val_loss:.6f}')\n",
    "\n",
    "save_train_val_losses(transformer_train_losses, transformer_val_losses, f'../데이터/Checkpoint/transformer/transformer_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "\n",
    "### transformer attention\n",
    "dataset = District_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, sub, DEVICE)\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "val_indices = range(train_size, train_size + val_size)\n",
    "# test_size = int(test_ratio * dataset_length)\n",
    "# test_indices = range(train_size + val_size, dataset_length)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "# test_dataset = Subset(dataset, test_indices)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "transformer_att_model = TransformerAttention(transformer_model, embedding_dim, 1, DEVICE).to(DEVICE)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(transformer_att_model.parameters(), lr=lr)\n",
    "\n",
    "transformer_att_train_losses = []\n",
    "transformer_att_val_losses = []\n",
    "\n",
    "max_early_stop_count = 3\n",
    "early_stop_count = 0\n",
    "transformer_att_best_val_loss = float('inf')\n",
    "transformer_att_best_model_weights = None\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    transformer_att_model.train()\n",
    "    transformer_att_total_train_loss = 0\n",
    "    transformer_att_total_train_num = 1e-9\n",
    "    for data in train_dataloader:\n",
    "        src = data[0][0].to(DEVICE)\n",
    "        max_len = data[1][0].to(DEVICE)\n",
    "        try:\n",
    "            anw = torch.nonzero(data[2][0]).to(DEVICE)[0]\n",
    "        except:\n",
    "            continue\n",
    "        trg = data[3][0].to(DEVICE)\n",
    "        \n",
    "        transformer_att_total_train_num += len(anw)\n",
    "\n",
    "        for index in anw:\n",
    "            output = transformer_att_model(src, index, max_len)\n",
    "            \n",
    "            transformer_att_train_loss = criterion(output, trg[index])\n",
    "            transformer_att_total_train_loss += transformer_att_train_loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            transformer_att_train_loss.backward()\n",
    "            optimizer.step() \n",
    "            \n",
    "    transformer_att_avg_train_loss = transformer_att_total_train_loss / transformer_att_total_train_num\n",
    "    transformer_att_train_losses.append(transformer_att_avg_train_loss)\n",
    "\n",
    "    transformer_att_model.eval()\n",
    "    transformer_att_total_val_loss = 0\n",
    "    transformer_att_total_val_num = 1e-9\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0][0].to(DEVICE)\n",
    "            max_len = data[1][0].to(DEVICE)\n",
    "            try:\n",
    "                anw = torch.nonzero(data[2][0]).to(DEVICE)[0]\n",
    "            except:\n",
    "                continue\n",
    "            trg = data[3][0].to(DEVICE)\n",
    "            \n",
    "            transformer_att_total_val_num += len(anw)\n",
    "\n",
    "            for index in anw:\n",
    "                output = transformer_att_model(src, index, max_len)\n",
    "\n",
    "                transformer_att_val_loss = criterion(output, trg[index])\n",
    "                transformer_att_total_val_loss += transformer_att_val_loss.item()\n",
    "                \n",
    "    transformer_att_avg_val_loss = transformer_att_total_val_loss / transformer_att_total_val_num\n",
    "    transformer_att_val_losses.append(transformer_att_avg_val_loss)\n",
    "            \n",
    "    if  transformer_att_best_val_loss > transformer_att_avg_val_loss:\n",
    "        transformer_att_best_val_loss = transformer_att_avg_val_loss\n",
    "        transformer_att_best_model_weights = copy.deepcopy(transformer_att_model.state_dict())\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "\n",
    "    if early_stop_count >= max_early_stop_count:\n",
    "        print(f'Attention\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_att_avg_train_loss:.6f}, Val Loss: {transformer_att_avg_val_loss:.6f} \\nEarly Stop Triggered!')\n",
    "        transformer_att_model.load_state_dict(transformer_att_best_model_weights)\n",
    "        torch.save(transformer_att_model, f'../데이터/Checkpoint/attention/attention_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Attention\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_att_avg_train_loss:.6f}, Val Loss: {transformer_att_avg_val_loss:.6f}')\n",
    "\n",
    "save_train_val_losses(transformer_att_train_losses, transformer_att_val_losses, f'../데이터/Checkpoint/attention/attention_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "\n",
    "# result_df = result_df.append({\n",
    "#     'lr': lr,\n",
    "#     'batch': batch,\n",
    "#     'sub': sub,\n",
    "#     'embedding_dim': embedding_dim,\n",
    "#     'window_size': window_size,\n",
    "#     'val_loss': min(transformer_att_val_losses),\n",
    "# }, ignore_index=True)\n",
    "\n",
    "# result_df = result_df.sort_values('val_loss')\n",
    "# result_df.to_excel(f'../데이터/Checkpoint/result/result_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = torch.load('../데이터/Checkpoint/embedding/embedding_lr_0.0001_batch_64_sub_True_emb_1024_ws_36_epochs_13.pth')\n",
    "transformer_model = torch.load('../데이터/Checkpoint/transformer/transformer_lr_0.0001_batch_64_sub_True_emb_1024_ws_36_epochs_19.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### transformer attention\n",
    "dataset = District_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, sub, DEVICE)\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "val_indices = range(train_size, train_size + val_size)\n",
    "# test_size = int(test_ratio * dataset_length)\n",
    "# test_indices = range(train_size + val_size, dataset_length)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "# test_dataset = Subset(dataset, test_indices)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "transformer_att_model = TransformerAttention(transformer_model, embedding_dim, 1, DEVICE).to(DEVICE)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(transformer_att_model.parameters(), lr=lr)\n",
    "\n",
    "transformer_att_train_losses = []\n",
    "transformer_att_val_losses = []\n",
    "\n",
    "max_early_stop_count = 3\n",
    "early_stop_count = 0\n",
    "transformer_att_best_val_loss = float('inf')\n",
    "transformer_att_best_model_weights = None\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    transformer_att_model.train()\n",
    "    transformer_att_total_train_loss = 0\n",
    "    transformer_att_total_train_num = 1e-9\n",
    "    for data in train_dataloader:\n",
    "        src = data[0][0].to(DEVICE)\n",
    "        max_len = data[1][0].to(DEVICE)\n",
    "        try:\n",
    "            anw = torch.nonzero(data[2][0]).to(DEVICE)[0]\n",
    "        except:\n",
    "            continue\n",
    "        trg = data[3][0].to(DEVICE)\n",
    "        \n",
    "        transformer_att_total_train_num += len(anw)\n",
    "\n",
    "        for index in anw:\n",
    "            output = transformer_att_model(src, index, max_len)\n",
    "            \n",
    "            transformer_att_train_loss = criterion(output, trg[index])\n",
    "            transformer_att_total_train_loss += transformer_att_train_loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            transformer_att_train_loss.backward()\n",
    "            optimizer.step() \n",
    "            \n",
    "    transformer_att_avg_train_loss = transformer_att_total_train_loss / transformer_att_total_train_num\n",
    "    transformer_att_train_losses.append(transformer_att_avg_train_loss)\n",
    "\n",
    "    transformer_att_model.eval()\n",
    "    transformer_att_total_val_loss = 0\n",
    "    transformer_att_total_val_num = 1e-9\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0][0].to(DEVICE)\n",
    "            max_len = data[1][0].to(DEVICE)\n",
    "            try:\n",
    "                anw = torch.nonzero(data[2][0]).to(DEVICE)[0]\n",
    "            except:\n",
    "                continue\n",
    "            trg = data[3][0].to(DEVICE)\n",
    "            \n",
    "            transformer_att_total_val_num += len(anw)\n",
    "\n",
    "            for index in anw:\n",
    "                output = transformer_att_model(src, index, max_len)\n",
    "\n",
    "                transformer_att_val_loss = criterion(output, trg[index])\n",
    "                transformer_att_total_val_loss += transformer_att_val_loss.item()\n",
    "                \n",
    "    transformer_att_avg_val_loss = transformer_att_total_val_loss / transformer_att_total_val_num\n",
    "    transformer_att_val_losses.append(transformer_att_avg_val_loss)\n",
    "            \n",
    "    if  transformer_att_best_val_loss > transformer_att_avg_val_loss:\n",
    "        transformer_att_best_val_loss = transformer_att_avg_val_loss\n",
    "        transformer_att_best_model_weights = copy.deepcopy(transformer_att_model.state_dict())\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "\n",
    "    if early_stop_count >= max_early_stop_count:\n",
    "        print(f'Attention\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_att_avg_train_loss:.6f}, Val Loss: {transformer_att_avg_val_loss:.6f} \\nEarly Stop Triggered!')\n",
    "        transformer_att_model.load_state_dict(transformer_att_best_model_weights)\n",
    "        torch.save(transformer_att_model, f'../데이터/Checkpoint/attention/attention_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Attention\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_att_avg_train_loss:.6f}, Val Loss: {transformer_att_avg_val_loss:.6f}')\n",
    "\n",
    "save_train_val_losses(transformer_att_train_losses, transformer_att_val_losses, f'../데이터/Checkpoint/attention/attention_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = torch.load(\"../데이터/Checkpoint/embedding/embedding_lr_0.0001_batch_64_sub_True_emb_1024_ws_12_epochs_13.pth\", map_location=DEVICE)\n",
    "transformer_att_model = torch.load(\"../데이터/Checkpoint/attention/attention_lr_0.0001_batch_64_sub_True_emb_1024_ws_12_epochs_5.pth\", map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 1.9205842983155024\n",
      "Test MSE: 6.093944797596516\n",
      "Test MAPE: 33.00446767047112\n"
     ]
    }
   ],
   "source": [
    "# transformer attention\n",
    "dataset = District_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, sub, DEVICE)\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "# train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "# val_indices = range(train_size, train_size + val_size)\n",
    "test_size = int(test_ratio * dataset_length)\n",
    "test_indices = range(train_size + val_size, dataset_length)\n",
    "# train_dataset = Subset(dataset, train_indices)\n",
    "# val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, drop_last=True)\n",
    "\n",
    "transformer_att_model.eval()\n",
    "transformer_att_test_rmses = []\n",
    "transformer_att_test_mses = []\n",
    "transformer_att_test_mapes = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        src = data[0][0].to(DEVICE)\n",
    "        max_len = data[1][0].to(DEVICE)\n",
    "        try:\n",
    "            anw = torch.nonzero(data[2][0]).to(DEVICE)[0]\n",
    "        except:\n",
    "            continue\n",
    "        trg = data[3][0].to(DEVICE)\n",
    "\n",
    "        for index in anw:\n",
    "            output = transformer_att_model(src, index, max_len)\n",
    "\n",
    "            transformer_att_test_rmse = rmse(output, trg[index])\n",
    "            transformer_att_test_mse = mse(output, trg[index])\n",
    "            transformer_att_test_mape = mape(output, trg[index])\n",
    "            \n",
    "            transformer_att_test_rmses.append(transformer_att_test_rmse.item())\n",
    "            transformer_att_test_mses.append(transformer_att_test_mse.item())\n",
    "            transformer_att_test_mapes.append(transformer_att_test_mape.item())\n",
    "\n",
    "transformer_att_avg_test_rmse = sum(transformer_att_test_rmses) / len(transformer_att_test_rmses)\n",
    "transformer_att_avg_test_mse = sum(transformer_att_test_mses) / len(transformer_att_test_mses)\n",
    "transformer_att_avg_test_mape = sum(transformer_att_test_mapes) / len(transformer_att_test_mapes)\n",
    "        \n",
    "print(f'Test RMSE: {transformer_att_avg_test_rmse}')\n",
    "print(f'Test MSE: {transformer_att_avg_test_mse}')\n",
    "print(f'Test MAPE: {transformer_att_avg_test_mape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_1160\\2884725520.py:13: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"k\" (-> color=(0.0, 0.0, 0.0, 1)). The keyword argument will take precedence.\n",
      "  plt.plot(x, p, 'k', linewidth=2, color='orange')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(transformer_att_test_rmses, bins=30, density=True, color='white', alpha=0.5)\n",
    "\n",
    "mu, std = norm.fit(transformer_att_test_rmses)\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(0, 10)\n",
    "p = norm.pdf(x, mu, std)\n",
    "\n",
    "plt.plot(x, p, 'k', linewidth=2, color='orange')\n",
    "plt.fill_between(x, p, color='orange', alpha=0.5)\n",
    "\n",
    "plt.axvline(mu, color='k', linestyle='dashed', linewidth=1)\n",
    "plt.text(mu + 0.1, max(p) + 0.05, 'mean = {:.2f}'.format(mu), color='k')\n",
    "\n",
    "plt.legend(['TSA-GA Net'], loc='upper right')\n",
    "plt.xlabel('General Error(R_pre)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
