{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "from Dataset.Embedding_Dataset import Embedding_Dataset\n",
    "from Model.Embedding import Embedding\n",
    "\n",
    "from Dataset.Apartment_Complex_Dataset import Apartment_Complex_Dataset\n",
    "from Model.LSTM import LSTM\n",
    "from Model.GRU import GRU\n",
    "from Model.Transformer import Transformer\n",
    "\n",
    "from Dataset.District_Dataset import District_Dataset\n",
    "from Model.LSTM_Attention import LSTMAttention\n",
    "from Model.GRU_Attention import GRUAttention\n",
    "from Model.Transformer_Attention import TransformerAttention\n",
    "\n",
    "from utils import RMSE, rmse, mse, mae, save_train_val_losses\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "table_1 = pd.read_csv('../데이터/Table/table_1.csv') \n",
    "table_2 = pd.read_csv('../데이터/Table/table_2.csv') \n",
    "table_3 = pd.read_csv('../데이터/Table/table_3.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "epochs = 10000\n",
    "lr = 1e-4\n",
    "batch = 64\n",
    "sub = True # True\n",
    "embedding_dim = 1024 # 1024\n",
    "window_size = 6 # 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer\t Epoch [1/10000], Train Loss: 4.674466, Val Loss: 4.605999\n",
      "Transformer\t Epoch [2/10000], Train Loss: 4.110735, Val Loss: 4.142936\n",
      "Transformer\t Epoch [3/10000], Train Loss: 3.989852, Val Loss: 4.049670\n",
      "Transformer\t Epoch [4/10000], Train Loss: 3.955190, Val Loss: 4.036336\n",
      "Transformer\t Epoch [5/10000], Train Loss: 3.943029, Val Loss: 4.039034\n",
      "Transformer\t Epoch [6/10000], Train Loss: 3.932531, Val Loss: 4.038503\n",
      "Transformer\t Epoch [7/10000], Train Loss: 3.918950, Val Loss: 4.023873\n",
      "Transformer\t Epoch [8/10000], Train Loss: 3.913451, Val Loss: 4.031286\n",
      "Transformer\t Epoch [9/10000], Train Loss: 3.908846, Val Loss: 4.033645\n",
      "Transformer\t Epoch [10/10000], Train Loss: 3.894617, Val Loss: 4.026731 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 3.894617406596206\n",
      "Min Val Loss: 4.023873405857354\n",
      "Attention\t Epoch [1/10000], Train Loss: 6.575953, Val Loss: 6.890311\n",
      "Attention\t Epoch [2/10000], Train Loss: 5.153767, Val Loss: 5.614050\n",
      "Attention\t Epoch [3/10000], Train Loss: 4.283476, Val Loss: 5.150592\n",
      "Attention\t Epoch [4/10000], Train Loss: 3.901044, Val Loss: 4.855539\n",
      "Attention\t Epoch [5/10000], Train Loss: 3.628932, Val Loss: 4.608900\n",
      "Attention\t Epoch [6/10000], Train Loss: 3.412063, Val Loss: 4.405860\n",
      "Attention\t Epoch [7/10000], Train Loss: 3.261314, Val Loss: 4.239838\n",
      "Attention\t Epoch [8/10000], Train Loss: 3.150385, Val Loss: 4.102174\n",
      "Attention\t Epoch [9/10000], Train Loss: 3.060089, Val Loss: 3.986430\n",
      "Min Train Loss: 3.060088994464267\n",
      "Min Val Loss: 3.9864297264258575\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# result_df = pd.DataFrame(columns=['lr','batch','sub','embedding_dim','window_size','val_loss'])\n",
    "\n",
    "### embedding\n",
    "dataset = Embedding_Dataset(table_1, table_2, table_3, DEVICE)\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "val_indices = range(train_size, train_size + val_size)\n",
    "# test_size = int(test_ratio * dataset_length)\n",
    "# test_indices = range(train_size + val_size, dataset_length)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "# test_dataset = Subset(dataset, test_indices)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "embedding_model = Embedding(128, 256, 512, embedding_dim, 512, 256, 128).to(DEVICE)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(embedding_model.parameters(), lr=lr)\n",
    "\n",
    "embedding_train_losses = []\n",
    "embedding_val_losses = []\n",
    "\n",
    "max_early_stop_count = 3\n",
    "early_stop_count = 0\n",
    "embedding_best_val_loss = float('inf')\n",
    "embedding_best_model_weights = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    embedding_model.train()\n",
    "    embedding_total_train_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        input = data[0].to(DEVICE)\n",
    "        target = data[1].to(DEVICE)\n",
    "        output = embedding_model(input).to(DEVICE)\n",
    "\n",
    "        embedding_train_loss = criterion(output, target)\n",
    "        embedding_total_train_loss += embedding_train_loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embedding_train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    embedding_avg_train_loss = embedding_total_train_loss / len(train_dataloader)\n",
    "    embedding_train_losses.append(embedding_avg_train_loss)\n",
    "\n",
    "    embedding_model.eval()\n",
    "    embedding_total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            input = data[0].to(DEVICE)\n",
    "            target = data[1].to(DEVICE)\n",
    "            output = embedding_model(input).to(DEVICE)\n",
    "\n",
    "            embedding_val_loss = criterion(output, target)\n",
    "            embedding_total_val_loss += embedding_val_loss.item()\n",
    "\n",
    "    embedding_avg_val_loss = embedding_total_val_loss / len(val_dataloader)\n",
    "    embedding_val_losses.append(embedding_avg_val_loss)\n",
    "\n",
    "    if  embedding_best_val_loss > embedding_avg_val_loss:\n",
    "        embedding_best_val_loss = embedding_avg_val_loss\n",
    "        embedding_best_model_weights = copy.deepcopy(embedding_model.state_dict())\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "\n",
    "    if early_stop_count >= max_early_stop_count:\n",
    "        print(f'Embedding\\t Epoch [{epoch+1}/{epochs}], Train Loss: {embedding_avg_train_loss:.6f}, Val Loss: {embedding_avg_val_loss:.6f} \\nEarly Stop Triggered!')\n",
    "        embedding_model.load_state_dict(embedding_best_model_weights)\n",
    "        torch.save(embedding_model, f'../데이터/Checkpoint/embedding/embedding_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Embedding\\t Epoch [{epoch+1}/{epochs}], Train Loss: {embedding_avg_train_loss:.6f}, Val Loss: {embedding_avg_val_loss:.6f}')\n",
    "    \n",
    "save_train_val_losses(embedding_train_losses, embedding_val_losses, f'../데이터/Checkpoint/embedding/embedding_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "\n",
    "### transformer\n",
    "# embedding_model = 'None'\n",
    "# embedding_dim = 'None'\n",
    "dataset = Apartment_Complex_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, 'DL', DEVICE)\n",
    "# embedding_dim = 12\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "val_indices = range(train_size, train_size + val_size)\n",
    "# test_size = int(test_ratio * dataset_length)\n",
    "# test_indices = range(train_size + val_size, dataset_length)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "# test_dataset = Subset(dataset, test_indices)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "transformer_model = Transformer(embedding_dim, 1, 2, 2).to(DEVICE)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=lr)\n",
    "\n",
    "transformer_train_losses = []\n",
    "transformer_val_losses = []\n",
    "\n",
    "max_early_stop_count = 3\n",
    "early_stop_count = 0\n",
    "transformer_best_val_loss = float('inf')\n",
    "transformer_best_model_weights = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    transformer_model.train()\n",
    "    transformer_total_train_loss = 0\n",
    "    transformer_total_train_num = 1e-9\n",
    "    for data in train_dataloader:\n",
    "        src = data[0].to(DEVICE)\n",
    "        trg = data[1].to(DEVICE)\n",
    "\n",
    "        if (trg[0] != 0):\n",
    "            transformer_total_train_num += 1\n",
    "\n",
    "            src_mask = transformer_model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "            output = transformer_model(src, src_mask)\n",
    "\n",
    "            transformer_train_loss = criterion(output[0], trg)\n",
    "            transformer_total_train_loss += transformer_train_loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            transformer_train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    transformer_avg_train_loss = transformer_total_train_loss / transformer_total_train_num\n",
    "    transformer_train_losses.append(transformer_avg_train_loss)\n",
    "\n",
    "    transformer_model.eval()\n",
    "    transformer_total_val_loss = 0\n",
    "    transformer_total_val_num = 1e-9\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0].to(DEVICE)\n",
    "            trg = data[1].to(DEVICE)\n",
    "\n",
    "            if (trg[0] != 0):\n",
    "                transformer_total_val_num += 1\n",
    "\n",
    "                src_mask = transformer_model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "                output = transformer_model(src, src_mask)\n",
    "\n",
    "                transformer_val_loss = criterion(output[0], trg)\n",
    "                transformer_total_val_loss += transformer_val_loss.item()\n",
    "\n",
    "    transformer_avg_val_loss = transformer_total_val_loss / transformer_total_val_num\n",
    "    transformer_val_losses.append(transformer_avg_val_loss)\n",
    "\n",
    "    if  transformer_best_val_loss > transformer_avg_val_loss:\n",
    "        transformer_best_val_loss = transformer_avg_val_loss\n",
    "        transformer_best_model_weights = copy.deepcopy(transformer_model.state_dict())\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "        \n",
    "    if early_stop_count >= max_early_stop_count:\n",
    "        print(f'Transformer\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_avg_train_loss:.6f}, Val Loss: {transformer_avg_val_loss:.6f} \\nEarly Stop Triggered!')\n",
    "        transformer_model.load_state_dict(transformer_best_model_weights)\n",
    "        torch.save(transformer_model, f'../데이터/Checkpoint/transformer/transformer_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Transformer\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_avg_train_loss:.6f}, Val Loss: {transformer_avg_val_loss:.6f}')\n",
    "\n",
    "save_train_val_losses(transformer_train_losses, transformer_val_losses, f'../데이터/Checkpoint/transformer/transformer_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "\n",
    "# embedding_dim = 'None'\n",
    "### transformer attention\n",
    "dataset = District_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, sub, DEVICE)\n",
    "# embedding_dim = 12\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "val_indices = range(train_size, train_size + val_size)\n",
    "# test_size = int(test_ratio * dataset_length)\n",
    "# test_indices = range(train_size + val_size, dataset_length)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "# test_dataset = Subset(dataset, test_indices)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "transformer_att_model = TransformerAttention(transformer_model, embedding_dim, 1, DEVICE).to(DEVICE)\n",
    "criterion = RMSE()\n",
    "optimizer = torch.optim.Adam(transformer_att_model.parameters(), lr=lr)\n",
    "\n",
    "transformer_att_train_losses = []\n",
    "transformer_att_val_losses = []\n",
    "\n",
    "max_early_stop_count = 3\n",
    "early_stop_count = 0\n",
    "transformer_att_best_val_loss = float('inf')\n",
    "transformer_att_best_model_weights = None\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    transformer_att_model.train()\n",
    "    transformer_att_total_train_loss = 0\n",
    "    transformer_att_total_train_num = 1e-9\n",
    "    for data in train_dataloader:\n",
    "        src = data[0][0].to(DEVICE)\n",
    "        max_len = data[1][0].to(DEVICE)\n",
    "        try:\n",
    "            anw = torch.nonzero(data[2][0]).to(DEVICE)[0]\n",
    "        except:\n",
    "            continue\n",
    "        trg = data[3][0].to(DEVICE)\n",
    "        \n",
    "        transformer_att_total_train_num += len(anw)\n",
    "\n",
    "        for index in anw:\n",
    "            output = transformer_att_model(src, index, max_len)\n",
    "            \n",
    "            transformer_att_train_loss = criterion(output, trg[index])\n",
    "            transformer_att_total_train_loss += transformer_att_train_loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            transformer_att_train_loss.backward()\n",
    "            optimizer.step() \n",
    "            \n",
    "    transformer_att_avg_train_loss = transformer_att_total_train_loss / transformer_att_total_train_num\n",
    "    transformer_att_train_losses.append(transformer_att_avg_train_loss)\n",
    "\n",
    "    transformer_att_model.eval()\n",
    "    transformer_att_total_val_loss = 0\n",
    "    transformer_att_total_val_num = 1e-9\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            src = data[0][0].to(DEVICE)\n",
    "            max_len = data[1][0].to(DEVICE)\n",
    "            try:\n",
    "                anw = torch.nonzero(data[2][0]).to(DEVICE)[0]\n",
    "            except:\n",
    "                continue\n",
    "            trg = data[3][0].to(DEVICE)\n",
    "            \n",
    "            transformer_att_total_val_num += len(anw)\n",
    "\n",
    "            for index in anw:\n",
    "                output = transformer_att_model(src, index, max_len)\n",
    "\n",
    "                transformer_att_val_loss = criterion(output, trg[index])\n",
    "                transformer_att_total_val_loss += transformer_att_val_loss.item()\n",
    "                \n",
    "    transformer_att_avg_val_loss = transformer_att_total_val_loss / transformer_att_total_val_num\n",
    "    transformer_att_val_losses.append(transformer_att_avg_val_loss)\n",
    "            \n",
    "    if  transformer_att_best_val_loss > transformer_att_avg_val_loss:\n",
    "        transformer_att_best_val_loss = transformer_att_avg_val_loss\n",
    "        transformer_att_best_model_weights = copy.deepcopy(transformer_att_model.state_dict())\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "\n",
    "    if early_stop_count >= max_early_stop_count:\n",
    "        print(f'Attention\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_att_avg_train_loss:.6f}, Val Loss: {transformer_att_avg_val_loss:.6f} \\nEarly Stop Triggered!')\n",
    "        transformer_att_model.load_state_dict(transformer_att_best_model_weights)\n",
    "        torch.save(transformer_att_model, f'../데이터/Checkpoint/transformer/attention/attention_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "        break\n",
    "\n",
    "    print(f'Attention\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_att_avg_train_loss:.6f}, Val Loss: {transformer_att_avg_val_loss:.6f}')\n",
    "\n",
    "save_train_val_losses(transformer_att_train_losses, transformer_att_val_losses, f'../데이터/Checkpoint/attention/transformer/attention_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "\n",
    "# result_df = result_df.append({\n",
    "#     'lr': lr,\n",
    "#     'batch': batch,\n",
    "#     'sub': sub,\n",
    "#     'embedding_dim': embedding_dim,\n",
    "#     'window_size': window_size,\n",
    "#     'val_loss': min(transformer_att_val_losses),\n",
    "# }, ignore_index=True)\n",
    "\n",
    "# result_df = result_df.sort_values('val_loss')\n",
    "# result_df.to_excel(f'../데이터/Checkpoint/result/result_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer_att_model, f'../데이터/Checkpoint/attention/attention_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 2.1812\n",
      "Test MSE: 4.7578\n",
      "Test MAE: 1.7245\n"
     ]
    }
   ],
   "source": [
    "### transformer attention\n",
    "embedding_model = torch.load(\"../데이터/Checkpoint/embedding/ws_18/embedding_lr_0.0001_batch_64_sub_True_emb_1024_ws_18_epochs_13.pth\", map_location=DEVICE)\n",
    "transformer_att_model = torch.load(\"../데이터/Checkpoint/transformer/attention/ws_18/transformer_attention_lr_0.0001_batch_64_sub_True_emb_1024_ws_18_epochs_9.pth\", map_location=DEVICE)\n",
    "dataset = District_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, sub, DEVICE)\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "# train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "# val_indices = range(train_size, train_size + val_size)\n",
    "test_size = int(test_ratio * dataset_length)\n",
    "test_indices = range(train_size + val_size, dataset_length)\n",
    "# train_dataset = Subset(dataset, train_indices)\n",
    "# val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "transformer_att_model.eval()\n",
    "transformer_att_test_rmses = []\n",
    "transformer_att_test_mses = []\n",
    "transformer_att_test_maes = []\n",
    "\n",
    "transformer_att_test_outputs = []\n",
    "transformer_att_test_trgs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        src = data[0][0].to(DEVICE)\n",
    "        max_len = data[1][0].to(DEVICE)\n",
    "        try:\n",
    "            anw = torch.nonzero(data[2][0]).to(DEVICE)[0]\n",
    "        except:\n",
    "            continue\n",
    "        trg = data[3][0].to(DEVICE)\n",
    "\n",
    "        for index in anw:\n",
    "            output = transformer_att_model(src, index, max_len)\n",
    "            \n",
    "            transformer_att_test_outputs.append(output)\n",
    "            transformer_att_test_trgs.append(trg[index])\n",
    "\n",
    "save_path = f'../데이터/Checkpoint/transformer/attention/ws_18/transformer_attention_lr_{lr}_batch_{batch}_sub_{sub}_emb_{embedding_dim}_ws_{window_size}_epochs_{9}'\n",
    "with open(f'{save_path}_test_rmses.txt', 'w') as f:\n",
    "    for item in transformer_att_test_rmses:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open(f'{save_path}_test_mses.txt', 'w') as f:\n",
    "    for item in transformer_att_test_mses:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open(f'{save_path}_test_maes.txt', 'w') as f:\n",
    "    for item in transformer_att_test_maes:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "transformer_att_test_outputs = torch.FloatTensor(transformer_att_test_outputs)\n",
    "transformer_att_test_trgs = torch.FloatTensor(transformer_att_test_trgs) \n",
    "\n",
    "transformer_att_test_rmse = rmse(transformer_att_test_outputs, transformer_att_test_trgs)\n",
    "transformer_att_test_mse = mse(transformer_att_test_outputs, transformer_att_test_trgs)\n",
    "transformer_att_test_mae = mae(transformer_att_test_outputs, transformer_att_test_trgs)\n",
    "        \n",
    "print(f'Test RMSE: {transformer_att_test_rmse:.4f}')\n",
    "print(f'Test MSE: {transformer_att_test_mse:.4f}')\n",
    "print(f'Test MAE: {transformer_att_test_mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\histograms.py:883: RuntimeWarning: invalid value encountered in divide\n",
      "  return n/db/n.sum(), bin_edges\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py:367: RuntimeWarning: Mean of empty slice.\n",
      "  loc = data.mean()\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py:372: RuntimeWarning: Mean of empty slice.\n",
      "  scale = np.sqrt(((data - loc)**2).mean())\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_27336\\1601207231.py:13: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"k\" (-> color=(0.0, 0.0, 0.0, 1)). The keyword argument will take precedence.\n",
      "  plt.plot(x, p, 'k', linewidth=2, color='orange')\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(transformer_att_test_rmses, bins=30, density=True, color='white', alpha=0.5)\n",
    "\n",
    "mu, std = norm.fit(transformer_att_test_rmses)\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(0, 10)\n",
    "p = norm.pdf(x, mu, std)\n",
    "\n",
    "plt.plot(x, p, 'k', linewidth=2, color='orange')\n",
    "plt.fill_between(x, p, color='orange', alpha=0.5)\n",
    "\n",
    "plt.axvline(mu, color='k', linestyle='dashed', linewidth=1)\n",
    "plt.text(mu + 0.1, max(p) + 0.05, 'mean = {:.2f}'.format(mu), color='k')\n",
    "\n",
    "plt.legend(['TSA-GA Net'], loc='upper right')\n",
    "plt.xlabel('General Error(R_pre)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코로나"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 10.3447\n",
      "Test MSE: 107.0138\n",
      "Test MAE: 8.1990\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class District_Dataset(Dataset):\n",
    "    def __init__(self, model, table_1, table_2, table_3, embedding_dim, window_size, SUB, DEVICE):\n",
    "        # 데이터프레임 복사본 생성\n",
    "        table_1_copy = table_1.copy()\n",
    "        table_2_copy = table_2.copy()\n",
    "        table_3_copy = table_3.copy()\n",
    "\n",
    "        # 정규화\n",
    "        scaler = StandardScaler()\n",
    "        table_1_copy[[cols for cols in table_1_copy.columns if cols not in ['aid','location','name']]] = scaler.fit_transform(table_1_copy[[cols for cols in table_1_copy.columns if cols not in ['aid','location','name']]])\n",
    "        scaler.fit(table_2_copy[[cols for cols in table_2_copy.columns if cols not in ['did','year','month']]][:135])\n",
    "        table_2_copy[[cols for cols in table_2_copy.columns if cols not in ['did','year','month']]] = scaler.transform(table_2_copy[[cols for cols in table_2_copy.columns if cols not in ['did','year','month']]])\n",
    "        table_2_copy = table_2_copy[(173<=table_2_copy['did'])*(table_2_copy['did']<=187)]\n",
    "        table_3_copy['price'] = table_3_copy['price'] * 0.0001 # 억 단위\n",
    "        table_3_copy = table_3_copy[(173<=table_3_copy['did'])*(table_3_copy['did']<=187)]\n",
    " \n",
    "        if SUB == True:\n",
    "            # 동 이름 바꾸기\n",
    "            old_and_new_dongs = {'용산동5가':'한강로동','한강로2가':'한강로동','창동':'창제동','돈암동':'정릉동','거여동':'위례동','문정동':'위례동','장지동':'위례동','문배동':'원효로동','산천동':'원효로동','신창동':'원효로동','원효로1가':'원효로동','화곡동':'우장산동','내발산동':'우장산동','영등포동8가':'영등포동','양평동3가':'양평동','안암동1가':'안암동','염리동':'아현동','성수동2가':'성수2가제2동','성수동1가':'성수1가제1동','중동':'성산동','노고산동':'서교동','신정동':'서강동','창전동':'서강동','삼선동4가':'삼선동','보문동3가':'보문동','동소문동7가':'동선동','당산동4가':'당산제2동','당산동5가':'당산제2동','당산동':'당산제2동','당산동3가':'당산제1동','당산동1가':'당산제1동','당산동2가':'당산제1동','본동':'노량진동','신수동':'노고산동','대흥동':'노고산동','금호동4가':'금호동','금호동2가':'금호동','충무로4가':'광희동','방화동':'공항동','도화동':'공덕동','신공덕동':'공덕동','일원동':'개포동'}\n",
    "            def change_dongs(location):\n",
    "                parts = location.split(' ')\n",
    "                if parts[2] in old_and_new_dongs:\n",
    "                    parts[2] = old_and_new_dongs[parts[2]]\n",
    "                return ' '.join(parts)\n",
    "            table_1_copy['location'] = table_1_copy['location'].apply(change_dongs)\n",
    "\n",
    "            # 동 종류\n",
    "            table_1_copy['district'] = table_1_copy['location'].apply(lambda x: x.split(' ')[2])\n",
    "            districts = table_1_copy['district'].unique()\n",
    "        elif SUB == False:\n",
    "            # 구 종류\n",
    "            table_1_copy['district'] = table_1_copy['location'].apply(lambda x: x.split(' ')[1])\n",
    "            districts = table_1_copy['district'].unique()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value for 'SUB'. It must be either True or False.\")\n",
    "            \n",
    "\n",
    "        # (동별) 최대 단지 개수\n",
    "        # TRAIN: 38 \n",
    "        # TEST: 24\n",
    "        max_apartment_complexes = max(table_1_copy.groupby('district')['name'].count())\n",
    "\n",
    "        # (전체 동 개수 * 204-window_size, 최대 단지 개수, window_size, embedding_dim) \n",
    "        # TRAIN: (22698, 38, 10, 1024)\n",
    "        # TEST: (1120, 24, 10, 1024)\n",
    "        districts_apartment_complexes_embedding_matrixes_with_window_size = [] \n",
    "        # 단지 개수 \n",
    "        # (전체 동 개수 * 204-window_size, 1)\n",
    "        districts_apartment_complexes_embedding_matrixes_with_window_size_num = [] \n",
    "        # y 값이 있는 단지 index \n",
    "        # (전체 동 개수 * 204-window_size, ?)\n",
    "        districts_apartment_complexes_embedding_matrixes_with_window_size_index = [] \n",
    "        # (전체 동 개수 * 204-window_size, 최대 단지 개수, 1)\n",
    "        # TRAIN: (22698, 38, 1)\n",
    "        # TEST: (1120, 24, 1)\n",
    "        districts_apartment_complexes_prices_with_window_size = [] \n",
    "\n",
    "        if model != 'None': # 임베딩 벡터를 사용할 때\n",
    "            model.eval()\n",
    "            model.to(DEVICE)\n",
    "\n",
    "        # 동 마다\n",
    "        for district in districts: \n",
    "            # dong_apartment_complexes_embedding_matrixes(동 안의 단지마다 임베팅 matrix 구한 뒤 리스트 형식으로 모으기) 완성 # (동 안의 단지 개수, 204, 6)\n",
    "            district_apartment_complexes_values = table_1_copy[table_1_copy['district'] == district][[cols for cols in table_1_copy.columns if cols not in ['aid','location','name','district']]].values # 하나의 동 안의 아파트 단지 값들 # (동 안의 단지 개수, 10)\n",
    "            economy_values = table_2_copy[['call_rate','m2']].values # 경제 지표 값들 # (204/20, 2)\n",
    "            economy_tensor = torch.FloatTensor(economy_values).to(DEVICE).type(torch.float32) # 경제 지표 텐서 변환\n",
    "\n",
    "            encoder_input_tensors = torch.zeros(district_apartment_complexes_values.shape[0], len(table_2_copy), 12).to(DEVICE).type(torch.float32) # 인코더 입력 텐서들 초기화(인코더 입력 텐서 여러개) # (동 안의 단지 개수, 204(시점), 12)\n",
    "            for i, district_apartment_complex_values in enumerate(district_apartment_complexes_values):\n",
    "                district_apartment_complex_tensor = torch.FloatTensor(district_apartment_complex_values).to(DEVICE).repeat(len(table_2_copy),1) \n",
    "                encoder_input_tensor = torch.cat((district_apartment_complex_tensor, economy_tensor), dim=1)\n",
    "                encoder_input_tensors[i] = encoder_input_tensor\n",
    "\n",
    "            if embedding_dim != 'None': # 임베딩 벡터를 사용할 때\n",
    "                with torch.no_grad():\n",
    "                    district_apartment_complexes_embedding_matrixes = torch.zeros(encoder_input_tensors.shape[0], len(table_2_copy), embedding_dim).type(torch.float32) # (동 안의 단지 개수, 204/20, 1024)\n",
    "                    for i in range(encoder_input_tensors.shape[0]): # 동 안의 단지 (204/20, 1024)\n",
    "                        district_apartment_complexes_embedding_matrixes[i] = model.encoder(encoder_input_tensors[i])\n",
    "\n",
    "            # dong_apartment_complexes_prices(동 안의 단지마다 가격 구한 뒤 리스트 형식으로 모으기) 완성 # (동 안의 단지 개수, 204/20, 1)\n",
    "            district_apartment_complexes_aids = table_1_copy[table_1_copy['district'] == district]['aid'].values # (동 안의 단지 개수, )\n",
    "            district_apartment_complexes_prices = torch.zeros(district_apartment_complexes_aids.shape[0], len(table_2_copy), 1).to(DEVICE).type(torch.float32) # (동 안의 단지 개수, 204/20, 1)\n",
    "            for i, district_apartment_complex_aid in zip(range(district_apartment_complexes_aids.shape[0]), district_apartment_complexes_aids): # 동 안의 단지 개수, 동 안의 단지들의 aids\n",
    "                district_apartment_complexes_prices[i] = torch.from_numpy(pd.DataFrame({'did': range(173, 173+len(table_2_copy))}).merge(table_3_copy[table_3_copy['aid'] == district_apartment_complex_aid][['did','price']], on='did', how='outer').fillna(0).set_index('did').values) # (204/20, 1)\n",
    "\n",
    "            if embedding_dim == 'None': # 임베딩 벡터가 없을 때\n",
    "                district_apartment_complexes_embedding_matrixes = encoder_input_tensors.type(torch.float32)\n",
    "                \n",
    "            # dong_apartment_complexes_embedding_matrixes와 dong_apartment_complexes_prices window_size로 나누기\n",
    "            for i in range(len(table_2_copy)-window_size): # window_size 고려한 시점(0~199/19)\n",
    "                if embedding_dim == 'None': # 임베딩 벡터가 없을 때\n",
    "                    district_apartment_complexes_embedding_matrixes_with_window_size = torch.zeros(max_apartment_complexes, window_size, 12)\n",
    "                else:\n",
    "                    district_apartment_complexes_embedding_matrixes_with_window_size = torch.zeros(max_apartment_complexes, window_size, embedding_dim) # (38/224, window_size, 1024)\n",
    "                district_apartment_complexes_prices_with_window_size = torch.zeros(max_apartment_complexes, 1).to(DEVICE) # (38/24, 1)\n",
    "                \n",
    "                district_apartment_complexes_embedding_matrixes_with_window_size[:district_apartment_complexes_embedding_matrixes.shape[0],:,:] = district_apartment_complexes_embedding_matrixes[:,i:i+window_size,:]\n",
    "                district_apartment_complexes_prices_with_window_size[:district_apartment_complexes_prices.shape[0],:] = district_apartment_complexes_prices[:,i+window_size,:]\n",
    "                district_apartment_complexes_embedding_matrixes_with_window_size_index = torch.where(district_apartment_complexes_prices_with_window_size > 0, 1, 0).squeeze()\n",
    "                 \n",
    "                districts_apartment_complexes_embedding_matrixes_with_window_size.append(district_apartment_complexes_embedding_matrixes_with_window_size) # (38, window_size, 1024)\n",
    "                districts_apartment_complexes_embedding_matrixes_with_window_size_num.append(district_apartment_complexes_embedding_matrixes.shape[0]) # 자연수\n",
    "                districts_apartment_complexes_embedding_matrixes_with_window_size_index.append(district_apartment_complexes_embedding_matrixes_with_window_size_index) # (1, )\n",
    "                districts_apartment_complexes_prices_with_window_size.append(district_apartment_complexes_prices_with_window_size) # (38/24, 1)\n",
    "\n",
    "        # 동마다 시점들 -> 시점들마다 동 \n",
    "        grouped_districts_apartment_complexes_embedding_matrixes_with_window_size = [districts_apartment_complexes_embedding_matrixes_with_window_size[i:i+len(table_2_copy)-window_size] for i in range(0,len(districts_apartment_complexes_embedding_matrixes_with_window_size),len(table_2_copy)-window_size)]\n",
    "        districts_apartment_complexes_embedding_matrixes_with_window_size = [item for group in zip(*grouped_districts_apartment_complexes_embedding_matrixes_with_window_size) for item in group]\n",
    "\n",
    "        self.districts_apartment_complexes_embedding_matrixes_with_window_size = districts_apartment_complexes_embedding_matrixes_with_window_size\n",
    "        self.districts_apartment_complexes_embedding_matrixes_with_window_size_num = districts_apartment_complexes_embedding_matrixes_with_window_size_num\n",
    "        self.districts_apartment_complexes_embedding_matrixes_with_window_size_index = districts_apartment_complexes_embedding_matrixes_with_window_size_index\n",
    "        self.districts_apartment_complexes_prices_with_window_size = districts_apartment_complexes_prices_with_window_size\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # 임베딩(x), 단지 개수, y값 있는 단지 인덱스, 가격(y)\n",
    "        return self.districts_apartment_complexes_embedding_matrixes_with_window_size[i], self.districts_apartment_complexes_embedding_matrixes_with_window_size_num[i], self.districts_apartment_complexes_embedding_matrixes_with_window_size_index[i], self.districts_apartment_complexes_prices_with_window_size[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.districts_apartment_complexes_embedding_matrixes_with_window_size)\n",
    "    \n",
    "embedding_model = torch.load(\"../데이터/Checkpoint/embedding/default/embedding_lr_0.0001_batch_64_sub_True_emb_1024_ws_12_epochs_13.pth\", map_location=DEVICE)\n",
    "test_dataset = District_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, sub, DEVICE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "# LSTM\n",
    "# model = torch.load(\"../데이터/Checkpoint/lstm/default/lstm_lr_0.0001_batch_64_hid_1024_sub_True_emb_1024_ws_12_epochs_10.pth\", map_location=DEVICE)\n",
    "\n",
    "# GRU\n",
    "# model = torch.load(\"../데이터/Checkpoint/gru/default/gru_lr_0.0001_batch_64_hid_1024_sub_True_emb_1024_ws_12_epochs_9.pth\", map_location=DEVICE)\n",
    "\n",
    "# transformer\n",
    "model = torch.load(\"../데이터/Checkpoint/transformer/default/transformer_lr_0.0001_batch_64_sub_True_emb_1024_ws_12_epochs_15.pth\", map_location=DEVICE)\n",
    "\n",
    "# LSTM attention\n",
    "# model = torch.load(\"../데이터/Checkpoint/lstm/attention/lstm_attention_lr_0.0001_batch_64_sub_True_emb_1024_ws_12_epochs_8.pth\", map_location=DEVICE)\n",
    "\n",
    "# GRU attention\n",
    "# model = torch.load(\"../데이터/Checkpoint/gru/attention/gru_attention_lr_0.0001_batch_64_sub_True_emb_1024_ws_12_epochs_4.pth\", map_location=DEVICE)\n",
    "\n",
    "# transformer attention\n",
    "# model = torch.load(\"../데이터/Checkpoint/transformer/attention/default/transformer_attention_lr_0.0001_batch_64_sub_True_emb_1024_ws_12_epochs_5.pth\", map_location=DEVICE)\n",
    "\n",
    "model.eval()\n",
    "test_rmses = []\n",
    "test_mses = []\n",
    "test_maes = []\n",
    "\n",
    "test_outputs = []\n",
    "test_trgs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        src = data[0][0].to(DEVICE)\n",
    "        max_len = data[1][0].to(DEVICE)\n",
    "        try:\n",
    "            anw = torch.nonzero(data[2][0]).to(DEVICE)[0]\n",
    "        except:\n",
    "            continue\n",
    "        trg = data[3][0].to(DEVICE)\n",
    "\n",
    "        for index in anw:\n",
    "            # LSTM\n",
    "            # output, _, _ = model(src)\n",
    "            \n",
    "            # GRU\n",
    "            # output, _ = model(src)\n",
    "            \n",
    "            # nlinear\n",
    "            # output, _ = model(src)\n",
    "            \n",
    "            # transformer\n",
    "            src_mask = model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "            output, _ = model(src, src_mask)\n",
    "            \n",
    "            test_outputs.append(output[index])\n",
    "            test_trgs.append(trg[index])\n",
    "\n",
    "            # attention\n",
    "            # output = model(src, index, max_len)\n",
    "\n",
    "            # test_outputs.append(output)\n",
    "            # test_trgs.append(trg[index])\n",
    "\n",
    "test_outputs = torch.FloatTensor(test_outputs)\n",
    "test_trgs = torch.FloatTensor(test_trgs)  \n",
    "\n",
    "test_rmse = rmse(test_outputs, test_trgs)\n",
    "test_mse = mse(test_outputs, test_trgs)\n",
    "test_mae = mae(test_outputs, test_trgs)\n",
    "\n",
    "print(f'Test RMSE: {test_rmse:.4f}')\n",
    "print(f'Test MSE: {test_mse:.4f}')\n",
    "print(f'Test MAE: {test_mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
