{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "from Dataset.Embedding_Dataset import Embedding_Dataset\n",
    "from Model.Embedding import Embedding\n",
    "\n",
    "from Dataset.Apartment_Complex_Dataset import Apartment_Complex_Dataset\n",
    "from Model.LSTM import LSTM\n",
    "from Model.GRU import GRU\n",
    "from Model.Transformer import Transformer\n",
    "\n",
    "from Dataset.District_Dataset import District_Dataset\n",
    "from Model.LSTM_Attention import LSTMAttention\n",
    "from Model.GRU_Attention import GRUAttention\n",
    "from Model.Transformer_Attention import TransformerAttention\n",
    "\n",
    "from utils import RMSE, rmse, mse, mae, mape, save_train_test_losses\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "table_1 = pd.read_csv('../데이터/Table/table_1.csv') \n",
    "table_2 = pd.read_csv('../데이터/Table/table_2.csv') \n",
    "table_3 = pd.read_csv('../데이터/Table/table_3.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1          1\n",
       "2          2\n",
       "3          3\n",
       "4          4\n",
       "        ... \n",
       "60364    204\n",
       "60365    205\n",
       "60366    210\n",
       "60367    211\n",
       "60368    212\n",
       "Name: did, Length: 60369, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_3['did']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "test_ratio = 0.3\n",
    "\n",
    "epochs = 10000\n",
    "lrs = 1e-4\n",
    "batches = 256\n",
    "subs = [True, False]\n",
    "embedding_dims = [512, 1024, 2048] \n",
    "window_sizes = [1, 7, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\t Epoch [1/10000], Train Loss: 0.955960, Test Loss: 3.812281\n",
      "Embedding\t Epoch [2/10000], Train Loss: 0.748264, Test Loss: 3.372955\n",
      "Embedding\t Epoch [3/10000], Train Loss: 0.681079, Test Loss: 3.078740\n",
      "Embedding\t Epoch [4/10000], Train Loss: 0.646248, Test Loss: 3.070028\n",
      "Embedding\t Epoch [5/10000], Train Loss: 0.623195, Test Loss: 2.811720\n",
      "Embedding\t Epoch [6/10000], Train Loss: 0.606986, Test Loss: 3.006784\n",
      "Embedding\t Epoch [7/10000], Train Loss: 0.596322, Test Loss: 3.083443\n",
      "Embedding\t Epoch [8/10000], Train Loss: 0.586563, Test Loss: 3.146584 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 0.5865627824357061\n",
      "Min Test Loss: 2.811720499755643\n",
      "Transformer\t Epoch [1/10000], Train Loss: 1.604182, Test Loss: 1.850612\n",
      "Transformer\t Epoch [2/10000], Train Loss: 1.518761, Test Loss: 1.840345\n",
      "Transformer\t Epoch [3/10000], Train Loss: 1.521134, Test Loss: 1.843635\n",
      "Transformer\t Epoch [4/10000], Train Loss: 1.514798, Test Loss: 1.848243\n",
      "Transformer\t Epoch [5/10000], Train Loss: 1.513887, Test Loss: 1.834905\n",
      "Transformer\t Epoch [6/10000], Train Loss: 1.508335, Test Loss: 1.855135\n",
      "Transformer\t Epoch [7/10000], Train Loss: 1.509545, Test Loss: 1.841579\n",
      "Transformer\t Epoch [8/10000], Train Loss: 1.508543, Test Loss: 1.839737 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 1.5083354344176316\n",
      "Min Test Loss: 1.834905399216546\n",
      "Attention\t Epoch [1/10000], Train Loss: 4.777939, Test Loss: 6.038553\n",
      "Attention\t Epoch [2/10000], Train Loss: 4.787133, Test Loss: 6.038388\n",
      "Attention\t Epoch [3/10000], Train Loss: 4.787133, Test Loss: 6.038392\n",
      "Attention\t Epoch [4/10000], Train Loss: 4.787133, Test Loss: 6.038305\n",
      "Attention\t Epoch [5/10000], Train Loss: 4.787133, Test Loss: 6.038285\n",
      "Attention\t Epoch [6/10000], Train Loss: 4.787133, Test Loss: 6.038292\n",
      "Attention\t Epoch [7/10000], Train Loss: 4.787133, Test Loss: 6.038306\n",
      "Min Train Loss: 4.777939468358562\n",
      "Min Test Loss: 6.038284644671463\n",
      "Min Train Loss: 4.777939468358562\n",
      "Min Test Loss: 44.47843836519781\n",
      "Min Train Loss: 4.777939468358562\n",
      "Min Test Loss: 6.038284644671463\n",
      "Min Train Loss: 4.777939468358562\n",
      "Min Test Loss: 148.1840426596627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_17560\\3420303466.py:268: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\t Epoch [1/10000], Train Loss: 0.964914, Test Loss: 4.044018\n",
      "Embedding\t Epoch [2/10000], Train Loss: 0.754344, Test Loss: 4.373922\n",
      "Embedding\t Epoch [3/10000], Train Loss: 0.689829, Test Loss: 3.966305\n",
      "Embedding\t Epoch [4/10000], Train Loss: 0.653211, Test Loss: 3.768061\n",
      "Embedding\t Epoch [5/10000], Train Loss: 0.633056, Test Loss: 3.855157\n",
      "Embedding\t Epoch [6/10000], Train Loss: 0.617749, Test Loss: 3.873092\n",
      "Embedding\t Epoch [7/10000], Train Loss: 0.605196, Test Loss: 3.929432 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 0.6051961693799857\n",
      "Min Test Loss: 3.7680614463826445\n",
      "Transformer\t Epoch [1/10000], Train Loss: 1.594301, Test Loss: 2.104819\n",
      "Transformer\t Epoch [2/10000], Train Loss: 1.559205, Test Loss: 2.067538\n",
      "Transformer\t Epoch [3/10000], Train Loss: 1.552741, Test Loss: 2.045520\n",
      "Transformer\t Epoch [4/10000], Train Loss: 1.547211, Test Loss: 2.041761\n",
      "Transformer\t Epoch [5/10000], Train Loss: 1.545771, Test Loss: 2.042471\n",
      "Transformer\t Epoch [6/10000], Train Loss: 1.546221, Test Loss: 2.111575\n",
      "Transformer\t Epoch [7/10000], Train Loss: 1.551803, Test Loss: 2.096117 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 1.5457710750718463\n",
      "Min Test Loss: 2.0417614662856387\n",
      "Attention\t Epoch [1/10000], Train Loss: 4.746811, Test Loss: 7.970377\n",
      "Attention\t Epoch [2/10000], Train Loss: 4.788098, Test Loss: 5.078198\n",
      "Attention\t Epoch [3/10000], Train Loss: 4.773928, Test Loss: 7.970254\n",
      "Attention\t Epoch [4/10000], Train Loss: 4.789913, Test Loss: 5.078149\n",
      "Attention\t Epoch [5/10000], Train Loss: 4.770058, Test Loss: 7.970178\n",
      "Attention\t Epoch [6/10000], Train Loss: 4.789913, Test Loss: 5.078141\n",
      "Min Train Loss: 4.746810714544455\n",
      "Min Test Loss: 5.0781407468124335\n",
      "Min Train Loss: 4.746810714544455\n",
      "Min Test Loss: 34.7090869828672\n",
      "Min Train Loss: 4.746810714544455\n",
      "Min Test Loss: 5.0781407468124335\n",
      "Min Train Loss: 4.746810714544455\n",
      "Min Test Loss: 97.14379221348347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_17560\\3420303466.py:268: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\t Epoch [1/10000], Train Loss: 0.918973, Test Loss: 4.862298\n",
      "Embedding\t Epoch [2/10000], Train Loss: 0.728655, Test Loss: 4.487514\n",
      "Embedding\t Epoch [3/10000], Train Loss: 0.672528, Test Loss: 4.302554\n",
      "Embedding\t Epoch [4/10000], Train Loss: 0.640181, Test Loss: 4.159017\n",
      "Embedding\t Epoch [5/10000], Train Loss: 0.621687, Test Loss: 4.075948\n",
      "Embedding\t Epoch [6/10000], Train Loss: 0.610254, Test Loss: 4.133104\n",
      "Embedding\t Epoch [7/10000], Train Loss: 0.598270, Test Loss: 4.129266\n",
      "Embedding\t Epoch [8/10000], Train Loss: 0.590794, Test Loss: 4.275051 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 0.5907943549481305\n",
      "Min Test Loss: 4.075947962754162\n",
      "Transformer\t Epoch [1/10000], Train Loss: 1.765861, Test Loss: 2.005414\n",
      "Transformer\t Epoch [2/10000], Train Loss: 1.707474, Test Loss: 1.925884\n",
      "Transformer\t Epoch [3/10000], Train Loss: 1.707393, Test Loss: 1.848288\n",
      "Transformer\t Epoch [4/10000], Train Loss: 1.695309, Test Loss: 1.847933\n",
      "Transformer\t Epoch [5/10000], Train Loss: 1.697474, Test Loss: 1.869065\n",
      "Transformer\t Epoch [6/10000], Train Loss: 1.698106, Test Loss: 1.868484\n",
      "Transformer\t Epoch [7/10000], Train Loss: 1.714190, Test Loss: 1.897278 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 1.6953087910610463\n",
      "Min Test Loss: 1.8479328971160085\n",
      "Attention\t Epoch [1/10000], Train Loss: 4.865188, Test Loss: 5.834209\n",
      "Attention\t Epoch [2/10000], Train Loss: 4.809954, Test Loss: 5.834152\n",
      "Attention\t Epoch [3/10000], Train Loss: 4.807573, Test Loss: 5.834035\n",
      "Attention\t Epoch [4/10000], Train Loss: 4.810681, Test Loss: 5.833968\n",
      "Attention\t Epoch [5/10000], Train Loss: 4.810681, Test Loss: 5.833946\n",
      "Attention\t Epoch [6/10000], Train Loss: 4.810681, Test Loss: 5.833879\n",
      "Min Train Loss: 4.8075726380948245\n",
      "Min Test Loss: 5.833878846782063\n",
      "Min Train Loss: 4.8075726380948245\n",
      "Min Test Loss: 43.5302066532683\n",
      "Min Train Loss: 4.8075726380948245\n",
      "Min Test Loss: 5.833878846782063\n",
      "Min Train Loss: 4.8075726380948245\n",
      "Min Test Loss: 125.80561537171064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_17560\\3420303466.py:268: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\t Epoch [1/10000], Train Loss: 0.950351, Test Loss: 4.435568\n",
      "Embedding\t Epoch [2/10000], Train Loss: 0.744580, Test Loss: 4.224352\n",
      "Embedding\t Epoch [3/10000], Train Loss: 0.686434, Test Loss: 3.865493\n",
      "Embedding\t Epoch [4/10000], Train Loss: 0.654174, Test Loss: 3.853646\n",
      "Embedding\t Epoch [5/10000], Train Loss: 0.633394, Test Loss: 3.885393\n",
      "Embedding\t Epoch [6/10000], Train Loss: 0.621140, Test Loss: 3.856897\n",
      "Embedding\t Epoch [7/10000], Train Loss: 0.611072, Test Loss: 3.837061\n",
      "Embedding\t Epoch [8/10000], Train Loss: 0.603525, Test Loss: 3.891354\n",
      "Embedding\t Epoch [9/10000], Train Loss: 0.596618, Test Loss: 4.007504\n",
      "Embedding\t Epoch [10/10000], Train Loss: 0.590406, Test Loss: 4.196214 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 0.590406030868039\n",
      "Min Test Loss: 3.8370605616282063\n",
      "Transformer\t Epoch [1/10000], Train Loss: 1.651732, Test Loss: 1.851031\n",
      "Transformer\t Epoch [2/10000], Train Loss: 1.521689, Test Loss: 1.835968\n",
      "Transformer\t Epoch [3/10000], Train Loss: 1.515548, Test Loss: 1.843358\n",
      "Transformer\t Epoch [4/10000], Train Loss: 1.538946, Test Loss: 1.887020\n",
      "Transformer\t Epoch [5/10000], Train Loss: 1.513921, Test Loss: 1.844937 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 1.5139210250287631\n",
      "Min Test Loss: 1.8359680168988697\n",
      "Attention\t Epoch [1/10000], Train Loss: 6.430787, Test Loss: 4.898320\n",
      "Attention\t Epoch [2/10000], Train Loss: 6.403163, Test Loss: 4.898297\n",
      "Attention\t Epoch [3/10000], Train Loss: 6.474180, Test Loss: 7.441964\n",
      "Attention\t Epoch [4/10000], Train Loss: 6.501270, Test Loss: 7.441663\n",
      "Min Train Loss: 6.403163203187814\n",
      "Min Test Loss: 4.89829748763611\n",
      "Min Train Loss: 6.403163203187814\n",
      "Min Test Loss: 28.305112563896802\n",
      "Min Train Loss: 6.403163203187814\n",
      "Min Test Loss: 4.89829748763611\n",
      "Min Train Loss: 6.403163203187814\n",
      "Min Test Loss: 76.62024782033119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_17560\\3420303466.py:268: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\t Epoch [1/10000], Train Loss: 0.943838, Test Loss: 4.228469\n",
      "Embedding\t Epoch [2/10000], Train Loss: 0.743092, Test Loss: 3.843249\n",
      "Embedding\t Epoch [3/10000], Train Loss: 0.688235, Test Loss: 3.895595\n",
      "Embedding\t Epoch [4/10000], Train Loss: 0.659332, Test Loss: 3.940623\n",
      "Embedding\t Epoch [5/10000], Train Loss: 0.634807, Test Loss: 4.117109 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 0.6348069556734779\n",
      "Min Test Loss: 3.843248560073528\n",
      "Transformer\t Epoch [1/10000], Train Loss: 1.784448, Test Loss: 2.182083\n",
      "Transformer\t Epoch [2/10000], Train Loss: 1.565141, Test Loss: 2.063454\n",
      "Transformer\t Epoch [3/10000], Train Loss: 1.547348, Test Loss: 2.051444\n",
      "Transformer\t Epoch [4/10000], Train Loss: 1.547056, Test Loss: 2.049689\n",
      "Transformer\t Epoch [5/10000], Train Loss: 1.547132, Test Loss: 2.049362\n",
      "Transformer\t Epoch [6/10000], Train Loss: 1.547156, Test Loss: 2.049280\n",
      "Transformer\t Epoch [7/10000], Train Loss: 1.547164, Test Loss: 2.049247\n",
      "Transformer\t Epoch [8/10000], Train Loss: 1.547169, Test Loss: 2.049228\n",
      "Transformer\t Epoch [9/10000], Train Loss: 1.547172, Test Loss: 2.049216\n",
      "Transformer\t Epoch [10/10000], Train Loss: 1.547174, Test Loss: 2.049207\n",
      "Transformer\t Epoch [11/10000], Train Loss: 1.547175, Test Loss: 2.049201\n",
      "Transformer\t Epoch [12/10000], Train Loss: 1.547176, Test Loss: 2.049196\n",
      "Transformer\t Epoch [13/10000], Train Loss: 1.547177, Test Loss: 2.049193\n",
      "Transformer\t Epoch [14/10000], Train Loss: 1.547178, Test Loss: 2.049191\n",
      "Transformer\t Epoch [15/10000], Train Loss: 1.547178, Test Loss: 2.049189\n",
      "Transformer\t Epoch [16/10000], Train Loss: 1.547178, Test Loss: 2.049188\n",
      "Transformer\t Epoch [17/10000], Train Loss: 1.547179, Test Loss: 2.049187\n",
      "Transformer\t Epoch [18/10000], Train Loss: 1.547179, Test Loss: 2.049186\n",
      "Transformer\t Epoch [19/10000], Train Loss: 1.547179, Test Loss: 2.049186\n",
      "Transformer\t Epoch [20/10000], Train Loss: 1.547179, Test Loss: 2.049185\n",
      "Transformer\t Epoch [21/10000], Train Loss: 1.547179, Test Loss: 2.049185\n",
      "Transformer\t Epoch [22/10000], Train Loss: 1.547179, Test Loss: 2.049185\n",
      "Transformer\t Epoch [23/10000], Train Loss: 1.547179, Test Loss: 2.049185\n",
      "Transformer\t Epoch [24/10000], Train Loss: 1.547179, Test Loss: 2.049184\n",
      "Transformer\t Epoch [25/10000], Train Loss: 1.547179, Test Loss: 2.049184\n",
      "Transformer\t Epoch [26/10000], Train Loss: 1.547179, Test Loss: 2.049184\n",
      "Transformer\t Epoch [27/10000], Train Loss: 1.547179, Test Loss: 2.049184\n",
      "Transformer\t Epoch [28/10000], Train Loss: 2.559374, Test Loss: 2.045848\n",
      "Transformer\t Epoch [29/10000], Train Loss: 1.555338, Test Loss: 2.048710\n",
      "Transformer\t Epoch [30/10000], Train Loss: 1.547204, Test Loss: 2.049128\n",
      "Transformer\t Epoch [31/10000], Train Loss: 1.547180, Test Loss: 2.049190 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 1.547056132614762\n",
      "Min Test Loss: 2.045848181198912\n",
      "Attention\t Epoch [1/10000], Train Loss: 6.383352, Test Loss: 5.730834\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m anw:\n\u001b[1;32m--> 199\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_att_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     transformer_att_train_loss \u001b[38;5;241m=\u001b[39m criterion(output, trg[index])\n\u001b[0;32m    202\u001b[0m     transformer_att_total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m transformer_att_train_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Model\\Transformer_Attention.py:17\u001b[0m, in \u001b[0;36mTransformerAttention.forward\u001b[1;34m(self, src, index, max_len)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, index, max_len):\n\u001b[1;32m---> 17\u001b[0m     src_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTransformer_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_square_subsequent_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTransformer_model(src, src_mask)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m hidden[index]  \u001b[38;5;66;03m# emb_dim\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(columns=['sub', 'embedding_dim', 'window_size', 'rmse', 'mse', 'mae', 'mape'])\n",
    "for batch in batches:\n",
    "    for sub in subs:\n",
    "        for embedding_dim in embedding_dims:\n",
    "            for window_size in window_sizes:\n",
    "                # embedding\n",
    "                dataset = Embedding_Dataset(table_1, table_2, table_3)\n",
    "                dataset_length = len(dataset)\n",
    "                train_size = int(train_ratio * dataset_length)\n",
    "                train_indices = range(0, train_size)\n",
    "                test_size = int(test_ratio * dataset_length)\n",
    "                test_indices = range(train_size, dataset_length)\n",
    "                train_dataset = Subset(dataset, train_indices)\n",
    "                test_dataset = Subset(dataset, test_indices)\n",
    "                train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "                test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "                embedding_model = Embedding(128, 256, 512, embedding_dim, 512, 256, 128).to(DEVICE)\n",
    "                criterion = RMSE()\n",
    "                optimizer = torch.optim.Adam(embedding_model.parameters(), lr=lr)\n",
    "\n",
    "                embedding_train_losses = []\n",
    "                embedding_test_losses = []\n",
    "\n",
    "                max_early_stop_count = 3\n",
    "                early_stop_count = 0\n",
    "                embedding_best_test_loss = float('inf')\n",
    "                embedding_best_model_weights = None\n",
    "\n",
    "                for epoch in range(epochs):\n",
    "                    embedding_model.train()\n",
    "                    embedding_total_train_loss = 0\n",
    "                    for data in train_dataloader:\n",
    "                        input = data[0].to(DEVICE)\n",
    "                        target = data[1].to(DEVICE)\n",
    "                        output = embedding_model(input).to(DEVICE)\n",
    "\n",
    "                        embedding_train_loss = criterion(output, target)\n",
    "                        embedding_total_train_loss += embedding_train_loss.item()\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        embedding_train_loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    embedding_avg_train_loss = embedding_total_train_loss / len(train_dataloader)\n",
    "                    embedding_train_losses.append(embedding_avg_train_loss)\n",
    "\n",
    "                    embedding_model.eval()\n",
    "                    embedding_total_test_loss = 0\n",
    "                    with torch.no_grad():\n",
    "                        for data in test_dataloader:\n",
    "                            input = data[0].to(DEVICE)\n",
    "                            target = data[1].to(DEVICE)\n",
    "                            output = embedding_model(input).to(DEVICE)\n",
    "\n",
    "                            embedding_test_loss = criterion(output, target)\n",
    "                            embedding_total_test_loss += embedding_test_loss.item()\n",
    "\n",
    "                    embedding_avg_test_loss = embedding_total_test_loss / len(test_dataloader)\n",
    "                    embedding_test_losses.append(embedding_avg_test_loss)\n",
    "\n",
    "                    if  embedding_best_test_loss > embedding_avg_test_loss:\n",
    "                        embedding_best_test_loss = embedding_avg_test_loss\n",
    "                        embedding_best_model_weights = copy.deepcopy(embedding_model.state_dict())\n",
    "                        early_stop_count = 0\n",
    "                    else:\n",
    "                        early_stop_count += 1\n",
    "\n",
    "                    if early_stop_count >= max_early_stop_count:\n",
    "                        print(f'Embedding\\t Epoch [{epoch+1}/{epochs}], Train Loss: {embedding_avg_train_loss:.6f}, Test Loss: {embedding_avg_test_loss:.6f} \\nEarly Stop Triggered!')\n",
    "                        embedding_model.load_state_dict(embedding_best_model_weights)\n",
    "                        torch.save(embedding_model, f'../데이터/Checkpoint/emb/embedding_lr_{lr}_batch_{batch}_sub_{sub}_ed_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "                        break\n",
    "\n",
    "                    print(f'Embedding\\t Epoch [{epoch+1}/{epochs}], Train Loss: {embedding_avg_train_loss:.6f}, Test Loss: {embedding_avg_test_loss:.6f}')\n",
    "                    \n",
    "                save_train_test_losses(embedding_train_losses, embedding_test_losses, f'../데이터/Checkpoint/emb/embedding_lr_{lr}_batch_{batch}_sub_{sub}_ed_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "\n",
    "                # transformer\n",
    "                dataset = Apartment_Complex_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, 'DL', DEVICE)\n",
    "                dataset_length = len(dataset)\n",
    "                train_size = int(train_ratio * dataset_length)\n",
    "                train_indices = range(0, train_size)\n",
    "                test_size = int(test_ratio * dataset_length)\n",
    "                test_indices = range(train_size, dataset_length)\n",
    "                train_dataset = Subset(dataset, train_indices)\n",
    "                test_dataset = Subset(dataset, test_indices)\n",
    "                train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "                test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "                transformer_model = Transformer(embedding_dim, 1, 2, 2).to(DEVICE)\n",
    "                criterion = RMSE()\n",
    "                optimizer = torch.optim.Adam(transformer_model.parameters(), lr=lr)\n",
    "\n",
    "                transformer_train_losses = []\n",
    "                transformer_test_losses = []\n",
    "\n",
    "                max_early_stop_count = 3\n",
    "                early_stop_count = 0\n",
    "                transformer_best_test_loss = float('inf')\n",
    "                transformer_best_model_weights = None\n",
    "\n",
    "                for epoch in range(epochs):\n",
    "                    transformer_model.train()\n",
    "                    transformer_total_train_loss = 0\n",
    "                    for data in train_dataloader:\n",
    "                        src = data[0].to(DEVICE)\n",
    "                        trg = data[1].to(DEVICE)\n",
    "\n",
    "                        if (trg[0] != 0):\n",
    "                            src_mask = transformer_model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "                            output = transformer_model(src, src_mask)\n",
    "\n",
    "                            transformer_train_loss = criterion(output[0], trg)\n",
    "                            transformer_total_train_loss += transformer_train_loss.item()\n",
    "\n",
    "                            optimizer.zero_grad()\n",
    "                            transformer_train_loss.backward()\n",
    "                            optimizer.step()\n",
    "                            \n",
    "                    transformer_avg_train_loss = transformer_total_train_loss / len(train_dataloader)\n",
    "                    transformer_train_losses.append(transformer_avg_train_loss)\n",
    "\n",
    "                    transformer_model.eval()\n",
    "                    transformer_total_test_loss = 0\n",
    "                    with torch.no_grad():\n",
    "                        for data in test_dataloader:\n",
    "                            src = data[0].to(DEVICE)\n",
    "                            trg = data[1].to(DEVICE)\n",
    "\n",
    "                            if (trg[0] != 0):\n",
    "                                src_mask = transformer_model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "                                output = transformer_model(src, src_mask)\n",
    "\n",
    "                                transformer_test_loss = criterion(output[0], trg)\n",
    "                                transformer_total_test_loss += transformer_test_loss.item()\n",
    "\n",
    "                    transformer_avg_test_loss = transformer_total_test_loss / len(test_dataloader)\n",
    "                    transformer_test_losses.append(transformer_avg_test_loss)\n",
    "\n",
    "                    if  transformer_best_test_loss > transformer_avg_test_loss:\n",
    "                        transformer_best_test_loss = transformer_avg_test_loss\n",
    "                        transformer_best_model_weights = copy.deepcopy(transformer_model.state_dict())\n",
    "                        early_stop_count = 0\n",
    "                    else:\n",
    "                        early_stop_count += 1\n",
    "                        \n",
    "                    if early_stop_count >= max_early_stop_count:\n",
    "                        print(f'Transformer\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_avg_train_loss:.6f}, Test Loss: {transformer_avg_test_loss:.6f} \\nEarly Stop Triggered!')\n",
    "                        transformer_model.load_state_dict(transformer_best_model_weights)\n",
    "                        torch.save(transformer_model, f'../데이터/Checkpoint/transformer/transformer_lr_{lr}_batch_{batch}_sub_{sub}_ed_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "                        break\n",
    "\n",
    "                    print(f'Transformer\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_avg_train_loss:.6f}, Test Loss: {transformer_avg_test_loss:.6f}')\n",
    "                \n",
    "                save_train_test_losses(transformer_train_losses, transformer_test_losses, f'../데이터/Checkpoint/transformer/transformer_lr_{lr}_batch_{batch}_sub_{sub}_ed_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "\n",
    "                # attention\n",
    "                dataset = District_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, sub, DEVICE)\n",
    "                dataset_length = len(dataset)\n",
    "                train_size = int(train_ratio * dataset_length)\n",
    "                train_indices = range(0, train_size)\n",
    "                test_size = int(test_ratio * dataset_length)\n",
    "                test_indices = range(train_size, dataset_length)\n",
    "                train_dataset = Subset(dataset, train_indices)\n",
    "                test_dataset = Subset(dataset, test_indices)\n",
    "                train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=False, drop_last=True)\n",
    "                test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, drop_last=True)\n",
    "\n",
    "                transformer_att_model = TransformerAttention(transformer_model, embedding_dim, 1, DEVICE).to(DEVICE)\n",
    "                criterion = RMSE()\n",
    "                optimizer = torch.optim.Adam(transformer_att_model.parameters(), lr=lr)\n",
    "\n",
    "                transformer_att_train_losses = []\n",
    "                transformer_att_test_rmses = []\n",
    "                transformer_att_test_mses = []\n",
    "                transformer_att_test_maes = []\n",
    "                transformer_att_test_mapes = []\n",
    "\n",
    "                max_early_stop_count = 3\n",
    "                early_stop_count = 0\n",
    "                transformer_att_best_test_rmse = float('inf')\n",
    "                transformer_att_best_model_weights = None\n",
    "\n",
    "                for epoch in range(epoch):\n",
    "                    transformer_att_model.train()\n",
    "                    transformer_att_total_train_loss = 0\n",
    "                    for data in train_dataloader:\n",
    "                        src = data[0][0].to(DEVICE)\n",
    "                        max_len = data[1][0].to(DEVICE)\n",
    "                        anw = data[2][0].to(DEVICE)\n",
    "                        trg = data[3][0].to(DEVICE)\n",
    "                        \n",
    "                        if len(anw)==0:\n",
    "                            continue\n",
    "\n",
    "                        for index in anw:\n",
    "                            output = transformer_att_model(src, index, max_len)\n",
    "                            \n",
    "                            transformer_att_train_loss = criterion(output, trg[index])\n",
    "                            transformer_att_total_train_loss += transformer_att_train_loss.item()\n",
    "                            \n",
    "                            optimizer.zero_grad()\n",
    "                            transformer_att_train_loss.backward()\n",
    "                            optimizer.step() \n",
    "                            \n",
    "                    transformer_att_avg_train_loss = transformer_att_total_train_loss / len(train_dataloader)\n",
    "                    transformer_att_train_losses.append(transformer_att_avg_train_loss)\n",
    "\n",
    "                    transformer_att_model.eval()\n",
    "                    transformer_att_total_test_rmse = 0\n",
    "                    transformer_att_total_test_mse = 0\n",
    "                    transformer_att_total_test_mae = 0\n",
    "                    transformer_att_total_test_mape = 0\n",
    "                    with torch.no_grad():\n",
    "                        for data in test_dataloader:\n",
    "                            src = data[0][0].to(DEVICE)\n",
    "                            max_len = data[1][0].to(DEVICE)\n",
    "                            anw = data[2][0].to(DEVICE)\n",
    "                            trg = data[3][0].to(DEVICE)\n",
    "\n",
    "                            if len(anw)==0:\n",
    "                                continue\n",
    "\n",
    "                            for index in anw:\n",
    "                                output = transformer_att_model(src, index, max_len)\n",
    "\n",
    "                                transformer_att_test_rmse = rmse(output, trg[index])\n",
    "                                transformer_att_test_mse = mse(output, trg[index])\n",
    "                                transformer_att_test_mae = mae(output, trg[index])\n",
    "                                transformer_att_test_mape = mape(output, trg[index])\n",
    "                                \n",
    "                                transformer_att_total_test_rmse += transformer_att_test_rmse.item()\n",
    "                                transformer_att_total_test_mse += transformer_att_test_mse.item()\n",
    "                                transformer_att_total_test_mae += transformer_att_test_mae.item()\n",
    "                                transformer_att_total_test_mape += transformer_att_test_mape.item()\n",
    "                                \n",
    "                    transformer_att_avg_test_rmse = transformer_att_total_test_rmse / len(test_dataloader)\n",
    "                    transformer_att_avg_test_mse = transformer_att_total_test_mse / len(test_dataloader)\n",
    "                    transformer_att_avg_test_mae = transformer_att_total_test_mae / len(test_dataloader)\n",
    "                    transformer_att_avg_test_mape = transformer_att_total_test_mape / len(test_dataloader)\n",
    "                    transformer_att_test_rmses.append(transformer_att_avg_test_rmse)\n",
    "                    transformer_att_test_mses.append(transformer_att_avg_test_mse)\n",
    "                    transformer_att_test_maes.append(transformer_att_avg_test_mae)\n",
    "                    transformer_att_test_mapes.append(transformer_att_avg_test_mape)\n",
    "                            \n",
    "                    if  transformer_att_best_test_rmse > transformer_att_avg_test_rmse:\n",
    "                        transformer_att_best_test_rmse = transformer_att_avg_test_rmse\n",
    "                        transformer_att_best_model_weights = copy.deepcopy(transformer_att_model.state_dict())\n",
    "                        early_stop_count = 0\n",
    "                    else:\n",
    "                        early_stop_count += 1\n",
    "\n",
    "                    if early_stop_count >= max_early_stop_count:\n",
    "                        print(f'Attention\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_att_avg_train_loss:.6f}, Test Loss: {transformer_att_avg_test_rmse:.6f} \\nEarly Stop Triggered!')\n",
    "                        transformer_att_model.load_state_dict(transformer_att_best_model_weights)\n",
    "                        torch.save(transformer_att_model, f'../데이터/Checkpoint/attention/attention_lr_{lr}_batch_{batch}_sub_{sub}_ed_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "                        break\n",
    "\n",
    "                    print(f'Attention\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_att_avg_train_loss:.6f}, Test Loss: {transformer_att_avg_test_rmse:.6f}')\n",
    "\n",
    "                save_train_test_losses(transformer_att_train_losses, transformer_att_test_rmses, f'../데이터/Checkpoint/attention/attention_rmse_lr_{lr}_batch_{batch}_sub_{sub}_ed_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "                save_train_test_losses(transformer_att_train_losses, transformer_att_test_mses, f'../데이터/Checkpoint/attention/attention_mse_lr_{lr}_batch_{batch}_sub_{sub}_ed_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "                save_train_test_losses(transformer_att_train_losses, transformer_att_test_maes, f'../데이터/Checkpoint/attention/attention_mae_lr_{lr}_batch_{batch}_sub_{sub}_ed_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "                save_train_test_losses(transformer_att_train_losses, transformer_att_test_mapes, f'../데이터/Checkpoint/attention/attention_mape_lr_{lr}_batch_{batch}_sub_{sub}_ed_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "\n",
    "                results_df = results_df.append({\n",
    "                    'lr': lr,\n",
    "                    'batch': batch,\n",
    "                    'sub': sub,\n",
    "                    'embedding_dim': embedding_dim,\n",
    "                    'window_size': window_size,\n",
    "                    'rmse': min(transformer_att_test_rmses),\n",
    "                    'mse': min(transformer_att_test_mses),\n",
    "                    'mae': min(transformer_att_test_maes),\n",
    "                    'mape': min(transformer_att_test_mapes)\n",
    "                }, ignore_index=True)\n",
    "\n",
    "results_df.to_excel('../데이터/Checkpoint/experiment_results.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
