{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "from Dataset.Embedding_Dataset import Embedding_Dataset\n",
    "from Model.Embedding import Embedding\n",
    "\n",
    "from Dataset.Apartment_Complex_Dataset import Apartment_Complex_Dataset\n",
    "from Model.LSTM import LSTM\n",
    "from Model.GRU import GRU\n",
    "from Model.Transformer import Transformer\n",
    "\n",
    "from Dataset.District_Dataset import District_Dataset\n",
    "from Model.LSTM_Attention import LSTMAttention\n",
    "from Model.GRU_Attention import GRUAttention\n",
    "from Model.Transformer_Attention import TransformerAttention\n",
    "\n",
    "from utils import RMSE, rmse, mse, mae, mape, save_train_val_losses\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "table_1 = pd.read_csv('../데이터/Table/table_1.csv') \n",
    "table_2 = pd.read_csv('../데이터/Table/table_2.csv') \n",
    "table_3 = pd.read_csv('../데이터/Table/table_3.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "epochs = 10000\n",
    "lr = 1e-4\n",
    "batch = 64\n",
    "sub = True\n",
    "embedding_dims = [256, 512, 1024]\n",
    "window_sizes = [6, 12, 36]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\t Epoch [1/10000], Train Loss: 2.691511, Val Loss: 4.174492\n",
      "Embedding\t Epoch [2/10000], Train Loss: 0.686539, Val Loss: 3.279496\n",
      "Embedding\t Epoch [3/10000], Train Loss: 0.628412, Val Loss: 3.185184\n",
      "Embedding\t Epoch [4/10000], Train Loss: 0.580377, Val Loss: 3.132907\n",
      "Embedding\t Epoch [5/10000], Train Loss: 0.546099, Val Loss: 3.108948\n",
      "Embedding\t Epoch [6/10000], Train Loss: 0.518425, Val Loss: 3.078731\n",
      "Embedding\t Epoch [7/10000], Train Loss: 0.497050, Val Loss: 3.072149\n",
      "Embedding\t Epoch [8/10000], Train Loss: 0.479025, Val Loss: 3.050579\n",
      "Embedding\t Epoch [9/10000], Train Loss: 0.462053, Val Loss: 3.039966\n",
      "Embedding\t Epoch [10/10000], Train Loss: 0.447593, Val Loss: 3.027392\n",
      "Embedding\t Epoch [11/10000], Train Loss: 0.433371, Val Loss: 3.011299\n",
      "Embedding\t Epoch [12/10000], Train Loss: 0.420582, Val Loss: 3.029461\n",
      "Embedding\t Epoch [13/10000], Train Loss: 0.410030, Val Loss: 2.958327\n",
      "Embedding\t Epoch [14/10000], Train Loss: 0.399629, Val Loss: 2.988436\n",
      "Embedding\t Epoch [15/10000], Train Loss: 0.390472, Val Loss: 2.964838\n",
      "Embedding\t Epoch [16/10000], Train Loss: 0.381744, Val Loss: 2.983696 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 0.38174381445754657\n",
      "Min Val Loss: 2.9583274224337113\n",
      "Transformer\t Epoch [1/10000], Train Loss: 4.001085, Val Loss: 4.075167\n",
      "Transformer\t Epoch [2/10000], Train Loss: 3.824819, Val Loss: 3.834404\n",
      "Transformer\t Epoch [3/10000], Train Loss: 3.750873, Val Loss: 3.781550\n",
      "Transformer\t Epoch [4/10000], Train Loss: 3.717317, Val Loss: 3.750843\n",
      "Transformer\t Epoch [5/10000], Train Loss: 3.693820, Val Loss: 3.723759\n",
      "Transformer\t Epoch [6/10000], Train Loss: 3.673000, Val Loss: 3.712316\n",
      "Transformer\t Epoch [7/10000], Train Loss: 3.661661, Val Loss: 3.744571\n",
      "Transformer\t Epoch [8/10000], Train Loss: 3.644234, Val Loss: 3.690176\n",
      "Transformer\t Epoch [9/10000], Train Loss: 3.636550, Val Loss: 3.688131\n",
      "Transformer\t Epoch [10/10000], Train Loss: 3.623171, Val Loss: 3.631507\n",
      "Transformer\t Epoch [11/10000], Train Loss: 3.610127, Val Loss: 3.615920\n",
      "Transformer\t Epoch [12/10000], Train Loss: 3.591552, Val Loss: 3.602332\n",
      "Transformer\t Epoch [13/10000], Train Loss: 3.586657, Val Loss: 3.573155\n",
      "Transformer\t Epoch [14/10000], Train Loss: 3.574146, Val Loss: 3.588529\n",
      "Transformer\t Epoch [15/10000], Train Loss: 3.556512, Val Loss: 3.617865\n",
      "Transformer\t Epoch [16/10000], Train Loss: 3.546156, Val Loss: 3.611316 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 3.5461559857443223\n",
      "Min Val Loss: 3.573154804835312\n",
      "Attention\t Epoch [1/10000], Train Loss: 3.026593, Val Loss: 3.025436\n",
      "Attention\t Epoch [2/10000], Train Loss: 2.649015, Val Loss: 2.980371\n",
      "Attention\t Epoch [3/10000], Train Loss: 2.638902, Val Loss: 2.989294\n",
      "Attention\t Epoch [4/10000], Train Loss: 2.639191, Val Loss: 3.015901\n",
      "Attention\t Epoch [5/10000], Train Loss: 2.652057, Val Loss: 3.042076 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 2.6389015727702065\n",
      "Min Val Loss: 2.9803706063557454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13620\\2399924745.py:267: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\t Epoch [1/10000], Train Loss: 2.820223, Val Loss: 4.288096\n",
      "Embedding\t Epoch [2/10000], Train Loss: 0.680887, Val Loss: 3.222119\n",
      "Embedding\t Epoch [3/10000], Train Loss: 0.626924, Val Loss: 3.132281\n",
      "Embedding\t Epoch [4/10000], Train Loss: 0.579559, Val Loss: 3.073628\n",
      "Embedding\t Epoch [5/10000], Train Loss: 0.545502, Val Loss: 3.029195\n",
      "Embedding\t Epoch [6/10000], Train Loss: 0.517922, Val Loss: 2.987049\n",
      "Embedding\t Epoch [7/10000], Train Loss: 0.495757, Val Loss: 2.982118\n",
      "Embedding\t Epoch [8/10000], Train Loss: 0.476553, Val Loss: 2.960867\n",
      "Embedding\t Epoch [9/10000], Train Loss: 0.461327, Val Loss: 2.959363\n",
      "Embedding\t Epoch [10/10000], Train Loss: 0.445645, Val Loss: 2.948445\n",
      "Embedding\t Epoch [11/10000], Train Loss: 0.431188, Val Loss: 2.948864\n",
      "Embedding\t Epoch [12/10000], Train Loss: 0.419242, Val Loss: 2.948785\n",
      "Embedding\t Epoch [13/10000], Train Loss: 0.409935, Val Loss: 2.962375 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 0.4099350881395918\n",
      "Min Val Loss: 2.9484451221341783\n",
      "Transformer\t Epoch [1/10000], Train Loss: 3.979259, Val Loss: 3.764719\n",
      "Transformer\t Epoch [2/10000], Train Loss: 3.804261, Val Loss: 3.714647\n",
      "Transformer\t Epoch [3/10000], Train Loss: 3.746551, Val Loss: 3.672384\n",
      "Transformer\t Epoch [4/10000], Train Loss: 3.714594, Val Loss: 3.628872\n",
      "Transformer\t Epoch [5/10000], Train Loss: 3.689114, Val Loss: 3.620462\n",
      "Transformer\t Epoch [6/10000], Train Loss: 3.667585, Val Loss: 3.613454\n",
      "Transformer\t Epoch [7/10000], Train Loss: 3.649803, Val Loss: 3.632541\n",
      "Transformer\t Epoch [8/10000], Train Loss: 3.638184, Val Loss: 3.670615\n",
      "Transformer\t Epoch [9/10000], Train Loss: 3.618976, Val Loss: 3.724321 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 3.6189759205221597\n",
      "Min Val Loss: 3.6134538884900724\n",
      "Attention\t Epoch [1/10000], Train Loss: 2.980114, Val Loss: 2.447071\n",
      "Attention\t Epoch [2/10000], Train Loss: 2.615692, Val Loss: 2.465446\n",
      "Attention\t Epoch [3/10000], Train Loss: 2.621776, Val Loss: 2.463147\n",
      "Attention\t Epoch [4/10000], Train Loss: 2.600803, Val Loss: 2.419583\n",
      "Attention\t Epoch [5/10000], Train Loss: 2.698863, Val Loss: 2.450406\n",
      "Attention\t Epoch [6/10000], Train Loss: 2.623651, Val Loss: 2.553825\n",
      "Attention\t Epoch [7/10000], Train Loss: 2.632040, Val Loss: 2.483872 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 2.600803011520896\n",
      "Min Val Loss: 2.419583178360636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13620\\2399924745.py:267: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\t Epoch [1/10000], Train Loss: 2.868630, Val Loss: 4.794413\n",
      "Embedding\t Epoch [2/10000], Train Loss: 0.710871, Val Loss: 3.522568\n",
      "Embedding\t Epoch [3/10000], Train Loss: 0.638268, Val Loss: 3.376236\n",
      "Embedding\t Epoch [4/10000], Train Loss: 0.589011, Val Loss: 3.300140\n",
      "Embedding\t Epoch [5/10000], Train Loss: 0.554462, Val Loss: 3.229333\n",
      "Embedding\t Epoch [6/10000], Train Loss: 0.527842, Val Loss: 3.192822\n",
      "Embedding\t Epoch [7/10000], Train Loss: 0.505491, Val Loss: 3.184837\n",
      "Embedding\t Epoch [8/10000], Train Loss: 0.485724, Val Loss: 3.153296\n",
      "Embedding\t Epoch [9/10000], Train Loss: 0.467698, Val Loss: 3.146536\n",
      "Embedding\t Epoch [10/10000], Train Loss: 0.452078, Val Loss: 3.123244\n",
      "Embedding\t Epoch [11/10000], Train Loss: 0.437240, Val Loss: 3.103764\n",
      "Embedding\t Epoch [12/10000], Train Loss: 0.423527, Val Loss: 3.085770\n",
      "Embedding\t Epoch [13/10000], Train Loss: 0.410903, Val Loss: 3.082884\n",
      "Embedding\t Epoch [14/10000], Train Loss: 0.398594, Val Loss: 3.077907\n",
      "Embedding\t Epoch [15/10000], Train Loss: 0.387982, Val Loss: 3.081175\n",
      "Embedding\t Epoch [16/10000], Train Loss: 0.377208, Val Loss: 3.102515\n",
      "Embedding\t Epoch [17/10000], Train Loss: 0.369406, Val Loss: 3.092980 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 0.36940551565000507\n",
      "Min Val Loss: 3.0779065383558577\n",
      "Transformer\t Epoch [1/10000], Train Loss: 4.172358, Val Loss: 4.067056\n",
      "Transformer\t Epoch [2/10000], Train Loss: 3.976176, Val Loss: 4.063077\n",
      "Transformer\t Epoch [3/10000], Train Loss: 3.906057, Val Loss: 3.963369\n",
      "Transformer\t Epoch [4/10000], Train Loss: 3.854245, Val Loss: 3.974814\n",
      "Transformer\t Epoch [5/10000], Train Loss: 3.808356, Val Loss: 3.982623\n",
      "Transformer\t Epoch [6/10000], Train Loss: 3.780295, Val Loss: 3.931140\n",
      "Transformer\t Epoch [7/10000], Train Loss: 3.754231, Val Loss: 3.888108\n",
      "Transformer\t Epoch [8/10000], Train Loss: 3.726611, Val Loss: 3.878174\n",
      "Transformer\t Epoch [9/10000], Train Loss: 3.693215, Val Loss: 3.873977\n",
      "Transformer\t Epoch [10/10000], Train Loss: 3.679874, Val Loss: 3.927987\n",
      "Transformer\t Epoch [11/10000], Train Loss: 3.663941, Val Loss: 3.866814\n",
      "Transformer\t Epoch [12/10000], Train Loss: 3.644539, Val Loss: 3.940546\n",
      "Transformer\t Epoch [13/10000], Train Loss: 3.622731, Val Loss: 3.890222\n",
      "Transformer\t Epoch [14/10000], Train Loss: 3.611340, Val Loss: 3.941972 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 3.6113403777703827\n",
      "Min Val Loss: 3.866813906669348\n",
      "Attention\t Epoch [1/10000], Train Loss: 3.408209, Val Loss: 3.117320\n",
      "Attention\t Epoch [2/10000], Train Loss: 3.030165, Val Loss: 3.129653\n",
      "Attention\t Epoch [3/10000], Train Loss: 3.023376, Val Loss: 3.137016\n",
      "Attention\t Epoch [4/10000], Train Loss: 3.015134, Val Loss: 3.182899 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 3.015133530389594\n",
      "Min Val Loss: 3.117319606757737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13620\\2399924745.py:267: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\t Epoch [1/10000], Train Loss: 2.907901, Val Loss: 4.689456\n",
      "Embedding\t Epoch [2/10000], Train Loss: 0.688701, Val Loss: 3.320739\n",
      "Embedding\t Epoch [3/10000], Train Loss: 0.623644, Val Loss: 3.245912\n",
      "Embedding\t Epoch [4/10000], Train Loss: 0.572159, Val Loss: 3.172281\n",
      "Embedding\t Epoch [5/10000], Train Loss: 0.537190, Val Loss: 3.137837\n",
      "Embedding\t Epoch [6/10000], Train Loss: 0.509800, Val Loss: 3.117202\n",
      "Embedding\t Epoch [7/10000], Train Loss: 0.487045, Val Loss: 3.121630\n",
      "Embedding\t Epoch [8/10000], Train Loss: 0.468008, Val Loss: 3.112559\n",
      "Embedding\t Epoch [9/10000], Train Loss: 0.450843, Val Loss: 3.137026\n",
      "Embedding\t Epoch [10/10000], Train Loss: 0.436465, Val Loss: 3.147764\n",
      "Embedding\t Epoch [11/10000], Train Loss: 0.423456, Val Loss: 3.134526 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 0.42345604194384634\n",
      "Min Val Loss: 3.1125592979344914\n",
      "Transformer\t Epoch [1/10000], Train Loss: 3.979823, Val Loss: 4.014244\n",
      "Transformer\t Epoch [2/10000], Train Loss: 3.774739, Val Loss: 3.861305\n",
      "Transformer\t Epoch [3/10000], Train Loss: 3.729794, Val Loss: 3.859693\n",
      "Transformer\t Epoch [4/10000], Train Loss: 3.701590, Val Loss: 3.797016\n",
      "Transformer\t Epoch [5/10000], Train Loss: 3.665569, Val Loss: 3.755118\n",
      "Transformer\t Epoch [6/10000], Train Loss: 3.652181, Val Loss: 3.725149\n",
      "Transformer\t Epoch [7/10000], Train Loss: 3.629843, Val Loss: 3.745175\n",
      "Transformer\t Epoch [8/10000], Train Loss: 3.612770, Val Loss: 3.723956\n",
      "Transformer\t Epoch [9/10000], Train Loss: 3.609455, Val Loss: 3.709164\n",
      "Transformer\t Epoch [10/10000], Train Loss: 3.585047, Val Loss: 3.678876\n",
      "Transformer\t Epoch [11/10000], Train Loss: 3.561388, Val Loss: 3.665539\n",
      "Transformer\t Epoch [12/10000], Train Loss: 3.556453, Val Loss: 3.692438\n",
      "Transformer\t Epoch [13/10000], Train Loss: 3.548527, Val Loss: 3.688397\n",
      "Transformer\t Epoch [14/10000], Train Loss: 3.525464, Val Loss: 3.681903 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 3.5254644151536025\n",
      "Min Val Loss: 3.6655388001372846\n",
      "Attention\t Epoch [1/10000], Train Loss: 2.862678, Val Loss: 3.270407\n",
      "Attention\t Epoch [2/10000], Train Loss: 2.695006, Val Loss: 3.169009\n",
      "Attention\t Epoch [3/10000], Train Loss: 2.652093, Val Loss: 3.031212\n",
      "Attention\t Epoch [4/10000], Train Loss: 2.612717, Val Loss: 3.017427\n",
      "Attention\t Epoch [5/10000], Train Loss: 2.645251, Val Loss: 2.966408\n",
      "Attention\t Epoch [6/10000], Train Loss: 2.628459, Val Loss: 3.016973\n",
      "Attention\t Epoch [7/10000], Train Loss: 2.647863, Val Loss: 3.039921\n",
      "Attention\t Epoch [8/10000], Train Loss: 2.678076, Val Loss: 3.058108 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 2.6127168055750114\n",
      "Min Val Loss: 2.966408410264508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13620\\2399924745.py:267: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\t Epoch [1/10000], Train Loss: 2.835907, Val Loss: 4.756550\n",
      "Embedding\t Epoch [2/10000], Train Loss: 0.694956, Val Loss: 3.488156\n",
      "Embedding\t Epoch [3/10000], Train Loss: 0.633267, Val Loss: 3.352349\n",
      "Embedding\t Epoch [4/10000], Train Loss: 0.584024, Val Loss: 3.230402\n",
      "Embedding\t Epoch [5/10000], Train Loss: 0.549609, Val Loss: 3.176133\n",
      "Embedding\t Epoch [6/10000], Train Loss: 0.519926, Val Loss: 3.154628\n",
      "Embedding\t Epoch [7/10000], Train Loss: 0.495799, Val Loss: 3.156580\n",
      "Embedding\t Epoch [8/10000], Train Loss: 0.474809, Val Loss: 3.141817\n",
      "Embedding\t Epoch [9/10000], Train Loss: 0.456408, Val Loss: 3.135414\n",
      "Embedding\t Epoch [10/10000], Train Loss: 0.440247, Val Loss: 3.116409\n",
      "Embedding\t Epoch [11/10000], Train Loss: 0.424966, Val Loss: 3.104204\n",
      "Embedding\t Epoch [12/10000], Train Loss: 0.409983, Val Loss: 3.114429\n",
      "Embedding\t Epoch [13/10000], Train Loss: 0.398030, Val Loss: 3.089877\n",
      "Embedding\t Epoch [14/10000], Train Loss: 0.386678, Val Loss: 3.103927\n",
      "Embedding\t Epoch [15/10000], Train Loss: 0.376082, Val Loss: 3.102301\n",
      "Embedding\t Epoch [16/10000], Train Loss: 0.368162, Val Loss: 3.069814\n",
      "Embedding\t Epoch [17/10000], Train Loss: 0.360347, Val Loss: 3.147307\n",
      "Embedding\t Epoch [18/10000], Train Loss: 0.354419, Val Loss: 3.119001\n",
      "Embedding\t Epoch [19/10000], Train Loss: 0.348289, Val Loss: 3.162810 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 0.3482890456237576\n",
      "Min Val Loss: 3.0698139209379542\n",
      "Transformer\t Epoch [1/10000], Train Loss: 3.966308, Val Loss: 3.975761\n",
      "Transformer\t Epoch [2/10000], Train Loss: 3.771765, Val Loss: 3.811323\n",
      "Transformer\t Epoch [3/10000], Train Loss: 3.738853, Val Loss: 3.835516\n",
      "Transformer\t Epoch [4/10000], Train Loss: 3.735519, Val Loss: 3.781538\n",
      "Transformer\t Epoch [5/10000], Train Loss: 3.717708, Val Loss: 3.777148\n",
      "Transformer\t Epoch [6/10000], Train Loss: 3.674987, Val Loss: 3.711100\n",
      "Transformer\t Epoch [7/10000], Train Loss: 3.636650, Val Loss: 3.687232\n",
      "Transformer\t Epoch [8/10000], Train Loss: 3.626437, Val Loss: 3.656366\n",
      "Transformer\t Epoch [9/10000], Train Loss: 3.600839, Val Loss: 3.622498\n",
      "Transformer\t Epoch [10/10000], Train Loss: 3.601776, Val Loss: 3.644739\n",
      "Transformer\t Epoch [11/10000], Train Loss: 3.579739, Val Loss: 3.615848\n",
      "Transformer\t Epoch [12/10000], Train Loss: 3.563713, Val Loss: 3.591500\n",
      "Transformer\t Epoch [13/10000], Train Loss: 3.556458, Val Loss: 3.601113\n",
      "Transformer\t Epoch [14/10000], Train Loss: 3.546439, Val Loss: 3.599582\n",
      "Transformer\t Epoch [15/10000], Train Loss: 3.532045, Val Loss: 3.679752 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 3.5320451919644857\n",
      "Min Val Loss: 3.591500213284113\n",
      "Attention\t Epoch [1/10000], Train Loss: 2.973104, Val Loss: 2.500010\n",
      "Attention\t Epoch [2/10000], Train Loss: 2.684876, Val Loss: 2.456884\n",
      "Attention\t Epoch [3/10000], Train Loss: 2.667499, Val Loss: 2.445413\n",
      "Attention\t Epoch [4/10000], Train Loss: 2.601191, Val Loss: 2.442983\n",
      "Attention\t Epoch [5/10000], Train Loss: 2.643039, Val Loss: 2.444700\n",
      "Attention\t Epoch [6/10000], Train Loss: 2.616881, Val Loss: 2.447205\n",
      "Attention\t Epoch [7/10000], Train Loss: 2.632087, Val Loss: 2.464421 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 2.6011909185490922\n",
      "Min Val Loss: 2.442983041686984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13620\\2399924745.py:267: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\t Epoch [1/10000], Train Loss: 2.555715, Val Loss: 3.983702\n",
      "Embedding\t Epoch [2/10000], Train Loss: 0.675334, Val Loss: 3.098135\n",
      "Embedding\t Epoch [3/10000], Train Loss: 0.622419, Val Loss: 2.971362\n",
      "Embedding\t Epoch [4/10000], Train Loss: 0.573446, Val Loss: 2.912646\n",
      "Embedding\t Epoch [5/10000], Train Loss: 0.539280, Val Loss: 2.900557\n",
      "Embedding\t Epoch [6/10000], Train Loss: 0.512494, Val Loss: 2.895371\n",
      "Embedding\t Epoch [7/10000], Train Loss: 0.489286, Val Loss: 2.873776\n",
      "Embedding\t Epoch [8/10000], Train Loss: 0.469948, Val Loss: 2.863860\n",
      "Embedding\t Epoch [9/10000], Train Loss: 0.453359, Val Loss: 2.875301\n",
      "Embedding\t Epoch [10/10000], Train Loss: 0.437284, Val Loss: 2.865549\n",
      "Embedding\t Epoch [11/10000], Train Loss: 0.423859, Val Loss: 2.867238 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 0.4238585347253265\n",
      "Min Val Loss: 2.8638595108022082\n",
      "Transformer\t Epoch [1/10000], Train Loss: 4.088533, Val Loss: 4.001967\n",
      "Transformer\t Epoch [2/10000], Train Loss: 3.941468, Val Loss: 3.938085\n",
      "Transformer\t Epoch [3/10000], Train Loss: 3.869251, Val Loss: 3.968033\n",
      "Transformer\t Epoch [4/10000], Train Loss: 3.796946, Val Loss: 4.009148\n",
      "Transformer\t Epoch [5/10000], Train Loss: 3.776481, Val Loss: 3.905750\n",
      "Transformer\t Epoch [6/10000], Train Loss: 3.762192, Val Loss: 3.947481\n",
      "Transformer\t Epoch [7/10000], Train Loss: 3.762476, Val Loss: 3.772682\n",
      "Transformer\t Epoch [8/10000], Train Loss: 3.728108, Val Loss: 3.827494\n",
      "Transformer\t Epoch [9/10000], Train Loss: 3.700456, Val Loss: 3.706582\n",
      "Transformer\t Epoch [10/10000], Train Loss: 3.670088, Val Loss: 3.796635\n",
      "Transformer\t Epoch [11/10000], Train Loss: 3.658830, Val Loss: 3.718959\n",
      "Transformer\t Epoch [12/10000], Train Loss: 3.644775, Val Loss: 3.756587 \n",
      "Early Stop Triggered!\n",
      "Min Train Loss: 3.6447750490245316\n",
      "Min Val Loss: 3.7065817135934407\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 57848758272 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    168\u001b[0m save_train_val_losses(transformer_train_losses, transformer_val_losses, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../데이터/Checkpoint/transformer/transformer_lr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_batch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_sub_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msub\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_ed_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_ws_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_epochs_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m### transformer attention\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDistrict_Dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m dataset_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset)\n\u001b[0;32m    173\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(train_ratio \u001b[38;5;241m*\u001b[39m dataset_length)\n",
      "File \u001b[1;32mc:\\Users\\USER\\Desktop\\sci\\SCI\\코드\\Dataset\\District_Dataset.py:112\u001b[0m, in \u001b[0;36mDistrict_Dataset.__init__\u001b[1;34m(self, model, table_1, table_2, table_3, embedding_dim, window_size, SUB, DEVICE)\u001b[0m\n\u001b[0;32m    109\u001b[0m         districts_apartment_complexes_prices_with_window_size\u001b[38;5;241m.\u001b[39mappend(district_apartment_complexes_prices_with_window_size) \u001b[38;5;66;03m# (38/24, 1)\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# 동마다 시점들 -> 시점들마다 동 \u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m districts_apartment_complexes_embedding_matrixes_with_window_size \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistricts_apartment_complexes_embedding_matrixes_with_window_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m districts_apartment_complexes_embedding_matrixes_with_window_size \u001b[38;5;241m=\u001b[39m districts_apartment_complexes_embedding_matrixes_with_window_size\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(districts), \u001b[38;5;28mlen\u001b[39m(table_2_copy)\u001b[38;5;241m-\u001b[39mwindow_size, max_apartment_complexes, window_size, embedding_dim)\n\u001b[0;32m    114\u001b[0m districts_apartment_complexes_embedding_matrixes_with_window_size \u001b[38;5;241m=\u001b[39m districts_apartment_complexes_embedding_matrixes_with_window_size\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 57848758272 bytes."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(columns=['lr','batch','sub','embedding_dim','window_size','val_loss'])\n",
    "for embedding_dim in embedding_dims:\n",
    "    for window_size in window_sizes:\n",
    "        ### embedding\n",
    "        dataset = Embedding_Dataset(table_1, table_2, table_3, DEVICE)\n",
    "        dataset_length = len(dataset)\n",
    "        train_size = int(train_ratio * dataset_length)\n",
    "        train_indices = range(0, train_size)\n",
    "        val_size = int(val_ratio * dataset_length)\n",
    "        val_indices = range(train_size, train_size + val_size)\n",
    "        # test_size = int(test_ratio * dataset_length)\n",
    "        # test_indices = range(train_size + val_size, dataset_length)\n",
    "        train_dataset = Subset(dataset, train_indices)\n",
    "        val_dataset = Subset(dataset, val_indices)\n",
    "        # test_dataset = Subset(dataset, test_indices)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "        # test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "        embedding_model = Embedding(128, 256, 512, embedding_dim, 512, 256, 128).to(DEVICE)\n",
    "        criterion = RMSE()\n",
    "        optimizer = torch.optim.Adam(embedding_model.parameters(), lr=lr)\n",
    "\n",
    "        embedding_train_losses = []\n",
    "        embedding_val_losses = []\n",
    "\n",
    "        max_early_stop_count = 3\n",
    "        early_stop_count = 0\n",
    "        embedding_best_val_loss = float('inf')\n",
    "        embedding_best_model_weights = None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            embedding_model.train()\n",
    "            embedding_total_train_loss = 0\n",
    "            for data in train_dataloader:\n",
    "                input = data[0].to(DEVICE)\n",
    "                target = data[1].to(DEVICE)\n",
    "                output = embedding_model(input).to(DEVICE)\n",
    "\n",
    "                embedding_train_loss = criterion(output, target)\n",
    "                embedding_total_train_loss += embedding_train_loss.item()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                embedding_train_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            embedding_avg_train_loss = embedding_total_train_loss / len(train_dataloader)\n",
    "            embedding_train_losses.append(embedding_avg_train_loss)\n",
    "\n",
    "            embedding_model.eval()\n",
    "            embedding_total_val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for data in val_dataloader:\n",
    "                    input = data[0].to(DEVICE)\n",
    "                    target = data[1].to(DEVICE)\n",
    "                    output = embedding_model(input).to(DEVICE)\n",
    "\n",
    "                    embedding_val_loss = criterion(output, target)\n",
    "                    embedding_total_val_loss += embedding_val_loss.item()\n",
    "\n",
    "            embedding_avg_val_loss = embedding_total_val_loss / len(val_dataloader)\n",
    "            embedding_val_losses.append(embedding_avg_val_loss)\n",
    "\n",
    "            if  embedding_best_val_loss > embedding_avg_val_loss:\n",
    "                embedding_best_val_loss = embedding_avg_val_loss\n",
    "                embedding_best_model_weights = copy.deepcopy(embedding_model.state_dict())\n",
    "                early_stop_count = 0\n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "\n",
    "            if early_stop_count >= max_early_stop_count:\n",
    "                print(f'Embedding\\t Epoch [{epoch+1}/{epochs}], Train Loss: {embedding_avg_train_loss:.6f}, Val Loss: {embedding_avg_val_loss:.6f} \\nEarly Stop Triggered!')\n",
    "                embedding_model.load_state_dict(embedding_best_model_weights)\n",
    "                torch.save(embedding_model, f'../데이터/Checkpoint/emb/embedding_lr_{lr}_batch_{batch}_sub_{sub}_ed_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "                break\n",
    "\n",
    "            print(f'Embedding\\t Epoch [{epoch+1}/{epochs}], Train Loss: {embedding_avg_train_loss:.6f}, Val Loss: {embedding_avg_val_loss:.6f}')\n",
    "            \n",
    "        save_train_val_losses(embedding_train_losses, embedding_val_losses, f'../데이터/Checkpoint/emb/embedding_lr_{lr}_batch_{batch}_sub_{sub}_ed_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "\n",
    "        ### transformer\n",
    "        dataset = Apartment_Complex_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, 'DL', DEVICE)\n",
    "        dataset_length = len(dataset)\n",
    "        train_size = int(train_ratio * dataset_length)\n",
    "        train_indices = range(0, train_size)\n",
    "        val_size = int(val_ratio * dataset_length)\n",
    "        val_indices = range(train_size, train_size + val_size)\n",
    "        # test_size = int(test_ratio * dataset_length)\n",
    "        # test_indices = range(train_size + val_size, dataset_length)\n",
    "        train_dataset = Subset(dataset, train_indices)\n",
    "        val_dataset = Subset(dataset, val_indices)\n",
    "        # test_dataset = Subset(dataset, test_indices)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "        # test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "        transformer_model = Transformer(embedding_dim, 1, 2, 2).to(DEVICE)\n",
    "        criterion = RMSE()\n",
    "        optimizer = torch.optim.Adam(transformer_model.parameters(), lr=lr)\n",
    "\n",
    "        transformer_train_losses = []\n",
    "        transformer_val_losses = []\n",
    "\n",
    "        max_early_stop_count = 3\n",
    "        early_stop_count = 0\n",
    "        transformer_best_val_loss = float('inf')\n",
    "        transformer_best_model_weights = None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            transformer_model.train()\n",
    "            transformer_total_train_loss = 0\n",
    "            transformer_total_train_num = 1e-9\n",
    "            for data in train_dataloader:\n",
    "                src = data[0].to(DEVICE)\n",
    "                trg = data[1].to(DEVICE)\n",
    "\n",
    "                if (trg[0] != 0):\n",
    "                    transformer_total_train_num += 1\n",
    "\n",
    "                    src_mask = transformer_model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "                    output = transformer_model(src, src_mask)\n",
    "\n",
    "                    transformer_train_loss = criterion(output[0], trg)\n",
    "                    transformer_total_train_loss += transformer_train_loss.item()\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    transformer_train_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "            transformer_avg_train_loss = transformer_total_train_loss / transformer_total_train_num\n",
    "            transformer_train_losses.append(transformer_avg_train_loss)\n",
    "\n",
    "            transformer_model.eval()\n",
    "            transformer_total_val_loss = 0\n",
    "            transformer_total_val_num = 1e-9\n",
    "            with torch.no_grad():\n",
    "                for data in val_dataloader:\n",
    "                    src = data[0].to(DEVICE)\n",
    "                    trg = data[1].to(DEVICE)\n",
    "\n",
    "                    if (trg[0] != 0):\n",
    "                        transformer_total_val_num += 1\n",
    "\n",
    "                        src_mask = transformer_model.generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
    "                        output = transformer_model(src, src_mask)\n",
    "\n",
    "                        transformer_val_loss = criterion(output[0], trg)\n",
    "                        transformer_total_val_loss += transformer_val_loss.item()\n",
    "\n",
    "            transformer_avg_val_loss = transformer_total_val_loss / transformer_total_val_num\n",
    "            transformer_val_losses.append(transformer_avg_val_loss)\n",
    "\n",
    "            if  transformer_best_val_loss > transformer_avg_val_loss:\n",
    "                transformer_best_val_loss = transformer_avg_val_loss\n",
    "                transformer_best_model_weights = copy.deepcopy(transformer_model.state_dict())\n",
    "                early_stop_count = 0\n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "                \n",
    "            if early_stop_count >= max_early_stop_count:\n",
    "                print(f'Transformer\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_avg_train_loss:.6f}, Val Loss: {transformer_avg_val_loss:.6f} \\nEarly Stop Triggered!')\n",
    "                transformer_model.load_state_dict(transformer_best_model_weights)\n",
    "                torch.save(transformer_model, f'../데이터/Checkpoint/transformer/transformer_lr_{lr}_batch_{batch}_sub_{sub}_ed_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "                break\n",
    "\n",
    "            print(f'Transformer\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_avg_train_loss:.6f}, Val Loss: {transformer_avg_val_loss:.6f}')\n",
    "        \n",
    "        save_train_val_losses(transformer_train_losses, transformer_val_losses, f'../데이터/Checkpoint/transformer/transformer_lr_{lr}_batch_{batch}_sub_{sub}_ed_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "\n",
    "        ### transformer attention\n",
    "        dataset = District_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, sub, DEVICE)\n",
    "        dataset_length = len(dataset)\n",
    "        train_size = int(train_ratio * dataset_length)\n",
    "        train_indices = range(0, train_size)\n",
    "        val_size = int(val_ratio * dataset_length)\n",
    "        val_indices = range(train_size, train_size + val_size)\n",
    "        # test_size = int(test_ratio * dataset_length)\n",
    "        # test_indices = range(train_size + val_size, dataset_length)\n",
    "        train_dataset = Subset(dataset, train_indices)\n",
    "        val_dataset = Subset(dataset, val_indices)\n",
    "        # test_dataset = Subset(dataset, test_indices)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "        # test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "        transformer_att_model = TransformerAttention(transformer_model, embedding_dim, 1, DEVICE).to(DEVICE)\n",
    "        criterion = RMSE()\n",
    "        optimizer = torch.optim.Adam(transformer_att_model.parameters(), lr=lr)\n",
    "\n",
    "        transformer_att_train_losses = []\n",
    "        transformer_att_val_losses = []\n",
    "\n",
    "        max_early_stop_count = 3\n",
    "        early_stop_count = 0\n",
    "        transformer_att_best_val_loss = float('inf')\n",
    "        transformer_att_best_model_weights = None\n",
    "\n",
    "        for epoch in range(epoch):\n",
    "            transformer_att_model.train()\n",
    "            transformer_att_total_train_loss = 0\n",
    "            transformer_att_total_train_num = 1e-9\n",
    "            for data in train_dataloader:\n",
    "                src = data[0][0].to(DEVICE)\n",
    "                max_len = data[1][0].to(DEVICE)\n",
    "                try:\n",
    "                    anw = torch.nonzero(data[2][0]).to(DEVICE)[0]\n",
    "                except:\n",
    "                    continue\n",
    "                trg = data[3][0].to(DEVICE)\n",
    "                \n",
    "                transformer_att_total_train_num += len(anw)\n",
    "\n",
    "                for index in anw:\n",
    "                    output = transformer_att_model(src, index, max_len)\n",
    "                    \n",
    "                    transformer_att_train_loss = criterion(output, trg[index])\n",
    "                    transformer_att_total_train_loss += transformer_att_train_loss.item()\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    transformer_att_train_loss.backward()\n",
    "                    optimizer.step() \n",
    "                    \n",
    "            transformer_att_avg_train_loss = transformer_att_total_train_loss / transformer_att_total_train_num\n",
    "            transformer_att_train_losses.append(transformer_att_avg_train_loss)\n",
    "\n",
    "            transformer_att_model.eval()\n",
    "            transformer_att_total_val_loss = 0\n",
    "            transformer_att_total_val_num = 1e-9\n",
    "            with torch.no_grad():\n",
    "                for data in val_dataloader:\n",
    "                    src = data[0][0].to(DEVICE)\n",
    "                    max_len = data[1][0].to(DEVICE)\n",
    "                    try:\n",
    "                        anw = torch.nonzero(data[2][0]).to(DEVICE)[0]\n",
    "                    except:\n",
    "                        continue\n",
    "                    trg = data[3][0].to(DEVICE)\n",
    "                    \n",
    "                    transformer_att_total_val_num += len(anw)\n",
    "\n",
    "                    for index in anw:\n",
    "                        output = transformer_att_model(src, index, max_len)\n",
    "\n",
    "                        transformer_att_val_loss = criterion(output, trg[index])\n",
    "                        transformer_att_total_val_loss += transformer_att_val_loss.item()\n",
    "                        \n",
    "            transformer_att_avg_val_loss = transformer_att_total_val_loss / transformer_att_total_val_num\n",
    "            transformer_att_val_losses.append(transformer_att_avg_val_loss)\n",
    "                    \n",
    "            if  transformer_att_best_val_loss > transformer_att_avg_val_loss:\n",
    "                transformer_att_best_val_loss = transformer_att_avg_val_loss\n",
    "                transformer_att_best_model_weights = copy.deepcopy(transformer_att_model.state_dict())\n",
    "                early_stop_count = 0\n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "\n",
    "            if early_stop_count >= max_early_stop_count:\n",
    "                print(f'Attention\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_att_avg_train_loss:.6f}, Val Loss: {transformer_att_avg_val_loss:.6f} \\nEarly Stop Triggered!')\n",
    "                transformer_att_model.load_state_dict(transformer_att_best_model_weights)\n",
    "                torch.save(transformer_att_model, f'../데이터/Checkpoint/attention/attention_lr_{lr}_batch_{batch}_sub_{sub}_ed_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}.pth')\n",
    "                break\n",
    "\n",
    "            print(f'Attention\\t Epoch [{epoch+1}/{epochs}], Train Loss: {transformer_att_avg_train_loss:.6f}, Val Loss: {transformer_att_avg_val_loss:.6f}')\n",
    "\n",
    "        save_train_val_losses(transformer_att_train_losses, transformer_att_val_losses, f'../데이터/Checkpoint/attention/attention_lr_{lr}_batch_{batch}_sub_{sub}_ed_{embedding_dim}_ws_{window_size}_epochs_{epoch+1}')\n",
    "\n",
    "        results_df = results_df.append({\n",
    "            'lr': lr,\n",
    "            'batch': batch,\n",
    "            'sub': sub,\n",
    "            'embedding_dim': embedding_dim,\n",
    "            'window_size': window_size,\n",
    "            'val_loss': min(transformer_att_val_losses),\n",
    "        }, ignore_index=True)\n",
    "\n",
    "results_df = results_df.sort_values('val_loss')\n",
    "results_df.to_excel('../데이터/Checkpoint/experiment_results.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_att_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer attention\n",
    "dataset = District_Dataset(embedding_model, table_1, table_2, table_3, embedding_dim, window_size, sub, DEVICE)\n",
    "dataset_length = len(dataset)\n",
    "train_size = int(train_ratio * dataset_length)\n",
    "# train_indices = range(0, train_size)\n",
    "val_size = int(val_ratio * dataset_length)\n",
    "# val_indices = range(train_size, train_size + val_size)\n",
    "test_size = int(test_ratio * dataset_length)\n",
    "test_indices = range(train_size + val_size, dataset_length)\n",
    "# train_dataset = Subset(dataset, train_indices)\n",
    "# val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
    "\n",
    "transformer_att_model.eval()\n",
    "transformer_att_total_test_rmse = 0\n",
    "transformer_att_total_test_mse = 0\n",
    "transformer_att_total_test_mae = 0\n",
    "transformer_att_total_test_mape = 0\n",
    "transformer_att_total_test_num = 1e-9\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        src = data[0][0].to(DEVICE)\n",
    "        max_len = data[1][0].to(DEVICE)\n",
    "        try:\n",
    "            anw = torch.nonzero(data[2][0]).to(DEVICE)[0]\n",
    "        except:\n",
    "            continue\n",
    "        trg = data[3][0].to(DEVICE)\n",
    "\n",
    "        transformer_att_total_test_num += len(anw)\n",
    "\n",
    "        for index in anw:\n",
    "            output = transformer_att_model(src, index, max_len)\n",
    "\n",
    "            transformer_att_test_rmse = rmse(output, trg[index])\n",
    "            transformer_att_test_mse = mse(output, trg[index])\n",
    "            transformer_att_test_mae = mae(output, trg[index])\n",
    "            transformer_att_test_mape = mape(output, trg[index])\n",
    "            \n",
    "            transformer_att_total_test_rmse += transformer_att_test_rmse.item()\n",
    "            transformer_att_total_test_mse += transformer_att_test_mse.item()\n",
    "            transformer_att_total_test_mae += transformer_att_test_mae.item()\n",
    "            transformer_att_total_test_mape += transformer_att_test_mape.item()\n",
    "            \n",
    "transformer_att_avg_test_rmse = transformer_att_total_test_rmse / transformer_att_total_test_num\n",
    "transformer_att_avg_test_mse = transformer_att_total_test_mse / transformer_att_total_test_num\n",
    "transformer_att_avg_test_mae = transformer_att_total_test_mae / transformer_att_total_test_num\n",
    "transformer_att_avg_test_mape = transformer_att_total_test_mape / transformer_att_total_test_num\n",
    "        \n",
    "print(f'Test RMSE: {transformer_att_avg_test_rmse}')\n",
    "print(f'Test MSE: {transformer_att_avg_test_mse}')\n",
    "print(f'Test MAE: {transformer_att_avg_test_mae}')\n",
    "print(f'Test MAPE: {transformer_att_avg_test_mape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
